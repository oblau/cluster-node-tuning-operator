2025-11-17T09:33:16.414515436Z I1117 09:33:16.414348       1 profiler.go:21] Starting profiling endpoint at http://127.0.0.1:6060/debug/pprof/
2025-11-17T09:33:16.414638163Z I1117 09:33:16.414526       1 observer_polling.go:159] Starting file observer
2025-11-17T09:33:16.414897936Z I1117 09:33:16.414869       1 cmd.go:233] Using service-serving-cert provided certificates
2025-11-17T09:33:16.414926296Z I1117 09:33:16.414906       1 leaderelection.go:122] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2025-11-17T09:33:16.415338420Z I1117 09:33:16.415322       1 observer_polling.go:159] Starting file observer
2025-11-17T09:33:16.432305462Z I1117 09:33:16.432246       1 builder.go:271] openshift-cluster-etcd-operator version 4.14.0-202511060117.p2.g9abf7d2.assembly.stream.el8-9abf7d2-9abf7d22b5fda0e06fb1f9a2fa90caaa09d5a932
2025-11-17T09:33:16.604379870Z I1117 09:33:16.604296       1 secure_serving.go:57] Forcing use of http/1.1 only
2025-11-17T09:33:16.604379870Z W1117 09:33:16.604337       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2025-11-17T09:33:16.604379870Z W1117 09:33:16.604344       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2025-11-17T09:33:16.608818326Z I1117 09:33:16.608761       1 leaderelection.go:250] attempting to acquire leader lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock...
2025-11-17T09:33:16.609832864Z I1117 09:33:16.609786       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2025-11-17T09:33:16.609888313Z I1117 09:33:16.609875       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
2025-11-17T09:33:16.609963993Z I1117 09:33:16.609951       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2025-11-17T09:33:16.609983054Z I1117 09:33:16.609977       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-11-17T09:33:16.610073901Z I1117 09:33:16.610044       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2025-11-17T09:33:16.610093819Z I1117 09:33:16.610087       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-11-17T09:33:16.612094353Z I1117 09:33:16.612069       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2025-11-17T09:33:16.612789906Z I1117 09:33:16.612769       1 secure_serving.go:210] Serving securely on [::]:8443
2025-11-17T09:33:16.612835176Z I1117 09:33:16.612820       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2025-11-17T09:33:16.617340141Z I1117 09:33:16.617297       1 leaderelection.go:260] successfully acquired lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock
2025-11-17T09:33:16.617511521Z I1117 09:33:16.617485       1 event.go:298] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-etcd-operator", Name:"openshift-cluster-etcd-operator-lock", UID:"5c5e32c5-449b-4b7e-8531-befaa8976a38", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"19641", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-6864485db7-n5l65_8e69978d-bde9-44b1-bd70-cd13a27827b7 became leader
2025-11-17T09:33:16.621302149Z I1117 09:33:16.621261       1 starter.go:166] recorded cluster versions: map[etcd:4.14.59 operator:4.14.59 raw-internal:4.14.59]
2025-11-17T09:33:16.630222086Z I1117 09:33:16.630184       1 simple_featuregate_reader.go:171] Starting feature-gate-detector
2025-11-17T09:33:16.633223189Z I1117 09:33:16.633170       1 starter.go:435] FeatureGates initializedenabled[AlibabaPlatform AzureWorkloadIdentity BuildCSIVolumes CloudDualStackNodeIPs ExternalCloudProviderAzure ExternalCloudProviderExternal PrivateHostedZoneAWS]disabled[AdminNetworkPolicy AdmissionWebhookMatchConditions AutomatedEtcdBackup CSIDriverSharedResource DynamicResourceAllocation EventedPLEG ExternalCloudProvider ExternalCloudProviderGCP GCPLabelsTags GatewayAPI InsightsConfigAPI MachineAPIOperatorDisableMachineHealthCheckController MachineAPIProviderOpenStack MaxUnavailableStatefulSet NetworkLiveMigration NodeSwap OpenShiftPodSecurityAdmission RetroactiveDefaultStorageClass RouteExternalCertificate SigstoreImageVerification VSphereStaticIPs ValidatingAdmissionPolicy]
2025-11-17T09:33:16.633261042Z I1117 09:33:16.633245       1 starter.go:490] waiting for cluster version informer sync...
2025-11-17T09:33:16.633541888Z I1117 09:33:16.633472       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'FeatureGatesInitialized' FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AlibabaPlatform", "AzureWorkloadIdentity", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "PrivateHostedZoneAWS"}, Disabled:[]v1.FeatureGateName{"AdminNetworkPolicy", "AdmissionWebhookMatchConditions", "AutomatedEtcdBackup", "CSIDriverSharedResource", "DynamicResourceAllocation", "EventedPLEG", "ExternalCloudProvider", "ExternalCloudProviderGCP", "GCPLabelsTags", "GatewayAPI", "InsightsConfigAPI", "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack", "MaxUnavailableStatefulSet", "NetworkLiveMigration", "NodeSwap", "OpenShiftPodSecurityAdmission", "RetroactiveDefaultStorageClass", "RouteExternalCertificate", "SigstoreImageVerification", "VSphereStaticIPs", "ValidatingAdmissionPolicy"}}
2025-11-17T09:33:16.644450325Z I1117 09:33:16.644408       1 starter.go:513] Detected available machine API, starting vertical scaling related controllers and informers...
2025-11-17T09:33:16.644886210Z I1117 09:33:16.644867       1 base_controller.go:67] Waiting for caches to sync for ClusterMemberRemovalController
2025-11-17T09:33:16.645086783Z I1117 09:33:16.645070       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2025-11-17T09:33:16.645093657Z I1117 09:33:16.645089       1 base_controller.go:67] Waiting for caches to sync for MachineDeletionHooksController
2025-11-17T09:33:16.645178663Z I1117 09:33:16.645162       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2025-11-17T09:33:16.645343052Z I1117 09:33:16.645327       1 base_controller.go:67] Waiting for caches to sync for EtcdCertSignerController
2025-11-17T09:33:16.645350364Z I1117 09:33:16.645345       1 base_controller.go:67] Waiting for caches to sync for EtcdEndpointsController
2025-11-17T09:33:16.645383098Z I1117 09:33:16.645374       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2025-11-17T09:33:16.645586093Z I1117 09:33:16.645572       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_etcd
2025-11-17T09:33:16.645592516Z I1117 09:33:16.645589       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2025-11-17T09:33:16.645606727Z I1117 09:33:16.645599       1 base_controller.go:67] Waiting for caches to sync for ClusterMemberController
2025-11-17T09:33:16.645619815Z I1117 09:33:16.645611       1 base_controller.go:67] Waiting for caches to sync for EtcdMembersController
2025-11-17T09:33:16.645619815Z I1117 09:33:16.645614       1 base_controller.go:73] Caches are synced for EtcdMembersController 
2025-11-17T09:33:16.645633385Z I1117 09:33:16.645622       1 base_controller.go:110] Starting #1 XXXXXX of EtcdMembersController controller ...
2025-11-17T09:33:16.645657809Z I1117 09:33:16.645650       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2025-11-17T09:33:16.645674216Z I1117 09:33:16.645666       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2025-11-17T09:33:16.645690544Z I1117 09:33:16.645682       1 base_controller.go:67] Waiting for caches to sync for GuardController
2025-11-17T09:33:16.645695117Z I1117 09:33:16.645683       1 envvarcontroller.go:188] Starting EnvVarController
2025-11-17T09:33:16.645743687Z I1117 09:33:16.645734       1 base_controller.go:67] Waiting for caches to sync for BootstrapTeardownController
2025-11-17T09:33:16.645758006Z I1117 09:33:16.645750       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2025-11-17T09:33:16.645774116Z I1117 09:33:16.645765       1 base_controller.go:67] Waiting for caches to sync for ScriptController
2025-11-17T09:33:16.645788700Z I1117 09:33:16.645780       1 base_controller.go:67] Waiting for caches to sync for DefragController
2025-11-17T09:33:16.645803881Z I1117 09:33:16.645791       1 base_controller.go:67] Waiting for caches to sync for BackingResourceController
2025-11-17T09:33:16.645829561Z I1117 09:33:16.645821       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2025-11-17T09:33:16.645882970Z E1117 09:33:16.645873       1 base_controller.go:268] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.646124250Z I1117 09:33:16.646107       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ReportEtcdMembersErrorUpdatingStatus' etcds.operator.openshift.io "cluster" not found
2025-11-17T09:33:16.646219148Z I1117 09:33:16.646209       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2025-11-17T09:33:16.646230914Z I1117 09:33:16.646224       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2025-11-17T09:33:16.646261879Z I1117 09:33:16.646252       1 base_controller.go:67] Waiting for caches to sync for PruneController
2025-11-17T09:33:16.646273358Z I1117 09:33:16.646266       1 base_controller.go:67] Waiting for caches to sync for NodeController
2025-11-17T09:33:16.646624657Z I1117 09:33:16.646608       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2025-11-17T09:33:16.646670000Z I1117 09:33:16.645795       1 base_controller.go:67] Waiting for caches to sync for ClusterUpgradeBackupController
2025-11-17T09:33:16.648725780Z I1117 09:33:16.647398       1 base_controller.go:67] Waiting for caches to sync for FSyncController
2025-11-17T09:33:16.648725780Z I1117 09:33:16.647417       1 base_controller.go:73] Caches are synced for FSyncController 
2025-11-17T09:33:16.648725780Z I1117 09:33:16.647423       1 base_controller.go:110] Starting #1 XXXXXX of FSyncController controller ...
2025-11-17T09:33:16.659470804Z E1117 09:33:16.659430       1 base_controller.go:268] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.659504425Z I1117 09:33:16.659481       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ReportEtcdMembersErrorUpdatingStatus' etcds.operator.openshift.io "cluster" not found
2025-11-17T09:33:16.659609025Z I1117 09:33:16.659587       1 base_controller.go:67] Waiting for caches to sync for EtcdStaticResources
2025-11-17T09:33:16.673174133Z I1117 09:33:16.673128       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ReportEtcdMembersErrorUpdatingStatus' etcds.operator.openshift.io "cluster" not found
2025-11-17T09:33:16.674362778Z E1117 09:33:16.674320       1 base_controller.go:268] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.676339760Z E1117 09:33:16.676307       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:16.684534560Z E1117 09:33:16.684496       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:16.700253557Z E1117 09:33:16.697822       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:16.704465351Z E1117 09:33:16.704417       1 base_controller.go:268] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.710360770Z I1117 09:33:16.710309       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-11-17T09:33:16.710360770Z I1117 09:33:16.710337       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
2025-11-17T09:33:16.710360770Z I1117 09:33:16.710318       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-11-17T09:33:16.720784025Z E1117 09:33:16.720748       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:16.745057994Z I1117 09:33:16.745006       1 base_controller.go:73] Caches are synced for ClusterMemberRemovalController 
2025-11-17T09:33:16.745057994Z I1117 09:33:16.745033       1 base_controller.go:110] Starting #1 XXXXXX of ClusterMemberRemovalController controller ...
2025-11-17T09:33:16.745307578Z E1117 09:33:16.745275       1 base_controller.go:268] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.745385891Z I1117 09:33:16.745371       1 base_controller.go:73] Caches are synced for EtcdCertSignerController 
2025-11-17T09:33:16.745393480Z I1117 09:33:16.745386       1 base_controller.go:110] Starting #1 XXXXXX of EtcdCertSignerController controller ...
2025-11-17T09:33:16.745399595Z I1117 09:33:16.745392       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2025-11-17T09:33:16.745416620Z I1117 09:33:16.745399       1 base_controller.go:110] Starting #1 XXXXXX of MissingStaticPodController controller ...
2025-11-17T09:33:16.745432079Z I1117 09:33:16.745416       1 base_controller.go:73] Caches are synced for TargetConfigController 
2025-11-17T09:33:16.745484924Z I1117 09:33:16.745462       1 base_controller.go:110] Starting #1 XXXXXX of TargetConfigController controller ...
2025-11-17T09:33:16.745596563Z E1117 09:33:16.745577       1 base_controller.go:268] TargetConfigController reconciliation failed: TargetConfigController missing env var values
2025-11-17T09:33:16.745642855Z I1117 09:33:16.745632       1 base_controller.go:73] Caches are synced for StatusSyncer_etcd 
2025-11-17T09:33:16.745661338Z I1117 09:33:16.745651       1 base_controller.go:110] Starting #1 XXXXXX of StatusSyncer_etcd controller ...
2025-11-17T09:33:16.745666655Z I1117 09:33:16.745655       1 base_controller.go:73] Caches are synced for ClusterMemberController 
2025-11-17T09:33:16.745673050Z I1117 09:33:16.745668       1 base_controller.go:110] Starting #1 XXXXXX of ClusterMemberController controller ...
2025-11-17T09:33:16.745761082Z I1117 09:33:16.745750       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2025-11-17T09:33:16.745766452Z I1117 09:33:16.745761       1 base_controller.go:110] Starting #1 XXXXXX of UnsupportedConfigOverridesController controller ...
2025-11-17T09:33:16.746261702Z I1117 09:33:16.745959       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2025-11-17T09:33:16.746261702Z I1117 09:33:16.745977       1 base_controller.go:110] Starting #1 XXXXXX of LoggingSyncer controller ...
2025-11-17T09:33:16.746525982Z I1117 09:33:16.746494       1 base_controller.go:73] Caches are synced for InstallerStateController 
2025-11-17T09:33:16.746538348Z I1117 09:33:16.746523       1 base_controller.go:110] Starting #1 XXXXXX of InstallerStateController controller ...
2025-11-17T09:33:16.746581242Z I1117 09:33:16.746566       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2025-11-17T09:33:16.746581242Z I1117 09:33:16.746578       1 base_controller.go:110] Starting #1 XXXXXX of StaticPodStateController controller ...
2025-11-17T09:33:16.746623675Z I1117 09:33:16.746606       1 base_controller.go:73] Caches are synced for PruneController 
2025-11-17T09:33:16.746629407Z I1117 09:33:16.746624       1 base_controller.go:110] Starting #1 XXXXXX of PruneController controller ...
2025-11-17T09:33:16.746785993Z I1117 09:33:16.746771       1 base_controller.go:73] Caches are synced for ClusterUpgradeBackupController 
2025-11-17T09:33:16.746791769Z I1117 09:33:16.746785       1 base_controller.go:73] Caches are synced for RevisionController 
2025-11-17T09:33:16.746799871Z I1117 09:33:16.746794       1 base_controller.go:110] Starting #1 XXXXXX of RevisionController controller ...
2025-11-17T09:33:16.746840641Z I1117 09:33:16.746823       1 base_controller.go:73] Caches are synced for InstallerController 
2025-11-17T09:33:16.746840641Z I1117 09:33:16.746838       1 base_controller.go:110] Starting #1 XXXXXX of InstallerController controller ...
2025-11-17T09:33:16.746885265Z I1117 09:33:16.746873       1 base_controller.go:73] Caches are synced for ScriptController 
2025-11-17T09:33:16.746890806Z I1117 09:33:16.746884       1 base_controller.go:110] Starting #1 XXXXXX of ScriptController controller ...
2025-11-17T09:33:16.746978330Z I1117 09:33:16.746785       1 base_controller.go:110] Starting #1 XXXXXX of ClusterUpgradeBackupController controller ...
2025-11-17T09:33:16.747030349Z I1117 09:33:16.747017       1 base_controller.go:73] Caches are synced for DefragController 
2025-11-17T09:33:16.747036020Z I1117 09:33:16.747031       1 base_controller.go:110] Starting #1 XXXXXX of DefragController controller ...
2025-11-17T09:33:16.747066429Z I1117 09:33:16.747058       1 envvarcontroller.go:194] caches synced
2025-11-17T09:33:16.747105571Z I1117 09:33:16.747096       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2025-11-17T09:33:16.747105571Z I1117 09:33:16.747103       1 base_controller.go:110] Starting #1 XXXXXX of UnsupportedConfigOverridesController controller ...
2025-11-17T09:33:16.747345715Z E1117 09:33:16.747333       1 base_controller.go:268] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.747428609Z I1117 09:33:16.747412       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:16.754695085Z E1117 09:33:16.754175       1 base_controller.go:268] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.759362125Z I1117 09:33:16.759313       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdEndpointsDegraded: failed to get member list: giving up getting a cached client after 3 tries\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:33:16.759729888Z E1117 09:33:16.759689       1 base_controller.go:268] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.760636947Z E1117 09:33:16.760613       1 base_controller.go:268] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.760916212Z I1117 09:33:16.760896       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:16.761311079Z E1117 09:33:16.761255       1 base_controller.go:268] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.761876252Z I1117 09:33:16.761852       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All XXXXXX nodes are ready\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nEtcdEndpointsDegraded: failed to get member list: giving up getting a cached client after 3 tries\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 4; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:16.765148033Z E1117 09:33:16.765099       1 base_controller.go:268] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.765272911Z E1117 09:33:16.765256       1 base_controller.go:268] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.765968801Z E1117 09:33:16.765954       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:16.772570677Z I1117 09:33:16.772525       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdEndpointsDegraded: failed to get member list: giving up getting a cached client after 3 tries\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All XXXXXX nodes are ready\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nEtcdEndpointsDegraded: failed to get member list: giving up getting a cached client after 3 tries\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:33:16.781717083Z E1117 09:33:16.781676       1 base_controller.go:268] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2025-11-17T09:33:16.782093379Z I1117 09:33:16.782070       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:16.782295406Z E1117 09:33:16.782275       1 base_controller.go:268] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.782685613Z E1117 09:33:16.782665       1 base_controller.go:268] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.782788132Z I1117 09:33:16.782755       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nEtcdEndpointsDegraded: failed to get member list: giving up getting a cached client after 3 tries\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 4; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:16.785936949Z E1117 09:33:16.785894       1 base_controller.go:268] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.790100869Z I1117 09:33:16.789640       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All XXXXXX nodes are ready\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nEtcdEndpointsDegraded: failed to get member list: giving up getting a cached client after 3 tries\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nEtcdEndpointsDegraded: failed to get member list: giving up getting a cached client after 3 tries\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:33:16.807912346Z E1117 09:33:16.807869       1 base_controller.go:268] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.826078949Z E1117 09:33:16.826036       1 base_controller.go:268] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.851098316Z E1117 09:33:16.851042       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:16.866634922Z E1117 09:33:16.866575       1 base_controller.go:268] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.968784771Z E1117 09:33:16.968737       1 base_controller.go:268] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:16.987187468Z E1117 09:33:16.987137       1 base_controller.go:268] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.014554995Z E1117 09:33:17.014475       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:17.027955731Z E1117 09:33:17.027898       1 base_controller.go:268] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.290138096Z E1117 09:33:17.290083       1 base_controller.go:268] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.309193647Z E1117 09:33:17.308305       1 base_controller.go:268] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.337706718Z E1117 09:33:17.337654       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:17.345821332Z I1117 09:33:17.345767       1 base_controller.go:73] Caches are synced for MachineDeletionHooksController 
2025-11-17T09:33:17.345821332Z I1117 09:33:17.345788       1 base_controller.go:110] Starting #1 XXXXXX of MachineDeletionHooksController controller ...
2025-11-17T09:33:17.345821332Z I1117 09:33:17.345792       1 base_controller.go:73] Caches are synced for BootstrapTeardownController 
2025-11-17T09:33:17.345846677Z I1117 09:33:17.345819       1 base_controller.go:110] Starting #1 XXXXXX of BootstrapTeardownController controller ...
2025-11-17T09:33:17.346124360Z E1117 09:33:17.346112       1 base_controller.go:268] BootstrapTeardownController reconciliation failed: error while canRemoveEtcdBootstrap: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.349208301Z E1117 09:33:17.349179       1 base_controller.go:268] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.351388087Z E1117 09:33:17.351366       1 base_controller.go:268] BootstrapTeardownController reconciliation failed: error while canRemoveEtcdBootstrap: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.361684844Z E1117 09:33:17.361644       1 base_controller.go:268] BootstrapTeardownController reconciliation failed: error while canRemoveEtcdBootstrap: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.382475184Z E1117 09:33:17.382117       1 base_controller.go:268] BootstrapTeardownController reconciliation failed: error while canRemoveEtcdBootstrap: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.422753219Z E1117 09:33:17.422701       1 base_controller.go:268] BootstrapTeardownController reconciliation failed: error while canRemoveEtcdBootstrap: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.503647392Z E1117 09:33:17.503593       1 base_controller.go:268] BootstrapTeardownController reconciliation failed: error while canRemoveEtcdBootstrap: getting cache client could not retrieve endpoints: node lister not synced
2025-11-17T09:33:17.546674674Z I1117 09:33:17.546627       1 base_controller.go:73] Caches are synced for BackingResourceController 
2025-11-17T09:33:17.546764572Z I1117 09:33:17.546755       1 base_controller.go:110] Starting #1 XXXXXX of BackingResourceController controller ...
2025-11-17T09:33:17.664649511Z I1117 09:33:17.664607       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:33:17.745814037Z I1117 09:33:17.745754       1 base_controller.go:73] Caches are synced for GuardController 
2025-11-17T09:33:17.745814037Z I1117 09:33:17.745779       1 base_controller.go:110] Starting #1 XXXXXX of GuardController controller ...
2025-11-17T09:33:17.745902984Z I1117 09:33:17.745887       1 base_controller.go:73] Caches are synced for EtcdEndpointsController 
2025-11-17T09:33:17.745928510Z I1117 09:33:17.745921       1 base_controller.go:110] Starting #1 XXXXXX of EtcdEndpointsController controller ...
2025-11-17T09:33:17.746416599Z I1117 09:33:17.746348       1 base_controller.go:73] Caches are synced for NodeController 
2025-11-17T09:33:17.746416599Z I1117 09:33:17.746360       1 base_controller.go:110] Starting #1 XXXXXX of NodeController controller ...
2025-11-17T09:33:17.761686432Z I1117 09:33:17.761645       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:17.761861448Z I1117 09:33:17.761845       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:33:17.762538640Z I1117 09:33:17.762491       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 4; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:17.762818841Z I1117 09:33:17.762793       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:33:17.762883396Z I1117 09:33:17.762861       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:33:17.773618568Z I1117 09:33:17.773567       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nEtcdEndpointsDegraded: failed to get member list: giving up getting a cached client after 3 tries\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:33:17.831408256Z I1117 09:33:17.831357       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:17.832110921Z I1117 09:33:17.832069       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 4; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:17.840695908Z I1117 09:33:17.840646       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:33:17.842951917Z I1117 09:33:17.842912       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 71376896
2025-11-17T09:33:17.845266467Z I1117 09:33:17.845233       1 request.go:697] Waited for 1.188530267s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps?limit=500&resourceVersion=0
2025-11-17T09:33:17.904491531Z I1117 09:33:17.904432       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 71299072
2025-11-17T09:33:17.904491531Z I1117 09:33:17.904449       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 71405568
2025-11-17T09:33:17.946182406Z I1117 09:33:17.946133       1 base_controller.go:73] Caches are synced for ConfigObserver 
2025-11-17T09:33:17.946182406Z I1117 09:33:17.946150       1 base_controller.go:110] Starting #1 XXXXXX of ConfigObserver controller ...
2025-11-17T09:33:17.984100306Z E1117 09:33:17.984051       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:17.995274427Z I1117 09:33:17.995234       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 71299072
2025-11-17T09:33:18.008402266Z I1117 09:33:18.008345       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:18.010776421Z I1117 09:33:18.010740       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 4; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:18.018524465Z I1117 09:33:18.016971       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:33:18.072624550Z I1117 09:33:18.072561       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 71299072
2025-11-17T09:33:18.072624550Z I1117 09:33:18.072580       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 71430144
2025-11-17T09:33:18.146335515Z I1117 09:33:18.146266       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2025-11-17T09:33:18.146335515Z I1117 09:33:18.146300       1 base_controller.go:110] Starting #1 XXXXXX of ResourceSyncController controller ...
2025-11-17T09:33:18.259730178Z I1117 09:33:18.259674       1 base_controller.go:73] Caches are synced for EtcdStaticResources 
2025-11-17T09:33:18.259730178Z I1117 09:33:18.259696       1 base_controller.go:110] Starting #1 XXXXXX of EtcdStaticResources controller ...
2025-11-17T09:33:18.845698731Z I1117 09:33:18.845543       1 request.go:697] Waited for 2.096974374s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-XXXXXX1
2025-11-17T09:33:19.263371589Z I1117 09:33:19.263321       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:19.263668119Z I1117 09:33:19.263640       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 4; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:19.267620764Z E1117 09:33:19.267574       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:19.271785007Z I1117 09:33:19.271751       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:33:19.387320261Z I1117 09:33:19.387267       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.02 %, dbSize: 71368704
2025-11-17T09:33:19.387320261Z I1117 09:33:19.387297       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 71467008
2025-11-17T09:33:19.845747117Z I1117 09:33:19.845688       1 request.go:697] Waited for 1.585566367s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2025-11-17T09:33:20.846965950Z I1117 09:33:20.846242       1 request.go:697] Waited for 1.389635669s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:33:21.654315702Z I1117 09:33:21.654243       1 installer_controller.go:512] "XXXXXX1" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 4
2025-11-17T09:33:21.831600698Z E1117 09:33:21.831425       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:21.846354378Z I1117 09:33:21.846243       1 request.go:697] Waited for 1.396898992s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:33:23.046140504Z I1117 09:33:23.046085       1 request.go:697] Waited for 1.193149294s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:33:24.648353827Z I1117 09:33:24.648306       1 installer_controller.go:512] "XXXXXX1" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 4
2025-11-17T09:33:26.954367662Z E1117 09:33:26.954310       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:35.467652883Z I1117 09:33:35.467228       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MasterNodeObserved' Observed new XXXXXX node XXXXXX0
2025-11-17T09:33:35.482211526Z I1117 09:33:35.481536       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodDisruptionBudgetUpdated' Updated PodDisruptionBudget.policy/etcd-guard-pdb -n openshift-etcd because it changed
2025-11-17T09:33:35.484091794Z I1117 09:33:35.484039       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MasterNodeObserved' Observed new XXXXXX node XXXXXX0
2025-11-17T09:33:35.485572862Z I1117 09:33:35.485548       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:35.488608236Z I1117 09:33:35.488562       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady ([container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes \"XXXXXX0\" not found])\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 4; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:35.522617270Z I1117 09:33:35.522511       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady ([container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes \"XXXXXX0\" not found])\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:33:35.731901805Z I1117 09:33:35.731783       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-peer-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:33:35.923805606Z I1117 09:33:35.923746       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-serving-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:33:36.100187110Z I1117 09:33:36.100074       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-serving-metrics-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:33:36.110000746Z I1117 09:33:36.109895       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretUpdated' Updated Secret/etcd-all-certs -n openshift-etcd because it changed
2025-11-17T09:33:36.110169662Z I1117 09:33:36.110147       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "required secret/etcd-all-certs has changed"
2025-11-17T09:33:36.666471378Z I1117 09:33:36.666402       1 request.go:697] Waited for 1.161996005s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:33:36.873397976Z I1117 09:33:36.870657       1 installer_controller.go:512] "XXXXXX1" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 4
2025-11-17T09:33:36.884194642Z I1117 09:33:36.881540       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:36.884194642Z I1117 09:33:36.882341       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady ([container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes \"XXXXXX0\" not found])\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:36.905034552Z I1117 09:33:36.904013       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 4; 1 nodes are at revision 6" to "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available"
2025-11-17T09:33:36.976957014Z I1117 09:33:36.976911       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.02 %, dbSize: 73248768
2025-11-17T09:33:37.199308014Z E1117 09:33:37.199253       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:37.272852086Z I1117 09:33:37.272490       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-scripts -n openshift-etcd:
2025-11-17T09:33:37.272852086Z cause by changes in data.etcd.env
2025-11-17T09:33:37.470186083Z I1117 09:33:37.470127       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-7 -n openshift-etcd because it was missing
2025-11-17T09:33:37.666635050Z I1117 09:33:37.666509       1 request.go:697] Waited for 1.396410762s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:33:38.666879420Z I1117 09:33:38.666832       1 request.go:697] Waited for 1.597714154s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:33:38.671070932Z I1117 09:33:38.671042       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:33:39.075091640Z I1117 09:33:39.075026       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-7 -n openshift-etcd because it was missing
2025-11-17T09:33:39.475636979Z I1117 09:33:39.473705       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-pod -n openshift-etcd:
2025-11-17T09:33:39.475636979Z cause by changes in data.pod.yaml
2025-11-17T09:33:39.866554212Z I1117 09:33:39.866498       1 request.go:697] Waited for 1.397324006s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-XXXXXX1
2025-11-17T09:33:40.471589341Z I1117 09:33:40.471530       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-7 -n openshift-etcd because it was missing
2025-11-17T09:33:41.067088510Z I1117 09:33:41.066974       1 request.go:697] Waited for 1.397711324s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:33:41.270868878Z I1117 09:33:41.270821       1 installer_controller.go:512] "XXXXXX1" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 4
2025-11-17T09:33:41.871225503Z I1117 09:33:41.871173       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-7 -n openshift-etcd because it was missing
2025-11-17T09:33:42.266215417Z I1117 09:33:42.266176       1 request.go:697] Waited for 1.396789668s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:33:42.273425073Z I1117 09:33:42.273371       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/restore-etcd-pod -n openshift-etcd:
2025-11-17T09:33:42.273425073Z cause by changes in data.pod.yaml
2025-11-17T09:33:43.076106055Z I1117 09:33:43.076037       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-7 -n openshift-etcd because it was missing
2025-11-17T09:33:43.266435252Z I1117 09:33:43.266388       1 request.go:697] Waited for 1.197521973s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:33:43.669853435Z I1117 09:33:43.669806       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:33:43.872533579Z I1117 09:33:43.872467       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-7 -n openshift-etcd because it was missing
2025-11-17T09:33:44.672191018Z I1117 09:33:44.672121       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-7 -n openshift-etcd because it was missing
2025-11-17T09:33:45.069936887Z I1117 09:33:45.069877       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:33:45.473640906Z I1117 09:33:45.473571       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-7 -n openshift-etcd because it was missing
2025-11-17T09:33:45.482309940Z I1117 09:33:45.482245       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 6 created because required secret/etcd-all-certs has changed
2025-11-17T09:33:45.482536399Z I1117 09:33:45.482511       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:45.488986175Z I1117 09:33:45.488949       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 8 triggered by "required configmap/etcd-pod has changed"
2025-11-17T09:33:45.536977262Z I1117 09:33:45.536934       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.02 %, dbSize: 73879552
2025-11-17T09:33:45.536977262Z I1117 09:33:45.536953       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 73990144
2025-11-17T09:33:45.560023563Z I1117 09:33:45.559986       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:45.560150621Z I1117 09:33:45.560133       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:45.573536279Z I1117 09:33:45.573482       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady ([container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes \"XXXXXX0\" not found])\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:33:45.639640919Z I1117 09:33:45.639591       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 73924608
2025-11-17T09:33:46.666830860Z I1117 09:33:46.666752       1 request.go:697] Waited for 1.183731181s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:33:46.869971212Z I1117 09:33:46.869910       1 installer_controller.go:500] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:33:46.869971212Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:33:46.869971212Z  CurrentRevision: (int32) 4,
2025-11-17T09:33:46.869971212Z  TargetRevision: (int32) 7,
2025-11-17T09:33:46.869971212Z  LastFailedRevision: (int32) 0,
2025-11-17T09:33:46.869971212Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:33:46.869971212Z  LastFailedReason: (string) "",
2025-11-17T09:33:46.869971212Z  LastFailedCount: (int) 0,
2025-11-17T09:33:46.869971212Z  LastFallbackCount: (int) 0,
2025-11-17T09:33:46.869971212Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:33:46.869971212Z }
2025-11-17T09:33:46.869971212Z  because new revision pending
2025-11-17T09:33:46.880311998Z I1117 09:33:46.880244       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:46.882110656Z I1117 09:33:46.880574       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:46.890708589Z I1117 09:33:46.890673       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6" to "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 7",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6\nEtcdMembersAvailable: 2 members are available" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 2 members are available"
2025-11-17T09:33:46.941748769Z I1117 09:33:46.941692       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 74252288
2025-11-17T09:33:47.071666110Z I1117 09:33:47.071570       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-8 -n openshift-etcd because it was missing
2025-11-17T09:33:47.866651151Z I1117 09:33:47.866612       1 request.go:697] Waited for 1.596660363s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2025-11-17T09:33:48.673614620Z I1117 09:33:48.672677       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-8 -n openshift-etcd because it was missing
2025-11-17T09:33:49.066965563Z I1117 09:33:49.066816       1 request.go:697] Waited for 1.589128806s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX1
2025-11-17T09:33:50.074521818Z I1117 09:33:50.074466       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-7-XXXXXX1 -n openshift-etcd because it was missing
2025-11-17T09:33:50.266922090Z I1117 09:33:50.266859       1 request.go:697] Waited for 1.594180712s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps
2025-11-17T09:33:50.272226755Z I1117 09:33:50.272153       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-8 -n openshift-etcd because it was missing
2025-11-17T09:33:51.467193372Z I1117 09:33:51.467034       1 request.go:697] Waited for 1.392574889s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-XXXXXX1
2025-11-17T09:33:51.471206687Z I1117 09:33:51.471167       1 installer_controller.go:512] "XXXXXX1" is in transition to 7, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:33:51.872662085Z I1117 09:33:51.872583       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-8 -n openshift-etcd because it was missing
2025-11-17T09:33:52.666057236Z I1117 09:33:52.665996       1 request.go:697] Waited for 1.396954905s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:33:53.271962633Z I1117 09:33:53.271911       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-8 -n openshift-etcd because it was missing
2025-11-17T09:33:53.669313839Z I1117 09:33:53.669253       1 request.go:697] Waited for 1.400355099s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:33:53.675282664Z I1117 09:33:53.674651       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:33:54.268664950Z I1117 09:33:54.268608       1 installer_controller.go:512] "XXXXXX1" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:33:54.670990102Z I1117 09:33:54.670929       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-8 -n openshift-etcd because it was missing
2025-11-17T09:33:54.866270677Z I1117 09:33:54.866142       1 request.go:697] Waited for 1.39652903s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:33:55.870078874Z I1117 09:33:55.870025       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-8 -n openshift-etcd because it was missing
2025-11-17T09:33:56.066216004Z I1117 09:33:56.066150       1 request.go:697] Waited for 1.196370837s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:33:56.871775169Z I1117 09:33:56.871696       1 installer_controller.go:512] "XXXXXX1" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:33:57.067031970Z I1117 09:33:57.066975       1 request.go:697] Waited for 1.315199097s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:33:57.274763277Z I1117 09:33:57.274712       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-8 -n openshift-etcd because it was missing
2025-11-17T09:33:57.287971308Z I1117 09:33:57.287925       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:57.289849525Z I1117 09:33:57.289806       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 7 created because required configmap/etcd-pod has changed
2025-11-17T09:33:57.342714803Z I1117 09:33:57.342661       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 74981376
2025-11-17T09:33:57.342714803Z I1117 09:33:57.342676       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.02 %, dbSize: 75059200
2025-11-17T09:33:57.687438343Z E1117 09:33:57.687387       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:33:58.466254441Z I1117 09:33:58.466138       1 request.go:697] Waited for 1.177918371s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:33:59.070401370Z I1117 09:33:59.069393       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:33:59.467344810Z I1117 09:33:59.467082       1 request.go:697] Waited for 1.397106147s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:33:59.873176412Z I1117 09:33:59.872736       1 installer_controller.go:500] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:33:59.873176412Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:33:59.873176412Z  CurrentRevision: (int32) 4,
2025-11-17T09:33:59.873176412Z  TargetRevision: (int32) 8,
2025-11-17T09:33:59.873176412Z  LastFailedRevision: (int32) 0,
2025-11-17T09:33:59.873176412Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:33:59.873176412Z  LastFailedReason: (string) "",
2025-11-17T09:33:59.873176412Z  LastFailedCount: (int) 0,
2025-11-17T09:33:59.873176412Z  LastFallbackCount: (int) 0,
2025-11-17T09:33:59.873176412Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:33:59.873176412Z }
2025-11-17T09:33:59.873176412Z  because new revision pending
2025-11-17T09:33:59.885269335Z I1117 09:33:59.885165       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:33:59.885269335Z I1117 09:33:59.885218       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:33:59.894309646Z I1117 09:33:59.894221       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 7" to "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 2 members are available" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 members are available"
2025-11-17T09:34:00.667951153Z I1117 09:34:00.666249       1 request.go:697] Waited for 1.194826689s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:34:01.667263609Z I1117 09:34:01.667122       1 request.go:697] Waited for 1.593169133s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:34:02.676917443Z I1117 09:34:02.676792       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-XXXXXX1 -n openshift-etcd because it was missing
2025-11-17T09:34:02.866825065Z I1117 09:34:02.866736       1 request.go:697] Waited for 1.196740645s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:34:04.067066535Z I1117 09:34:04.067001       1 request.go:697] Waited for 1.390331903s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX1
2025-11-17T09:34:04.070070994Z I1117 09:34:04.069831       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:34:04.469277478Z I1117 09:34:04.469227       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:05.266988634Z I1117 09:34:05.266924       1 request.go:697] Waited for 1.195974953s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX1
2025-11-17T09:34:06.466599206Z I1117 09:34:06.466534       1 request.go:697] Waited for 1.197238265s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX1
2025-11-17T09:34:06.469374094Z I1117 09:34:06.469201       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:34:08.069481006Z I1117 09:34:08.069416       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:34:08.269941570Z I1117 09:34:08.269793       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:08.273010754Z I1117 09:34:08.272979       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:08.273010754Z E1117 09:34:08.272997       1 guard_controller.go:277] Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:34:09.669319958Z I1117 09:34:09.669253       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:34:09.879896747Z E1117 09:34:09.879850       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:34:09.880262472Z I1117 09:34:09.880231       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:34:09.883809405Z I1117 09:34:09.883773       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nGuardControllerDegraded: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:34:09.890860534Z E1117 09:34:09.890830       1 guard_controller.go:277] Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:34:09.893698155Z I1117 09:34:09.893658       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nGuardControllerDegraded: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:34:09.946937237Z I1117 09:34:09.946903       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 75722752
2025-11-17T09:34:11.066659052Z I1117 09:34:11.066570       1 request.go:697] Waited for 1.176556133s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX1
2025-11-17T09:34:12.066893854Z I1117 09:34:12.066843       1 request.go:697] Waited for 1.397329626s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:34:12.268256799Z I1117 09:34:12.268198       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:34:12.468828875Z I1117 09:34:12.468779       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:12.469062660Z E1117 09:34:12.469042       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:34:12.481602874Z E1117 09:34:12.481558       1 guard_controller.go:277] Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:34:13.266481020Z I1117 09:34:13.266331       1 request.go:697] Waited for 1.195209065s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:34:14.466042088Z I1117 09:34:14.465999       1 request.go:697] Waited for 1.196020986s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:34:14.669682708Z I1117 09:34:14.669621       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:14.669922981Z E1117 09:34:14.669870       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:34:15.070615969Z I1117 09:34:15.070555       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:34:16.469243331Z I1117 09:34:16.469158       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:16.480749057Z I1117 09:34:16.480705       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:34:16.481061377Z I1117 09:34:16.481038       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:34:16.491229131Z I1117 09:34:16.489440       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nGuardControllerDegraded: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:34:16.585257035Z I1117 09:34:16.585206       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 75829248
2025-11-17T09:34:16.585257035Z I1117 09:34:16.585222       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 75952128
2025-11-17T09:34:16.652011209Z E1117 09:34:16.651945       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:34:17.666463215Z I1117 09:34:17.666382       1 request.go:697] Waited for 1.18529836s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2025-11-17T09:34:18.666780360Z I1117 09:34:18.666563       1 request.go:697] Waited for 1.395845735s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX1
2025-11-17T09:34:19.469179933Z I1117 09:34:19.469130       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:34:19.867178548Z I1117 09:34:19.866962       1 request.go:697] Waited for 1.397444359s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:34:20.869183106Z I1117 09:34:20.869126       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:24.270258460Z I1117 09:34:24.270125       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:34:24.469113115Z I1117 09:34:24.469067       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:25.670373389Z I1117 09:34:25.670314       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:34:27.069629825Z I1117 09:34:27.069572       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:38.655514748Z E1117 09:34:38.655460       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:35:16.680540921Z E1117 09:35:16.678443       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:35:46.647134686Z W1117 09:35:46.646973       1 etcdcli_pool.go:94] cached client considered unhealthy after 0 tries, trying again. Err: error during cache client health connection check: context deadline exceeded
2025-11-17T09:35:46.647134686Z I1117 09:35:46.646996       1 etcdcli_pool.go:157] closing cached client
2025-11-17T09:35:46.749564111Z W1117 09:35:46.749482       1 etcdcli_pool.go:94] cached client considered unhealthy after 0 tries, trying again. Err: error during cache client health connection check: context deadline exceeded
2025-11-17T09:35:46.749564111Z I1117 09:35:46.749529       1 etcdcli_pool.go:157] closing cached client
2025-11-17T09:35:47.349152983Z W1117 09:35:47.349054       1 etcdcli_pool.go:94] cached client considered unhealthy after 0 tries, trying again. Err: error during cache client health connection check: context deadline exceeded
2025-11-17T09:35:47.349152983Z I1117 09:35:47.349075       1 etcdcli_pool.go:157] closing cached client
2025-11-17T09:35:47.748449340Z W1117 09:35:47.748223       1 etcdcli_pool.go:94] cached client considered unhealthy after 0 tries, trying again. Err: error during cache client health connection check: context deadline exceeded
2025-11-17T09:35:47.748449340Z I1117 09:35:47.748245       1 etcdcli_pool.go:157] closing cached client
2025-11-17T09:35:48.647992490Z I1117 09:35:48.647782       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:35:48.750763690Z I1117 09:35:48.750552       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:35:49.350017549Z I1117 09:35:49.349905       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:35:49.749251404Z I1117 09:35:49.749143       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:36:00.658599932Z E1117 09:36:00.658495       1 leaderelection.go:332] error retrieving resource lock openshift-etcd-operator/openshift-cluster-etcd-operator-lock: the server was unable to return a response in the time allotted, but may still be processing the request (get leases.coordination.k8s.io openshift-cluster-etcd-operator-lock)
2025-11-17T09:36:02.661502713Z I1117 09:36:02.661170       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:36:02.668653713Z I1117 09:36:02.667009       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:36:02.783791889Z I1117 09:36:02.783687       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 77565952
2025-11-17T09:36:02.783791889Z I1117 09:36:02.783705       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 77697024
2025-11-17T09:36:04.372467501Z I1117 09:36:04.372417       1 request.go:697] Waited for 1.106595324s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces?resourceVersion=21051
2025-11-17T09:36:05.570017147Z I1117 09:36:05.569959       1 request.go:697] Waited for 1.425765323s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:36:06.570093435Z I1117 09:36:06.570039       1 request.go:697] Waited for 1.596245462s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:36:07.373531523Z I1117 09:36:07.373475       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:36:07.570625230Z I1117 09:36:07.570564       1 request.go:697] Waited for 1.395177924s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:36:07.972895226Z I1117 09:36:07.972839       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:09.572888723Z I1117 09:36:09.572035       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:36:11.173256490Z I1117 09:36:11.173188       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:16.654905888Z E1117 09:36:16.654849       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:36:19.577586214Z I1117 09:36:19.577526       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 6
2025-11-17T09:36:19.960321284Z I1117 09:36:19.958247       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:22.754238892Z I1117 09:36:22.754074       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 6
2025-11-17T09:36:22.946376482Z I1117 09:36:22.946326       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:24.744419813Z I1117 09:36:24.744369       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 6
2025-11-17T09:36:25.145159013Z I1117 09:36:25.145111       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:37.263629896Z I1117 09:36:37.263580       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:37.266298087Z I1117 09:36:37.266251       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:40.235093774Z I1117 09:36:40.235039       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:44.559732608Z I1117 09:36:44.559684       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:36:44.663414465Z I1117 09:36:44.663372       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 80519168
2025-11-17T09:36:44.663414465Z I1117 09:36:44.663388       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 80613376
2025-11-17T09:36:45.723599805Z I1117 09:36:45.722411       1 request.go:697] Waited for 1.162347217s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX1
2025-11-17T09:36:46.723011814Z I1117 09:36:46.722966       1 request.go:697] Waited for 1.783967171s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:36:47.723108961Z I1117 09:36:47.723060       1 request.go:697] Waited for 2.580041363s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps?resourceVersion=21100
2025-11-17T09:36:48.723553414Z I1117 09:36:48.723506       1 request.go:697] Waited for 3.186475702s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/endpoints?resourceVersion=21068
2025-11-17T09:36:49.922931747Z I1117 09:36:49.922873       1 request.go:697] Waited for 1.993270611s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:36:50.331343028Z I1117 09:36:50.330436       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 6
2025-11-17T09:36:53.294435353Z I1117 09:36:53.294390       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:55.924621242Z I1117 09:36:55.924570       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:58.864731258Z I1117 09:36:58.864684       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:59.453173211Z I1117 09:36:59.453110       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:14.590847851Z I1117 09:37:14.590749       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:15.908953629Z I1117 09:37:15.908893       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:18.668357293Z E1117 09:37:18.668299       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:37:19.097636673Z I1117 09:37:19.097596       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:20.292401782Z I1117 09:37:20.292345       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:22.093014985Z I1117 09:37:22.092942       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:22.500368163Z E1117 09:37:22.500296       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:37:23.892661428Z I1117 09:37:23.892601       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:24.691995790Z I1117 09:37:24.691944       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:25.492379963Z I1117 09:37:25.492315       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:27.293012816Z I1117 09:37:27.292957       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:29.127965538Z I1117 09:37:29.127912       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:39.625411543Z I1117 09:37:39.625267       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:37:40.222234881Z I1117 09:37:40.222095       1 request.go:697] Waited for 1.153849881s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:37:41.222175871Z I1117 09:37:41.222100       1 request.go:697] Waited for 1.196652183s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:37:41.225073074Z I1117 09:37:41.225035       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:41.227628278Z E1117 09:37:41.227603       1 guard_controller.go:277] Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:37:42.644727946Z I1117 09:37:42.643956       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:37:42.882077572Z I1117 09:37:42.882003       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:42.945869017Z I1117 09:37:42.945799       1 prune_controller.go:269] Nothing to prune
2025-11-17T09:37:42.947201661Z I1117 09:37:42.947132       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nGuardControllerDegraded: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:37:42.951044047Z E1117 09:37:42.950985       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:37:43.040040362Z I1117 09:37:43.039955       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nGuardControllerDegraded: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:37:43.042808680Z I1117 09:37:43.041681       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:43.042808680Z E1117 09:37:43.041708       1 guard_controller.go:277] Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:37:43.213756793Z I1117 09:37:43.213612       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 85110784
2025-11-17T09:37:44.022051922Z I1117 09:37:44.021993       1 request.go:697] Waited for 1.075464544s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2025-11-17T09:37:45.022677344Z I1117 09:37:45.022611       1 request.go:697] Waited for 1.397586504s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX1
2025-11-17T09:37:46.016070113Z E1117 09:37:46.015994       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1
2025-11-17T09:37:46.222471061Z I1117 09:37:46.222412       1 request.go:697] Waited for 1.196465509s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:37:46.225741258Z I1117 09:37:46.225707       1 installer_controller.go:500] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:37:46.225741258Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:37:46.225741258Z  CurrentRevision: (int32) 8,
2025-11-17T09:37:46.225741258Z  TargetRevision: (int32) 0,
2025-11-17T09:37:46.225741258Z  LastFailedRevision: (int32) 0,
2025-11-17T09:37:46.225741258Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:37:46.225741258Z  LastFailedReason: (string) "",
2025-11-17T09:37:46.225741258Z  LastFailedCount: (int) 0,
2025-11-17T09:37:46.225741258Z  LastFallbackCount: (int) 0,
2025-11-17T09:37:46.225741258Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:37:46.225741258Z }
2025-11-17T09:37:46.225741258Z  because static pod is ready
2025-11-17T09:37:46.266951889Z I1117 09:37:46.266885       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX1" from revision 4 to 8 because static pod is ready
2025-11-17T09:37:46.269370322Z I1117 09:37:46.269321       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nGuardControllerDegraded: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:37:46.336062679Z I1117 09:37:46.334376       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8" to "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 4; 1 nodes are at revision 6; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 members are available" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available"
2025-11-17T09:37:47.223045346Z I1117 09:37:47.222985       1 request.go:697] Waited for 1.156152352s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:37:48.422770328Z I1117 09:37:48.422680       1 request.go:697] Waited for 1.493533337s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:37:49.561229848Z I1117 09:37:49.561138       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-XXXXXX2 -n openshift-etcd because it was missing
2025-11-17T09:37:49.822189323Z I1117 09:37:49.822131       1 request.go:697] Waited for 1.106060397s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:37:50.830843718Z I1117 09:37:50.830771       1 installer_controller.go:500] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:37:50.830843718Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:37:50.830843718Z  CurrentRevision: (int32) 8,
2025-11-17T09:37:50.830843718Z  TargetRevision: (int32) 0,
2025-11-17T09:37:50.830843718Z  LastFailedRevision: (int32) 0,
2025-11-17T09:37:50.830843718Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:37:50.830843718Z  LastFailedReason: (string) "",
2025-11-17T09:37:50.830843718Z  LastFailedCount: (int) 0,
2025-11-17T09:37:50.830843718Z  LastFallbackCount: (int) 0,
2025-11-17T09:37:50.830843718Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:37:50.830843718Z }
2025-11-17T09:37:50.830843718Z  because static pod is ready
2025-11-17T09:37:51.022547559Z I1117 09:37:51.022479       1 request.go:697] Waited for 1.295164576s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:37:51.025575374Z I1117 09:37:51.025542       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:51.038791820Z I1117 09:37:51.038745       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:37:51.046995679Z I1117 09:37:51.046872       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nGuardControllerDegraded: Missing PodIP in operand etcd-XXXXXX1 on node XXXXXX1\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:37:51.830533629Z I1117 09:37:51.829159       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-XXXXXX1 -n openshift-etcd because it was missing
2025-11-17T09:37:52.022804257Z I1117 09:37:52.022740       1 request.go:697] Waited for 1.39753847s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2025-11-17T09:37:53.222192477Z I1117 09:37:53.222131       1 request.go:697] Waited for 1.795963068s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:37:54.421799216Z I1117 09:37:54.421722       1 request.go:697] Waited for 1.596068868s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX1
2025-11-17T09:37:55.227960159Z I1117 09:37:55.227884       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:37:55.422028373Z I1117 09:37:55.421977       1 request.go:697] Waited for 1.588130955s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:37:55.825129895Z I1117 09:37:55.825057       1 installer_controller.go:524] node XXXXXX0 static pod not found and needs new revision 8
2025-11-17T09:37:55.825129895Z I1117 09:37:55.825103       1 installer_controller.go:532] "XXXXXX0" moving to (v1.NodeStatus) {
2025-11-17T09:37:55.825129895Z  NodeName: (string) (len=7) "XXXXXX0",
2025-11-17T09:37:55.825129895Z  CurrentRevision: (int32) 0,
2025-11-17T09:37:55.825129895Z  TargetRevision: (int32) 8,
2025-11-17T09:37:55.825129895Z  LastFailedRevision: (int32) 0,
2025-11-17T09:37:55.825129895Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:37:55.825129895Z  LastFailedReason: (string) "",
2025-11-17T09:37:55.825129895Z  LastFailedCount: (int) 0,
2025-11-17T09:37:55.825129895Z  LastFallbackCount: (int) 0,
2025-11-17T09:37:55.825129895Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:37:55.825129895Z }
2025-11-17T09:37:55.834208759Z I1117 09:37:55.834117       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "XXXXXX0" from revision 0 to 8 because node XXXXXX0 static pod not found
2025-11-17T09:37:56.422631470Z I1117 09:37:56.422571       1 request.go:697] Waited for 1.596319775s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:37:57.622556796Z I1117 09:37:57.622401       1 request.go:697] Waited for 1.786885023s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX0
2025-11-17T09:37:57.824721231Z I1117 09:37:57.824666       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:58.822728111Z I1117 09:37:58.822648       1 request.go:697] Waited for 1.797471779s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2025-11-17T09:37:59.230445714Z I1117 09:37:59.230357       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:37:59.822774072Z I1117 09:37:59.822722       1 request.go:697] Waited for 1.504943365s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:38:00.825801243Z I1117 09:38:00.825740       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:01.022740105Z I1117 09:38:01.022636       1 request.go:697] Waited for 1.597799622s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX1
2025-11-17T09:38:02.221846896Z I1117 09:38:02.221787       1 request.go:697] Waited for 1.394908459s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX0
2025-11-17T09:38:03.225943060Z I1117 09:38:03.224986       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:03.424067499Z I1117 09:38:03.424017       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:04.222072873Z I1117 09:38:04.222012       1 request.go:697] Waited for 1.071997941s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:38:06.037016397Z I1117 09:38:06.036956       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:06.039695860Z I1117 09:38:06.039622       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:07.374188403Z I1117 09:38:07.374050       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:09.426159345Z I1117 09:38:09.426091       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:11.225274157Z I1117 09:38:11.225221       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:13.734023408Z I1117 09:38:13.733957       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:16.657109229Z E1117 09:38:16.657048       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2025-11-17T09:38:21.718401937Z I1117 09:38:21.713494       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:38:21.913216662Z I1117 09:38:21.913132       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:38:21.913687832Z I1117 09:38:21.913647       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:38:22.439946310Z E1117 09:38:22.437003       1 base_controller.go:268] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:38:23.070341336Z I1117 09:38:23.070274       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:23.537748129Z I1117 09:38:23.537532       1 request.go:697] Waited for 1.1044996s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:38:24.551771886Z E1117 09:38:24.551714       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:24.564152818Z E1117 09:38:24.564103       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:24.567638002Z I1117 09:38:24.567564       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nGuardControllerDegraded: Missing operand on node XXXXXX0\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:38:24.576917036Z I1117 09:38:24.576841       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All XXXXXX nodes are ready\nGuardControllerDegraded: Missing operand on node XXXXXX0\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:38:25.737817218Z I1117 09:38:25.737762       1 request.go:697] Waited for 1.171067925s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:38:26.739272286Z I1117 09:38:26.739199       1 request.go:697] Waited for 1.59721792s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:38:27.340230484Z I1117 09:38:27.340170       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:27.937766319Z I1117 09:38:27.937703       1 request.go:697] Waited for 1.397095428s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2025-11-17T09:38:28.937891059Z I1117 09:38:28.937843       1 request.go:697] Waited for 1.196236303s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:38:29.340071423Z E1117 09:38:29.339928       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:29.340310307Z E1117 09:38:29.340275       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:30.137794671Z I1117 09:38:30.137720       1 request.go:697] Waited for 1.138240027s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX0
2025-11-17T09:38:31.140683851Z I1117 09:38:31.140606       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:31.337910167Z I1117 09:38:31.337774       1 request.go:697] Waited for 1.060895517s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:38:32.742230374Z E1117 09:38:32.742155       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:32.742607088Z E1117 09:38:32.742571       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:35.140792779Z I1117 09:38:35.140633       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:35.340005984Z E1117 09:38:35.339950       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:35.340259241Z E1117 09:38:35.340241       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:35.342711030Z E1117 09:38:35.342686       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:36.540776646Z E1117 09:38:36.540718       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:37.419947946Z E1117 09:38:37.419905       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:37.420426956Z E1117 09:38:37.420275       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:38.140404609Z E1117 09:38:38.140338       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:38.140662546Z E1117 09:38:38.140644       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:38.740172060Z E1117 09:38:38.740119       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:39.545084407Z E1117 09:38:39.545031       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:39.548159319Z E1117 09:38:39.548121       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:40.738176882Z I1117 09:38:40.738124       1 request.go:697] Waited for 1.186273679s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX0
2025-11-17T09:38:41.937480365Z I1117 09:38:41.937409       1 request.go:697] Waited for 1.19674171s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX0
2025-11-17T09:38:41.999916410Z I1117 09:38:41.999796       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:38:42.937725023Z I1117 09:38:42.937671       1 request.go:697] Waited for 1.190535911s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:38:43.938396273Z I1117 09:38:43.938193       1 request.go:697] Waited for 1.19721224s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:38:43.941732334Z E1117 09:38:43.941684       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:44.340521123Z I1117 09:38:44.340467       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:38:45.538042140Z I1117 09:38:45.537985       1 request.go:697] Waited for 1.013148326s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:38:46.141465126Z I1117 09:38:46.140863       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:38:47.140051999Z E1117 09:38:47.139999       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:47.140474676Z E1117 09:38:47.140279       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:48.140654209Z E1117 09:38:48.140516       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:48.140825512Z E1117 09:38:48.140802       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:49.017931811Z E1117 09:38:49.017869       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:49.018185185Z E1117 09:38:49.018123       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:50.741108288Z E1117 09:38:50.741061       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:50.741433436Z E1117 09:38:50.741419       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:52.340063973Z E1117 09:38:52.339991       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:52.340294768Z E1117 09:38:52.340263       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:55.540461432Z E1117 09:38:55.540380       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:55.540710896Z E1117 09:38:55.540695       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:57.148340543Z E1117 09:38:57.144983       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:57.148340543Z E1117 09:38:57.145299       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:58.340529424Z E1117 09:38:58.340469       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:58.340734446Z E1117 09:38:58.340718       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:59.025096659Z E1117 09:38:59.025035       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:00.540029639Z E1117 09:39:00.539979       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:04.654408190Z E1117 09:39:04.654357       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:04.654575009Z E1117 09:39:04.654559       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:05.237013749Z E1117 09:39:05.236956       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:05.237274103Z E1117 09:39:05.237250       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:10.088718316Z E1117 09:39:10.088672       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:10.089014247Z E1117 09:39:10.089000       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:11.237900697Z E1117 09:39:11.237833       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:11.238150751Z E1117 09:39:11.238125       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:11.743342299Z I1117 09:39:11.741416       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:11.787448478Z I1117 09:39:11.787404       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:12.029483602Z I1117 09:39:12.029430       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:39:12.787564538Z I1117 09:39:12.787505       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:12.829401520Z I1117 09:39:12.829328       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:13.031267734Z E1117 09:39:13.031196       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:13.031607377Z E1117 09:39:13.031489       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:13.827374657Z I1117 09:39:13.827261       1 request.go:697] Waited for 1.045284705s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:39:13.909543362Z I1117 09:39:13.909490       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:14.742862769Z I1117 09:39:14.742726       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:15.027113216Z I1117 09:39:15.027056       1 request.go:697] Waited for 1.196936803s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:39:15.831999813Z I1117 09:39:15.831924       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:16.654070798Z E1117 09:39:16.654011       1 base_controller.go:268] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp 172.30.11.168:9091: connect: connection refused
2025-11-17T09:39:16.787384131Z I1117 09:39:16.787337       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:17.827303759Z I1117 09:39:17.827222       1 request.go:697] Waited for 1.07570499s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:39:19.026805101Z I1117 09:39:19.026740       1 request.go:697] Waited for 1.196211865s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:39:19.285173165Z I1117 09:39:19.285126       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:20.274974562Z I1117 09:39:20.274923       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:21.026648426Z I1117 09:39:21.026588       1 request.go:697] Waited for 1.08728807s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX0
2025-11-17T09:39:21.436466137Z I1117 09:39:21.436378       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/etcd-guard-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:39:21.450924706Z I1117 09:39:21.450870       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:39:21.461996221Z I1117 09:39:21.459781       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nGuardControllerDegraded: Missing operand on node XXXXXX0\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:39:21.488911150Z I1117 09:39:21.488865       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:21.527139831Z I1117 09:39:21.527099       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:21.833332795Z I1117 09:39:21.833256       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:22.626630760Z I1117 09:39:22.626546       1 request.go:697] Waited for 1.177849869s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:39:22.629928766Z I1117 09:39:22.629865       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:22.805930881Z I1117 09:39:22.805878       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:23.410498868Z I1117 09:39:23.410426       1 clustermembercontroller.go:374] Skipping etcd-XXXXXX0 as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2025-11-17T09:39:23.799423038Z I1117 09:39:23.799322       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MemberAddAsLearner' successfully added new member https://XXXXXXXXXXX:2380
2025-11-17T09:39:23.818051718Z I1117 09:39:23.817990       1 clustermembercontroller.go:293] Not ready for promotion: etcd learner member (https://XXXXXXXXXXX:2380) is not yet in sync with leader's log 
2025-11-17T09:39:23.830470846Z I1117 09:39:23.830419       1 request.go:697] Waited for 1.801227732s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:39:23.833527308Z I1117 09:39:23.833492       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:23.845635417Z W1117 09:39:23.845597       1 etcdcli.go:346] UnstartedEtcdMember found: [NAME-PENDING-XXXXXXXXXXX]
2025-11-17T09:39:23.845635417Z W1117 09:39:23.845610       1 etcdcli.go:351] UnhealthyEtcdMember found: [NAME-PENDING-XXXXXXXXXXX]
2025-11-17T09:39:25.027151518Z I1117 09:39:25.027024       1 request.go:697] Waited for 1.5968601s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:39:26.226463943Z I1117 09:39:26.226403       1 request.go:697] Waited for 1.39620192s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-8-XXXXXX0
2025-11-17T09:39:27.226912121Z I1117 09:39:27.226854       1 request.go:697] Waited for 1.155754931s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:39:27.830235960Z I1117 09:39:27.830182       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:29.436309775Z I1117 09:39:29.436185       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodUpdated' Updated Pod/etcd-guard-XXXXXX0 -n openshift-etcd because it changed
2025-11-17T09:39:31.228631512Z I1117 09:39:31.228475       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MemberPromote' successfully promoted learner member https://XXXXXXXXXXX:2380
2025-11-17T09:39:31.229597079Z I1117 09:39:31.229573       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:34.041229875Z I1117 09:39:34.041161       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:41.761830654Z I1117 09:39:41.761781       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:42.948846845Z I1117 09:39:42.948780       1 request.go:697] Waited for 1.162018962s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:39:44.551476370Z I1117 09:39:44.551405       1 installer_controller.go:500] "XXXXXX0" moving to (v1.NodeStatus) {
2025-11-17T09:39:44.551476370Z  NodeName: (string) (len=7) "XXXXXX0",
2025-11-17T09:39:44.551476370Z  CurrentRevision: (int32) 8,
2025-11-17T09:39:44.551476370Z  TargetRevision: (int32) 0,
2025-11-17T09:39:44.551476370Z  LastFailedRevision: (int32) 0,
2025-11-17T09:39:44.551476370Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:39:44.551476370Z  LastFailedReason: (string) "",
2025-11-17T09:39:44.551476370Z  LastFailedCount: (int) 0,
2025-11-17T09:39:44.551476370Z  LastFallbackCount: (int) 0,
2025-11-17T09:39:44.551476370Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:39:44.551476370Z }
2025-11-17T09:39:44.551476370Z  because static pod is ready
2025-11-17T09:39:44.561174748Z I1117 09:39:44.561098       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX0" from revision 0 to 8 because static pod is ready
2025-11-17T09:39:44.564344758Z I1117 09:39:44.564306       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:39:44.573532373Z I1117 09:39:44.573472       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8" to "NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available"
2025-11-17T09:39:44.582386869Z W1117 09:39:44.581822       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]] vs. [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]]
2025-11-17T09:39:44.589746555Z I1117 09:39:44.589689       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-endpoints -n openshift-etcd:
2025-11-17T09:39:44.589746555Z cause by changes in data.d2dd98de6e8ab051
2025-11-17T09:39:44.609519648Z I1117 09:39:44.609444       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 9 triggered by "required configmap/etcd-endpoints has changed"
2025-11-17T09:39:44.629531448Z W1117 09:39:44.629466       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]] vs. [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]]
2025-11-17T09:39:44.633964529Z W1117 09:39:44.633926       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]] vs. [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]]
2025-11-17T09:39:44.659929329Z W1117 09:39:44.659884       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]] vs. [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]]
2025-11-17T09:39:44.697421790Z I1117 09:39:44.697367       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.22 %, dbSize: 104108032
2025-11-17T09:39:44.697421790Z I1117 09:39:44.697387       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.17 %, dbSize: 103997440
2025-11-17T09:39:44.697421790Z I1117 09:39:44.697393       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.18 %, dbSize: 104116224
2025-11-17T09:39:45.748887727Z I1117 09:39:45.748835       1 request.go:697] Waited for 1.184726288s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:39:46.356913790Z I1117 09:39:46.356839       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-9 -n openshift-etcd because it was missing
2025-11-17T09:39:46.948734283Z I1117 09:39:46.948500       1 request.go:697] Waited for 1.796395411s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:39:47.952346525Z I1117 09:39:47.948681       1 request.go:697] Waited for 1.591843364s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps
2025-11-17T09:39:47.957678685Z I1117 09:39:47.957610       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-9 -n openshift-etcd because it was missing
2025-11-17T09:39:49.148803507Z I1117 09:39:49.148721       1 request.go:697] Waited for 1.598026635s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:39:49.554262346Z I1117 09:39:49.554203       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-9 -n openshift-etcd because it was missing
2025-11-17T09:39:50.348424913Z I1117 09:39:50.348267       1 request.go:697] Waited for 1.189386789s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:39:50.553769618Z I1117 09:39:50.553690       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-9 -n openshift-etcd because it was missing
2025-11-17T09:39:51.354960007Z I1117 09:39:51.353819       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-9 -n openshift-etcd because it was missing
2025-11-17T09:39:51.951755943Z I1117 09:39:51.951703       1 installer_controller.go:524] node XXXXXX2 with revision 6 is the oldest and needs new revision 8
2025-11-17T09:39:51.951799886Z I1117 09:39:51.951749       1 installer_controller.go:532] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:39:51.951799886Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:39:51.951799886Z  CurrentRevision: (int32) 6,
2025-11-17T09:39:51.951799886Z  TargetRevision: (int32) 8,
2025-11-17T09:39:51.951799886Z  LastFailedRevision: (int32) 0,
2025-11-17T09:39:51.951799886Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:39:51.951799886Z  LastFailedReason: (string) "",
2025-11-17T09:39:51.951799886Z  LastFailedCount: (int) 0,
2025-11-17T09:39:51.951799886Z  LastFallbackCount: (int) 0,
2025-11-17T09:39:51.951799886Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:39:51.951799886Z }
2025-11-17T09:39:51.971566040Z I1117 09:39:51.971515       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "XXXXXX2" from revision 6 to 8 because node XXXXXX2 with revision 6 is the oldest
2025-11-17T09:39:52.057574889Z I1117 09:39:52.057516       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.04 %, dbSize: 104726528
2025-11-17T09:39:52.057574889Z I1117 09:39:52.057534       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.02 %, dbSize: 104632320
2025-11-17T09:39:52.057574889Z I1117 09:39:52.057539       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.02 %, dbSize: 104734720
2025-11-17T09:39:52.154763487Z I1117 09:39:52.154124       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-9 -n openshift-etcd because it was missing
2025-11-17T09:39:53.148225722Z I1117 09:39:53.148164       1 request.go:697] Waited for 1.175048798s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:39:53.755365961Z I1117 09:39:53.755255       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-9 -n openshift-etcd because it was missing
2025-11-17T09:39:54.148596794Z I1117 09:39:54.148516       1 request.go:697] Waited for 1.596989646s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:39:54.155801494Z I1117 09:39:54.155731       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-scripts -n openshift-etcd:
2025-11-17T09:39:54.155801494Z cause by changes in data.etcd.env
2025-11-17T09:39:54.957178449Z I1117 09:39:54.957125       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-XXXXXX2 -n openshift-etcd because it was missing
2025-11-17T09:39:55.148599985Z I1117 09:39:55.148552       1 request.go:697] Waited for 1.596516365s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:39:55.382040788Z I1117 09:39:55.381578       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-9 -n openshift-etcd because it was missing
2025-11-17T09:39:55.395112295Z I1117 09:39:55.394884       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 8 created because required configmap/etcd-endpoints has changed
2025-11-17T09:39:55.497682989Z I1117 09:39:55.497630       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.02 %, dbSize: 104845312
2025-11-17T09:39:56.148891167Z I1117 09:39:56.148745       1 request.go:697] Waited for 1.597681103s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:39:56.751829334Z I1117 09:39:56.751700       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:39:56.767694191Z I1117 09:39:56.767624       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:39:56.775831581Z I1117 09:39:56.775241       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8" to "NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9",Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8\nEtcdMembersAvailable: 2 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9\nEtcdMembersAvailable: 2 members are available"
2025-11-17T09:39:56.853748202Z I1117 09:39:56.853685       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.01 %, dbSize: 104984576
2025-11-17T09:39:56.853748202Z I1117 09:39:56.853702       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 104882176
2025-11-17T09:39:56.853748202Z I1117 09:39:56.853706       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.02 %, dbSize: 104988672
2025-11-17T09:39:57.348619736Z I1117 09:39:57.348558       1 request.go:697] Waited for 1.797485639s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:39:57.362566547Z I1117 09:39:57.362086       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-pod -n openshift-etcd:
2025-11-17T09:39:57.362566547Z cause by changes in data.pod.yaml
2025-11-17T09:39:57.366909834Z I1117 09:39:57.366860       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 10 triggered by "required configmap/etcd-pod has changed"
2025-11-17T09:39:58.548263806Z I1117 09:39:58.548183       1 request.go:697] Waited for 1.78291696s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX2
2025-11-17T09:39:59.352626742Z I1117 09:39:59.352484       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-10 -n openshift-etcd because it was missing
2025-11-17T09:39:59.748762524Z I1117 09:39:59.748626       1 request.go:697] Waited for 1.997780925s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods
2025-11-17T09:39:59.756679018Z I1117 09:39:59.756605       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-9-XXXXXX2 -n openshift-etcd because it was missing
2025-11-17T09:40:00.555631172Z I1117 09:40:00.555230       1 installer_controller.go:500] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:40:00.555631172Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:40:00.555631172Z  CurrentRevision: (int32) 6,
2025-11-17T09:40:00.555631172Z  TargetRevision: (int32) 9,
2025-11-17T09:40:00.555631172Z  LastFailedRevision: (int32) 0,
2025-11-17T09:40:00.555631172Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:40:00.555631172Z  LastFailedReason: (string) "",
2025-11-17T09:40:00.555631172Z  LastFailedCount: (int) 0,
2025-11-17T09:40:00.555631172Z  LastFallbackCount: (int) 0,
2025-11-17T09:40:00.555631172Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:40:00.555631172Z }
2025-11-17T09:40:00.555631172Z  because new revision pending
2025-11-17T09:40:00.676724360Z I1117 09:40:00.675474       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.02 %, dbSize: 105488384
2025-11-17T09:40:00.748803343Z I1117 09:40:00.748760       1 request.go:697] Waited for 1.99798974s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX0
2025-11-17T09:40:00.955212447Z I1117 09:40:00.955146       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/restore-etcd-pod -n openshift-etcd:
2025-11-17T09:40:00.955212447Z cause by changes in data.pod.yaml
2025-11-17T09:40:01.159379098Z I1117 09:40:01.159310       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-10 -n openshift-etcd because it was missing
2025-11-17T09:40:01.949354551Z I1117 09:40:01.949062       1 request.go:697] Waited for 1.798034133s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2025-11-17T09:40:03.148885290Z I1117 09:40:03.148814       1 request.go:697] Waited for 1.989508889s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps
2025-11-17T09:40:03.156326531Z I1117 09:40:03.156257       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-10 -n openshift-etcd because it was missing
2025-11-17T09:40:03.555868986Z I1117 09:40:03.555567       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-9-XXXXXX1 -n openshift-etcd because it was missing
2025-11-17T09:40:04.348733029Z I1117 09:40:04.348667       1 request.go:697] Waited for 1.805944746s due to client-side throttling, not priority and fairness, request: DELETE:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-XXXXXX2
2025-11-17T09:40:04.351612794Z I1117 09:40:04.351559       1 installer_controller.go:500] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:40:04.351612794Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:40:04.351612794Z  CurrentRevision: (int32) 6,
2025-11-17T09:40:04.351612794Z  TargetRevision: (int32) 9,
2025-11-17T09:40:04.351612794Z  LastFailedRevision: (int32) 0,
2025-11-17T09:40:04.351612794Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:40:04.351612794Z  LastFailedReason: (string) "",
2025-11-17T09:40:04.351612794Z  LastFailedCount: (int) 0,
2025-11-17T09:40:04.351612794Z  LastFallbackCount: (int) 0,
2025-11-17T09:40:04.351612794Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:40:04.351612794Z }
2025-11-17T09:40:04.351612794Z  because new revision pending
2025-11-17T09:40:04.953851644Z I1117 09:40:04.953800       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-10 -n openshift-etcd because it was missing
2025-11-17T09:40:05.547917837Z I1117 09:40:05.547859       1 request.go:697] Waited for 1.789378779s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:40:06.548184622Z I1117 09:40:06.548118       1 request.go:697] Waited for 1.796290926s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:40:06.755017081Z I1117 09:40:06.754941       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-10 -n openshift-etcd because it was missing
2025-11-17T09:40:06.955591499Z I1117 09:40:06.955524       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-9-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:40:07.548534073Z I1117 09:40:07.548476       1 request.go:697] Waited for 1.594131015s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:40:07.754487108Z I1117 09:40:07.754427       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-9-XXXXXX2 -n openshift-etcd because it was missing
2025-11-17T09:40:08.352959955Z I1117 09:40:08.352819       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-10 -n openshift-etcd because it was missing
2025-11-17T09:40:08.748120915Z I1117 09:40:08.748068       1 request.go:697] Waited for 1.576374926s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:40:09.352205763Z I1117 09:40:09.352124       1 installer_controller.go:512] "XXXXXX2" is in transition to 9, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:40:09.748853084Z I1117 09:40:09.748807       1 request.go:697] Waited for 1.596901774s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:40:09.952479897Z I1117 09:40:09.952413       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-10 -n openshift-etcd because it was missing
2025-11-17T09:40:10.953990987Z I1117 09:40:10.949395       1 request.go:697] Waited for 1.594123964s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-9-XXXXXX2
2025-11-17T09:40:11.556008187Z I1117 09:40:11.555933       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-10 -n openshift-etcd because it was missing
2025-11-17T09:40:11.565114825Z I1117 09:40:11.565040       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 9 created because required configmap/etcd-pod has changed
2025-11-17T09:40:11.580321994Z I1117 09:40:11.576124       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 10 triggered by "required configmap/etcd-pod has changed"
2025-11-17T09:40:11.580703533Z W1117 09:40:11.580499       1 staticpod.go:38] revision 10 is unexpectedly already the XXXXXX available revision. This is a possible race!
2025-11-17T09:40:11.591626189Z E1117 09:40:11.591581       1 base_controller.go:268] RevisionController reconciliation failed: conflicting XXXXXXAvailableRevision 10
2025-11-17T09:40:11.593264868Z I1117 09:40:11.593227       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"RevisionControllerDegraded: conflicting XXXXXXAvailableRevision 10\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:40:11.600620493Z I1117 09:40:11.600577       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "RevisionControllerDegraded: conflicting XXXXXXAvailableRevision 10\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:40:11.610180739Z I1117 09:40:11.609949       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:40:11.619610792Z I1117 09:40:11.619541       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "RevisionControllerDegraded: conflicting XXXXXXAvailableRevision 10\nNodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:40:11.673199277Z I1117 09:40:11.673141       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.02 %, dbSize: 108425216
2025-11-17T09:40:11.673199277Z I1117 09:40:11.673177       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.02 %, dbSize: 108331008
2025-11-17T09:40:11.673199277Z I1117 09:40:11.673183       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.03 %, dbSize: 108376064
2025-11-17T09:40:12.148493902Z I1117 09:40:12.148425       1 request.go:697] Waited for 1.591827445s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:40:12.550868224Z I1117 09:40:12.550824       1 installer_controller.go:512] "XXXXXX2" is in transition to 9, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:40:12.563610298Z I1117 09:40:12.563542       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:40:12.575270979Z I1117 09:40:12.574573       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9" to "NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10",Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 9\nEtcdMembersAvailable: 2 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 2 members are available"
2025-11-17T09:40:12.693454204Z I1117 09:40:12.693360       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.01 %, dbSize: 108539904
2025-11-17T09:40:12.693454204Z I1117 09:40:12.693405       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 108453888
2025-11-17T09:40:13.148553156Z I1117 09:40:13.148486       1 request.go:697] Waited for 1.583232799s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:40:14.348649650Z I1117 09:40:14.348594       1 request.go:697] Waited for 1.787259379s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-9-XXXXXX2
2025-11-17T09:40:15.348798288Z I1117 09:40:15.348707       1 request.go:697] Waited for 1.797458542s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods
2025-11-17T09:40:15.354356227Z I1117 09:40:15.353835       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-10-XXXXXX2 -n openshift-etcd because it was missing
2025-11-17T09:40:16.154601196Z I1117 09:40:16.154206       1 installer_controller.go:500] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:40:16.154601196Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:40:16.154601196Z  CurrentRevision: (int32) 6,
2025-11-17T09:40:16.154601196Z  TargetRevision: (int32) 10,
2025-11-17T09:40:16.154601196Z  LastFailedRevision: (int32) 0,
2025-11-17T09:40:16.154601196Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:40:16.154601196Z  LastFailedReason: (string) "",
2025-11-17T09:40:16.154601196Z  LastFailedCount: (int) 0,
2025-11-17T09:40:16.154601196Z  LastFallbackCount: (int) 0,
2025-11-17T09:40:16.154601196Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:40:16.154601196Z }
2025-11-17T09:40:16.154601196Z  because new revision pending
2025-11-17T09:40:16.252455450Z I1117 09:40:16.252401       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.02 %, dbSize: 108707840
2025-11-17T09:40:16.548348299Z I1117 09:40:16.547798       1 request.go:697] Waited for 1.796440101s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:40:16.685873765Z I1117 09:40:16.685826       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:40:16.709453532Z I1117 09:40:16.709409       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:40:16.717427195Z I1117 09:40:16.717281       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 2 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 3 members are available"
2025-11-17T09:40:17.547990519Z I1117 09:40:17.547912       1 request.go:697] Waited for 1.594713133s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:40:18.548145054Z I1117 09:40:18.547981       1 request.go:697] Waited for 1.796330563s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:40:18.755737181Z I1117 09:40:18.755674       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-10-XXXXXX1 -n openshift-etcd because it was missing
2025-11-17T09:40:19.548634651Z I1117 09:40:19.548569       1 request.go:697] Waited for 1.795670499s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:40:19.754215138Z I1117 09:40:19.753823       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-10-XXXXXX2 -n openshift-etcd because it was missing
2025-11-17T09:40:20.747882258Z I1117 09:40:20.747824       1 request.go:697] Waited for 1.796865675s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:40:21.351822481Z I1117 09:40:21.351691       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:40:21.748751124Z I1117 09:40:21.748677       1 request.go:697] Waited for 1.597918612s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:40:21.955080665Z I1117 09:40:21.955001       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-10-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:40:22.948523647Z I1117 09:40:22.948465       1 request.go:697] Waited for 1.397288885s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX0
2025-11-17T09:40:24.148715344Z I1117 09:40:24.148655       1 request.go:697] Waited for 1.3979088s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-10-XXXXXX2
2025-11-17T09:40:24.152071652Z I1117 09:40:24.151942       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:40:25.148856875Z I1117 09:40:25.148785       1 request.go:697] Waited for 1.388516583s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:40:26.551607982Z I1117 09:40:26.551542       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:40:28.152273168Z I1117 09:40:28.152212       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:40:29.750829906Z I1117 09:40:29.750772       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:40:30.950998725Z I1117 09:40:30.950934       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:41:05.854332887Z I1117 09:41:05.854245       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:41:07.640581915Z I1117 09:41:07.640535       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 6
2025-11-17T09:41:09.840746256Z I1117 09:41:09.840697       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 6
2025-11-17T09:41:18.736091557Z I1117 09:41:18.736032       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 6
2025-11-17T09:41:20.885439616Z W1117 09:41:20.885388       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX2]
2025-11-17T09:41:31.684790946Z E1117 09:41:31.684744       1 etcdmemberscontroller.go:81] Unhealthy etcd member found: XXXXXX2, took=, err=create client failure: failed to make etcd client for endpoints [https://XXXXXXXXXXX:2379]: context deadline exceeded
2025-11-17T09:41:31.700996195Z I1117 09:41:31.700946       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX2 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:41:31.716598413Z I1117 09:41:31.716553       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX2 is unhealthy"
2025-11-17T09:41:31.721120677Z I1117 09:41:31.721084       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX2 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX2 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:41:31.734035731Z I1117 09:41:31.733991       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX2 is unhealthy"
2025-11-17T09:41:32.899605726Z I1117 09:41:32.899396       1 request.go:697] Waited for 1.179628313s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:41:33.503439697Z I1117 09:41:33.503379       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 6
2025-11-17T09:41:33.900146911Z I1117 09:41:33.900022       1 request.go:697] Waited for 1.391385155s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:41:35.100186346Z I1117 09:41:35.100130       1 request.go:697] Waited for 1.192958243s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2025-11-17T09:41:35.918611816Z W1117 09:41:35.918535       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX2]
2025-11-17T09:41:36.299550795Z I1117 09:41:36.299493       1 request.go:697] Waited for 1.196544066s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2025-11-17T09:41:36.904203619Z I1117 09:41:36.904101       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 6
2025-11-17T09:41:45.520396689Z I1117 09:41:45.520185       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:41:46.315176246Z I1117 09:41:46.315120       1 request.go:697] Waited for 1.163743949s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:41:46.750000270Z E1117 09:41:46.749942       1 base_controller.go:268] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, XXXXXX2 is unhealthy
2025-11-17T09:41:47.514307723Z I1117 09:41:47.514247       1 request.go:697] Waited for 1.193549167s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:41:48.515855755Z I1117 09:41:48.515668       1 request.go:697] Waited for 1.197865515s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:41:49.117308353Z I1117 09:41:49.117242       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:41:49.517112287Z I1117 09:41:49.516415       1 request.go:697] Waited for 1.197601249s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:41:49.631071682Z I1117 09:41:49.631015       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.07 %, dbSize: 114880512
2025-11-17T09:41:49.631071682Z I1117 09:41:49.631031       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.06 %, dbSize: 114765824
2025-11-17T09:41:49.717718694Z I1117 09:41:49.717650       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.07 %, dbSize: 114880512
2025-11-17T09:41:49.717718694Z I1117 09:41:49.717673       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.06 %, dbSize: 114765824
2025-11-17T09:41:50.714353417Z I1117 09:41:50.714136       1 request.go:697] Waited for 1.189051551s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:41:50.950923646Z W1117 09:41:50.950853       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX2]
2025-11-17T09:41:51.714828581Z I1117 09:41:51.714758       1 request.go:697] Waited for 1.197928157s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX1
2025-11-17T09:41:52.120155212Z I1117 09:41:52.120085       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:41:54.718270454Z I1117 09:41:54.718127       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:41:57.118033907Z I1117 09:41:57.117967       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:42:05.218554649Z I1117 09:42:05.218484       1 installer_controller.go:512] "XXXXXX2" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:42:07.202755645Z I1117 09:42:07.202690       1 installer_controller.go:500] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:42:07.202755645Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:42:07.202755645Z  CurrentRevision: (int32) 10,
2025-11-17T09:42:07.202755645Z  TargetRevision: (int32) 0,
2025-11-17T09:42:07.202755645Z  LastFailedRevision: (int32) 0,
2025-11-17T09:42:07.202755645Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:42:07.202755645Z  LastFailedReason: (string) "",
2025-11-17T09:42:07.202755645Z  LastFailedCount: (int) 0,
2025-11-17T09:42:07.202755645Z  LastFallbackCount: (int) 0,
2025-11-17T09:42:07.202755645Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:42:07.202755645Z }
2025-11-17T09:42:07.202755645Z  because static pod is ready
2025-11-17T09:42:07.213615825Z I1117 09:42:07.213534       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX2" from revision 6 to 10 because static pod is ready
2025-11-17T09:42:07.215115998Z I1117 09:42:07.215073       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX2 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 2 nodes are at revision 8; 1 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX2 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:42:07.236403492Z I1117 09:42:07.236351       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10" to "NodeInstallerProgressing: 2 nodes are at revision 8; 1 nodes are at revision 10",Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 8; 0 nodes have achieved new revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX2 is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX2 is unhealthy"
2025-11-17T09:42:07.308676317Z I1117 09:42:07.308532       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.08 %, dbSize: 115499008
2025-11-17T09:42:07.308676317Z I1117 09:42:07.308558       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.08 %, dbSize: 115519488
2025-11-17T09:42:07.308676317Z I1117 09:42:07.308563       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.06 %, dbSize: 115429376
2025-11-17T09:42:08.399966531Z I1117 09:42:08.399899       1 request.go:697] Waited for 1.184790468s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2025-11-17T09:42:09.600165820Z I1117 09:42:09.600092       1 request.go:697] Waited for 1.596296671s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:42:10.799743924Z I1117 09:42:10.799674       1 request.go:697] Waited for 1.196435329s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:42:11.800158384Z I1117 09:42:11.800072       1 request.go:697] Waited for 1.397675193s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:42:12.999831715Z I1117 09:42:12.999749       1 request.go:697] Waited for 1.19643555s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX0
2025-11-17T09:42:14.003029639Z I1117 09:42:14.002962       1 installer_controller.go:524] node XXXXXX1 with revision 8 is the oldest and needs new revision 10
2025-11-17T09:42:14.003029639Z I1117 09:42:14.003020       1 installer_controller.go:532] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:42:14.003029639Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:42:14.003029639Z  CurrentRevision: (int32) 8,
2025-11-17T09:42:14.003029639Z  TargetRevision: (int32) 10,
2025-11-17T09:42:14.003029639Z  LastFailedRevision: (int32) 0,
2025-11-17T09:42:14.003029639Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:42:14.003029639Z  LastFailedReason: (string) "",
2025-11-17T09:42:14.003029639Z  LastFailedCount: (int) 0,
2025-11-17T09:42:14.003029639Z  LastFallbackCount: (int) 0,
2025-11-17T09:42:14.003029639Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:42:14.003029639Z }
2025-11-17T09:42:14.016571539Z I1117 09:42:14.016509       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "XXXXXX1" from revision 8 to 10 because node XXXXXX1 with revision 8 is the oldest
2025-11-17T09:42:14.126788183Z I1117 09:42:14.126733       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.01 %, dbSize: 115699712
2025-11-17T09:42:14.126845807Z I1117 09:42:14.126836       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.01 %, dbSize: 115732480
2025-11-17T09:42:14.126864565Z I1117 09:42:14.126857       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.02 %, dbSize: 115662848
2025-11-17T09:42:15.200200174Z I1117 09:42:15.200141       1 request.go:697] Waited for 1.181218857s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:42:16.399732973Z I1117 09:42:16.399677       1 request.go:697] Waited for 1.471649738s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:42:16.708719284Z I1117 09:42:16.708659       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 2 nodes are at revision 8; 1 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX2 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:42:16.717235716Z I1117 09:42:16.717180       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX2 is unhealthy" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:42:16.718386483Z I1117 09:42:16.718348       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 2 nodes are at revision 8; 1 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:42:16.726243975Z I1117 09:42:16.726195       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX2 is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available"
2025-11-17T09:42:16.726393367Z I1117 09:42:16.726372       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 2 nodes are at revision 8; 1 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:42:16.731708953Z E1117 09:42:16.731672       1 base_controller.go:268] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:42:16.818628238Z I1117 09:42:16.818491       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.03 %, dbSize: 115871744
2025-11-17T09:42:16.818628238Z I1117 09:42:16.818510       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.03 %, dbSize: 115798016
2025-11-17T09:42:16.919714424Z I1117 09:42:16.919651       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.01 %, dbSize: 115871744
2025-11-17T09:42:16.919714424Z I1117 09:42:16.919669       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.03 %, dbSize: 115798016
2025-11-17T09:42:17.009641888Z I1117 09:42:17.009572       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-10-XXXXXX1 -n openshift-etcd because it was missing
2025-11-17T09:42:17.400180447Z I1117 09:42:17.400048       1 request.go:697] Waited for 1.397433387s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-10-XXXXXX0
2025-11-17T09:42:18.400179799Z I1117 09:42:18.400124       1 request.go:697] Waited for 1.692414275s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:42:18.803452758Z I1117 09:42:18.803366       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:42:19.600016168Z I1117 09:42:19.599958       1 request.go:697] Waited for 1.795878252s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:42:20.799928905Z I1117 09:42:20.799876       1 request.go:697] Waited for 1.796740991s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:42:21.800330844Z I1117 09:42:21.800175       1 request.go:697] Waited for 1.796967815s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:42:22.202650921Z I1117 09:42:22.202472       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:42:22.999651490Z I1117 09:42:22.999597       1 request.go:697] Waited for 1.594368939s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:42:24.000215014Z I1117 09:42:23.999994       1 request.go:697] Waited for 1.196783906s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:42:24.602232432Z I1117 09:42:24.602177       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:42:52.517344616Z I1117 09:42:52.516718       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:42:54.505675172Z I1117 09:42:54.505612       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 8
2025-11-17T09:42:56.507527016Z I1117 09:42:56.507463       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 8
2025-11-17T09:43:02.495587439Z I1117 09:43:02.495346       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 8
2025-11-17T09:43:07.535909155Z W1117 09:43:07.535855       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX1]
2025-11-17T09:43:16.692066913Z I1117 09:43:16.691999       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.5244733050758983 over 5 minutes on "BareMetal"; disk metrics are: etcd-XXXXXX0=0.002350,etcd-XXXXXX1=0.005032,etcd-XXXXXX2=0.001972. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2025-11-17T09:43:22.567328663Z W1117 09:43:22.567195       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX1]
2025-11-17T09:43:31.682555219Z E1117 09:43:31.682415       1 etcdmemberscontroller.go:81] Unhealthy etcd member found: XXXXXX1, took=, err=create client failure: failed to make etcd client for endpoints [https://XXXXXXXXXXX:2379]: context deadline exceeded
2025-11-17T09:43:31.698227951Z I1117 09:43:31.698111       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX1 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 2 nodes are at revision 8; 1 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:43:31.707110191Z I1117 09:43:31.705644       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX1 is unhealthy"
2025-11-17T09:43:31.725273516Z I1117 09:43:31.725197       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX1 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 2 nodes are at revision 8; 1 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX1 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:43:31.736275744Z I1117 09:43:31.736231       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX1 is unhealthy"
2025-11-17T09:43:32.899195455Z I1117 09:43:32.899015       1 request.go:697] Waited for 1.186248881s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-10-XXXXXX0
2025-11-17T09:43:33.899512859Z I1117 09:43:33.899443       1 request.go:697] Waited for 1.39018182s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:43:34.104041826Z I1117 09:43:34.103980       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:43:35.099001406Z I1117 09:43:35.098865       1 request.go:697] Waited for 1.595898205s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:43:36.099171714Z I1117 09:43:36.099018       1 request.go:697] Waited for 1.593336166s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:43:37.299351539Z I1117 09:43:37.299224       1 request.go:697] Waited for 1.597666731s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-10-XXXXXX1
2025-11-17T09:43:37.597950588Z W1117 09:43:37.597879       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX1]
2025-11-17T09:43:38.498707526Z I1117 09:43:38.498660       1 request.go:697] Waited for 1.3960617s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:43:38.702433757Z I1117 09:43:38.702375       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:43:39.499371345Z I1117 09:43:39.499326       1 request.go:697] Waited for 1.197334009s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:43:41.303472041Z I1117 09:43:41.303415       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:43:46.750589135Z E1117 09:43:46.750541       1 base_controller.go:268] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, XXXXXX1 is unhealthy
2025-11-17T09:43:52.628146018Z W1117 09:43:52.628093       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX1]
2025-11-17T09:43:53.863956035Z I1117 09:43:53.863884       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:43:57.108093395Z I1117 09:43:57.108038       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.02 %, dbSize: 119078912
2025-11-17T09:43:57.108093395Z I1117 09:43:57.108055       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.02 %, dbSize: 118956032
2025-11-17T09:43:57.108093395Z I1117 09:43:57.108059       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.02 %, dbSize: 119099392
2025-11-17T09:43:57.185516356Z I1117 09:43:57.185466       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.02 %, dbSize: 119078912
2025-11-17T09:43:57.185516356Z I1117 09:43:57.185483       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.01 %, dbSize: 118956032
2025-11-17T09:43:57.185516356Z I1117 09:43:57.185487       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.01 %, dbSize: 119099392
2025-11-17T09:44:03.039322798Z I1117 09:44:03.039243       1 installer_controller.go:512] "XXXXXX1" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:44:04.227448969Z I1117 09:44:04.226471       1 request.go:697] Waited for 1.145503032s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:44:05.830630755Z I1117 09:44:05.830481       1 installer_controller.go:500] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:44:05.830630755Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:44:05.830630755Z  CurrentRevision: (int32) 10,
2025-11-17T09:44:05.830630755Z  TargetRevision: (int32) 0,
2025-11-17T09:44:05.830630755Z  LastFailedRevision: (int32) 0,
2025-11-17T09:44:05.830630755Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:44:05.830630755Z  LastFailedReason: (string) "",
2025-11-17T09:44:05.830630755Z  LastFailedCount: (int) 0,
2025-11-17T09:44:05.830630755Z  LastFallbackCount: (int) 0,
2025-11-17T09:44:05.830630755Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:44:05.830630755Z }
2025-11-17T09:44:05.830630755Z  because static pod is ready
2025-11-17T09:44:05.842015439Z I1117 09:44:05.841962       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX1" from revision 8 to 10 because static pod is ready
2025-11-17T09:44:05.847724495Z I1117 09:44:05.847682       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX1 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 8; 2 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX1 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:44:05.859379948Z I1117 09:44:05.859231       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 8; 1 nodes are at revision 10" to "NodeInstallerProgressing: 1 nodes are at revision 8; 2 nodes are at revision 10",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 8; 1 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX1 is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX1 is unhealthy"
2025-11-17T09:44:05.940499276Z I1117 09:44:05.940452       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.03 %, dbSize: 119316480
2025-11-17T09:44:05.940556911Z I1117 09:44:05.940548       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.06 %, dbSize: 119504896
2025-11-17T09:44:07.026188294Z I1117 09:44:07.026019       1 request.go:697] Waited for 1.179946212s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:44:08.225795068Z I1117 09:44:08.225730       1 request.go:697] Waited for 1.595986387s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:44:09.426232481Z I1117 09:44:09.426137       1 request.go:697] Waited for 1.396009916s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:44:10.625940760Z I1117 09:44:10.625874       1 request.go:697] Waited for 1.39696593s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:44:12.431923339Z I1117 09:44:12.430853       1 installer_controller.go:524] node XXXXXX0 with revision 8 is the oldest and needs new revision 10
2025-11-17T09:44:12.431923339Z I1117 09:44:12.430899       1 installer_controller.go:532] "XXXXXX0" moving to (v1.NodeStatus) {
2025-11-17T09:44:12.431923339Z  NodeName: (string) (len=7) "XXXXXX0",
2025-11-17T09:44:12.431923339Z  CurrentRevision: (int32) 8,
2025-11-17T09:44:12.431923339Z  TargetRevision: (int32) 10,
2025-11-17T09:44:12.431923339Z  LastFailedRevision: (int32) 0,
2025-11-17T09:44:12.431923339Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:44:12.431923339Z  LastFailedReason: (string) "",
2025-11-17T09:44:12.431923339Z  LastFailedCount: (int) 0,
2025-11-17T09:44:12.431923339Z  LastFallbackCount: (int) 0,
2025-11-17T09:44:12.431923339Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:44:12.431923339Z }
2025-11-17T09:44:12.443428378Z I1117 09:44:12.442503       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "XXXXXX0" from revision 8 to 10 because node XXXXXX0 with revision 8 is the oldest
2025-11-17T09:44:12.537314985Z I1117 09:44:12.537255       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.02 %, dbSize: 119570432
2025-11-17T09:44:12.537353309Z I1117 09:44:12.537318       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.03 %, dbSize: 119455744
2025-11-17T09:44:12.537353309Z I1117 09:44:12.537327       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.02 %, dbSize: 119595008
2025-11-17T09:44:13.625934378Z I1117 09:44:13.625852       1 request.go:697] Waited for 1.181584901s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:44:14.626043679Z I1117 09:44:14.625989       1 request.go:697] Waited for 1.595257245s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:44:15.236388202Z I1117 09:44:15.236316       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-10-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:44:15.825767790Z I1117 09:44:15.825703       1 request.go:697] Waited for 1.395059539s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:44:16.629672187Z I1117 09:44:16.629522       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:44:16.695400316Z I1117 09:44:16.695326       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.3717565384747568 over 5 minutes on "BareMetal"; disk metrics are: etcd-XXXXXX0=0.002350,etcd-XXXXXX1=0.005032,etcd-XXXXXX2=0.001972. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2025-11-17T09:44:16.714591778Z I1117 09:44:16.712264       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 8; 2 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX1 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:44:16.726679567Z I1117 09:44:16.726353       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX1 is unhealthy" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:44:16.759348047Z I1117 09:44:16.758634       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 8; 2 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:44:16.776176401Z I1117 09:44:16.769626       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX1 is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available"
2025-11-17T09:44:16.777440301Z I1117 09:44:16.777053       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 8; 2 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:44:16.781667838Z E1117 09:44:16.781619       1 base_controller.go:268] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:44:17.025325661Z I1117 09:44:17.025246       1 request.go:697] Waited for 1.596545007s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:44:18.025307650Z I1117 09:44:18.025248       1 request.go:697] Waited for 1.314199237s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:44:19.026364526Z I1117 09:44:19.026211       1 request.go:697] Waited for 1.795198741s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:44:19.628133243Z I1117 09:44:19.628082       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:44:20.225698611Z I1117 09:44:20.225653       1 request.go:697] Waited for 1.79633973s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-10-XXXXXX1
2025-11-17T09:44:21.426029693Z I1117 09:44:21.425879       1 request.go:697] Waited for 1.796445148s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-10-XXXXXX0
2025-11-17T09:44:22.426020705Z I1117 09:44:22.425968       1 request.go:697] Waited for 1.597484454s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:44:23.031040464Z I1117 09:44:23.030960       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:44:23.426323867Z I1117 09:44:23.426255       1 request.go:697] Waited for 1.596439793s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-10-XXXXXX2
2025-11-17T09:44:50.943836887Z I1117 09:44:50.943782       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:44:52.939587847Z I1117 09:44:52.939530       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 8
2025-11-17T09:44:54.940322735Z I1117 09:44:54.940245       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 8
2025-11-17T09:45:03.708597298Z I1117 09:45:03.708522       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 8
2025-11-17T09:45:05.968904342Z W1117 09:45:05.968859       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX0]
2025-11-17T09:45:16.690610320Z I1117 09:45:16.690517       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 3.3333333333333335 over 5 minutes on "BareMetal"; disk metrics are: etcd-XXXXXX0=0.002350,etcd-XXXXXX1=0.005032,etcd-XXXXXX2=0.001972. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2025-11-17T09:45:21.000232484Z W1117 09:45:21.000174       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX0]
2025-11-17T09:45:30.548832579Z I1117 09:45:30.548698       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:45:30.945311362Z I1117 09:45:30.945150       1 request.go:697] Waited for 1.183176175s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:45:31.689123469Z E1117 09:45:31.689072       1 etcdmemberscontroller.go:81] Unhealthy etcd member found: XXXXXX0, took=, err=create client failure: failed to make etcd client for endpoints [https://XXXXXXXXXXX:2379]: context deadline exceeded
2025-11-17T09:45:31.700199331Z I1117 09:45:31.700148       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX0 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 8; 2 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:45:31.708753590Z I1117 09:45:31.708692       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX0 is unhealthy"
2025-11-17T09:45:31.715415914Z I1117 09:45:31.715372       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX0 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:20:48Z","message":"NodeInstallerProgressing: 1 nodes are at revision 8; 2 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX0 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:45:31.723719900Z I1117 09:45:31.723664       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX0 is unhealthy"
2025-11-17T09:45:32.144918815Z I1117 09:45:32.144857       1 request.go:697] Waited for 1.195645164s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:45:33.345084223Z I1117 09:45:33.345016       1 request.go:697] Waited for 1.644112788s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-10-XXXXXX2
2025-11-17T09:45:34.544881592Z I1117 09:45:34.544760       1 request.go:697] Waited for 1.796797354s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:45:35.348897203Z I1117 09:45:35.348835       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:45:35.745115206Z I1117 09:45:35.745065       1 request.go:697] Waited for 1.796578334s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:45:36.030565479Z W1117 09:45:36.030522       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX0]
2025-11-17T09:45:36.945048281Z I1117 09:45:36.945004       1 request.go:697] Waited for 1.595007225s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-10-XXXXXX0
2025-11-17T09:45:37.945082298Z I1117 09:45:37.945015       1 request.go:697] Waited for 1.196429196s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-10-XXXXXX2
2025-11-17T09:45:39.144442012Z I1117 09:45:39.144337       1 request.go:697] Waited for 1.196541419s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-10-XXXXXX1
2025-11-17T09:45:39.348365757Z I1117 09:45:39.348236       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:45:46.749394428Z E1117 09:45:46.749345       1 base_controller.go:268] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, XXXXXX0 is unhealthy
2025-11-17T09:45:51.061568955Z W1117 09:45:51.061511       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX0]
2025-11-17T09:45:55.790614445Z I1117 09:45:55.790545       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:45:55.900632186Z I1117 09:45:55.900579       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.01 %, dbSize: 121987072
2025-11-17T09:45:59.868772417Z I1117 09:45:59.868720       1 installer_controller.go:512] "XXXXXX0" is in transition to 10, but has not made progress because static pod is pending
2025-11-17T09:46:01.047969593Z I1117 09:46:01.047909       1 request.go:697] Waited for 1.146729367s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:46:02.651359046Z I1117 09:46:02.651239       1 installer_controller.go:500] "XXXXXX0" moving to (v1.NodeStatus) {
2025-11-17T09:46:02.651359046Z  NodeName: (string) (len=7) "XXXXXX0",
2025-11-17T09:46:02.651359046Z  CurrentRevision: (int32) 10,
2025-11-17T09:46:02.651359046Z  TargetRevision: (int32) 0,
2025-11-17T09:46:02.651359046Z  LastFailedRevision: (int32) 0,
2025-11-17T09:46:02.651359046Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:46:02.651359046Z  LastFailedReason: (string) "",
2025-11-17T09:46:02.651359046Z  LastFailedCount: (int) 0,
2025-11-17T09:46:02.651359046Z  LastFallbackCount: (int) 0,
2025-11-17T09:46:02.651359046Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:46:02.651359046Z }
2025-11-17T09:46:02.651359046Z  because static pod is ready
2025-11-17T09:46:02.660965304Z I1117 09:46:02.660927       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX0" from revision 8 to 10 because static pod is ready
2025-11-17T09:46:02.661460286Z I1117 09:46:02.661429       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX0 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:46:02Z","message":"NodeInstallerProgressing: 3 nodes are at revision 10\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX0 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:46:02.667615461Z I1117 09:46:02.667216       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 10\nEtcdMembersProgressing: No unstarted etcd members found"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 8; 2 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX0 is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX0 is unhealthy"
2025-11-17T09:46:02.731231562Z I1117 09:46:02.731165       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-endpoints -n openshift-etcd:
2025-11-17T09:46:02.731446331Z W1117 09:46:02.731425       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]] vs. [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]]
2025-11-17T09:46:02.731655968Z W1117 09:46:02.731628       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]] vs. [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]]
2025-11-17T09:46:02.731977778Z W1117 09:46:02.731956       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]] vs. [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]]
2025-11-17T09:46:02.735716439Z W1117 09:46:02.735674       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]] vs. [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]]
2025-11-17T09:46:02.738908314Z W1117 09:46:02.738569       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]] vs. [[https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379 https://XXXXXXXXXXX:2379]]
2025-11-17T09:46:02.771929653Z I1117 09:46:02.771886       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.04 %, dbSize: 122159104
2025-11-17T09:46:02.771986508Z I1117 09:46:02.771977       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.07 %, dbSize: 122101760
2025-11-17T09:46:02.772006161Z I1117 09:46:02.771998       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.07 %, dbSize: 122236928
2025-11-17T09:46:03.848891618Z I1117 09:46:03.848833       1 request.go:697] Waited for 1.186465921s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:46:04.848966881Z I1117 09:46:04.848912       1 request.go:697] Waited for 1.597811514s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:46:06.048846861Z I1117 09:46:06.048805       1 request.go:697] Waited for 1.546955875s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:46:07.248514193Z I1117 09:46:07.248467       1 request.go:697] Waited for 1.396700473s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:46:08.448897108Z I1117 09:46:08.448842       1 request.go:697] Waited for 1.197307625s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:46:09.648547727Z I1117 09:46:09.648386       1 request.go:697] Waited for 1.195641925s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:46:16.701374713Z I1117 09:46:16.701313       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.2222222222222223 over 5 minutes on "BareMetal"; disk metrics are: etcd-XXXXXX0=0.002350,etcd-XXXXXX1=0.005032,etcd-XXXXXX2=0.001972. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2025-11-17T09:46:16.712151429Z I1117 09:46:16.711773       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:46:02Z","message":"NodeInstallerProgressing: 3 nodes are at revision 10\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX0 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:46:16.712350151Z I1117 09:46:16.712328       1 etcdcli_pool.go:70] creating a new cached client
2025-11-17T09:46:16.721206703Z I1117 09:46:16.721144       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX0 is unhealthy" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2025-11-17T09:46:16.746206247Z I1117 09:46:16.746152       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:46:02Z","message":"NodeInstallerProgressing: 3 nodes are at revision 10\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:46:16.756086699Z I1117 09:46:16.756037       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX0 is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available"
2025-11-17T09:46:16.820539883Z I1117 09:46:16.820483       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 55.66 %, dbSize: 124542976
2025-11-17T09:46:16.820659500Z I1117 09:46:16.820618       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentAttempt' Attempting defrag on member: XXXXXX0, memberID: d2dd98de6e8ab051, dbSize: 124542976, dbInUse: 55222272, leader ID: 12197590100771669504
2025-11-17T09:46:17.344338569Z I1117 09:46:17.344236       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentSuccess' etcd member has been defragmented: XXXXXX0, memberID: 15194468798922666065
2025-11-17T09:46:17.360125495Z I1117 09:46:17.360079       1 etcdcli_pool.go:157] closing cached client
2025-11-17T09:46:17.848453884Z I1117 09:46:17.848383       1 request.go:697] Waited for 1.101906001s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:46:19.048412209Z I1117 09:46:19.048245       1 request.go:697] Waited for 1.597348334s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:46:19.056750167Z I1117 09:46:19.056700       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-scripts -n openshift-etcd:
2025-11-17T09:46:19.056750167Z cause by changes in data.etcd.env
2025-11-17T09:46:19.655550211Z I1117 09:46:19.655493       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-pod -n openshift-etcd:
2025-11-17T09:46:19.655550211Z cause by changes in data.pod.yaml
2025-11-17T09:46:19.664023651Z I1117 09:46:19.663960       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 11 triggered by "required configmap/etcd-pod has changed"
2025-11-17T09:46:20.048594603Z I1117 09:46:20.048440       1 request.go:697] Waited for 1.390492764s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:46:21.048622479Z I1117 09:46:21.048553       1 request.go:697] Waited for 1.384575796s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps
2025-11-17T09:46:21.056735071Z I1117 09:46:21.056681       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-11 -n openshift-etcd because it was missing
2025-11-17T09:46:22.248454309Z I1117 09:46:22.248397       1 request.go:697] Waited for 1.397396191s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:46:22.254162986Z I1117 09:46:22.253797       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/restore-etcd-pod -n openshift-etcd:
2025-11-17T09:46:22.254162986Z cause by changes in data.pod.yaml
2025-11-17T09:46:22.454393130Z I1117 09:46:22.454331       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-11 -n openshift-etcd because it was missing
2025-11-17T09:46:23.248814941Z I1117 09:46:23.248761       1 request.go:697] Waited for 1.197081736s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:46:23.654846642Z I1117 09:46:23.654765       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-11 -n openshift-etcd because it was missing
2025-11-17T09:46:24.655531725Z I1117 09:46:24.655475       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-11 -n openshift-etcd because it was missing
2025-11-17T09:46:25.453520570Z I1117 09:46:25.453464       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-11 -n openshift-etcd because it was missing
2025-11-17T09:46:26.054696086Z I1117 09:46:26.054624       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-11 -n openshift-etcd because it was missing
2025-11-17T09:46:26.653543038Z I1117 09:46:26.653468       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-11 -n openshift-etcd because it was missing
2025-11-17T09:46:27.255248055Z I1117 09:46:27.255173       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-11 -n openshift-etcd because it was missing
2025-11-17T09:46:27.273082308Z I1117 09:46:27.272908       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 10 created because required configmap/etcd-pod has changed
2025-11-17T09:46:27.473107603Z I1117 09:46:27.473061       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:46:27Z","message":"NodeInstallerProgressing: 3 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10; 0 nodes have achieved new revision 11\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:46:27.486336758Z I1117 09:46:27.485811       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing changed from False to True ("NodeInstallerProgressing: 3 nodes are at revision 10; 0 nodes have achieved new revision 11"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10; 0 nodes have achieved new revision 11\nEtcdMembersAvailable: 3 members are available"
2025-11-17T09:46:28.448256065Z I1117 09:46:28.448202       1 request.go:697] Waited for 1.17954382s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-11-XXXXXX2
2025-11-17T09:46:29.448428093Z I1117 09:46:29.448263       1 request.go:697] Waited for 1.395868117s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:46:29.857465581Z I1117 09:46:29.857392       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-11-XXXXXX2 -n openshift-etcd because it was missing
2025-11-17T09:46:30.448478713Z I1117 09:46:30.448345       1 request.go:697] Waited for 1.396281331s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:46:31.648282690Z I1117 09:46:31.648070       1 request.go:697] Waited for 1.596547036s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:46:32.648484269Z I1117 09:46:32.648380       1 request.go:697] Waited for 1.594577227s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:46:32.854345240Z I1117 09:46:32.854269       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-11-XXXXXX1 -n openshift-etcd because it was missing
2025-11-17T09:46:33.848509897Z I1117 09:46:33.848433       1 request.go:697] Waited for 1.38899639s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2025-11-17T09:46:34.849462579Z I1117 09:46:34.849329       1 request.go:697] Waited for 1.397286476s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:46:35.655672506Z I1117 09:46:35.655610       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-11-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:46:36.048636798Z I1117 09:46:36.048485       1 request.go:697] Waited for 1.397057256s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:46:36.052242930Z I1117 09:46:36.052204       1 installer_controller.go:524] node XXXXXX2 with revision 10 is the oldest and needs new revision 11
2025-11-17T09:46:36.052297747Z I1117 09:46:36.052247       1 installer_controller.go:532] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:46:36.052297747Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:46:36.052297747Z  CurrentRevision: (int32) 10,
2025-11-17T09:46:36.052297747Z  TargetRevision: (int32) 11,
2025-11-17T09:46:36.052297747Z  LastFailedRevision: (int32) 0,
2025-11-17T09:46:36.052297747Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:46:36.052297747Z  LastFailedReason: (string) "",
2025-11-17T09:46:36.052297747Z  LastFailedCount: (int) 0,
2025-11-17T09:46:36.052297747Z  LastFallbackCount: (int) 0,
2025-11-17T09:46:36.052297747Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:46:36.052297747Z }
2025-11-17T09:46:36.063337648Z I1117 09:46:36.063236       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "XXXXXX2" from revision 10 to 11 because node XXXXXX2 with revision 10 is the oldest
2025-11-17T09:46:37.248242173Z I1117 09:46:37.248181       1 request.go:697] Waited for 1.185186358s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2025-11-17T09:46:38.248803365Z I1117 09:46:38.248664       1 request.go:697] Waited for 1.787551105s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:46:39.449004092Z I1117 09:46:39.448873       1 request.go:697] Waited for 1.596654451s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:46:40.648248306Z I1117 09:46:40.648124       1 request.go:697] Waited for 1.395690247s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:46:41.648415508Z I1117 09:46:41.648359       1 request.go:697] Waited for 1.394695769s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-11-XXXXXX2
2025-11-17T09:46:42.648645471Z I1117 09:46:42.648587       1 request.go:697] Waited for 1.396610073s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:46:43.848443957Z I1117 09:46:43.848373       1 request.go:697] Waited for 1.189880562s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:46:44.451102796Z I1117 09:46:44.451052       1 installer_controller.go:524] node XXXXXX2 with revision 10 is the oldest and needs new revision 11
2025-11-17T09:46:44.451102796Z I1117 09:46:44.451094       1 installer_controller.go:532] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:46:44.451102796Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:46:44.451102796Z  CurrentRevision: (int32) 10,
2025-11-17T09:46:44.451102796Z  TargetRevision: (int32) 11,
2025-11-17T09:46:44.451102796Z  LastFailedRevision: (int32) 0,
2025-11-17T09:46:44.451102796Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:46:44.451102796Z  LastFailedReason: (string) "",
2025-11-17T09:46:44.451102796Z  LastFailedCount: (int) 0,
2025-11-17T09:46:44.451102796Z  LastFallbackCount: (int) 0,
2025-11-17T09:46:44.451102796Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:46:44.451102796Z }
2025-11-17T09:46:45.848277199Z I1117 09:46:45.848189       1 request.go:697] Waited for 1.196029924s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:46:46.848761302Z I1117 09:46:46.848619       1 request.go:697] Waited for 1.397904167s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods
2025-11-17T09:46:46.855344747Z I1117 09:46:46.855277       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-11-XXXXXX2 -n openshift-etcd because it was missing
2025-11-17T09:46:48.048737075Z I1117 09:46:48.048511       1 request.go:697] Waited for 1.193516986s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-11-XXXXXX2
2025-11-17T09:46:48.052013058Z I1117 09:46:48.051910       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:46:49.248620089Z I1117 09:46:49.248531       1 request.go:697] Waited for 1.195495501s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-11-XXXXXX2
2025-11-17T09:46:50.448412183Z I1117 09:46:50.448341       1 request.go:697] Waited for 1.392375585s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2025-11-17T09:46:50.650876537Z I1117 09:46:50.650824       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:46:51.648397085Z I1117 09:46:51.648321       1 request.go:697] Waited for 1.396586874s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:46:52.648469128Z I1117 09:46:52.648337       1 request.go:697] Waited for 1.396196716s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2025-11-17T09:46:53.253503385Z I1117 09:46:53.253460       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:46:55.395899506Z I1117 09:46:55.395837       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 55.27 %, dbSize: 123695104
2025-11-17T09:46:55.395997053Z I1117 09:46:55.395952       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentAttempt' Attempting defrag on member: XXXXXX1, memberID: d85a978ed2572588, dbSize: 123695104, dbInUse: 55332864, leader ID: 12197590100771669504
2025-11-17T09:46:56.029384276Z I1117 09:46:56.029265       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentSuccess' etcd member has been defragmented: XXXXXX1, memberID: 15589939699766470024
2025-11-17T09:47:16.700521801Z I1117 09:47:16.700425       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.2222222222222223 over 5 minutes on "BareMetal"; disk metrics are: etcd-XXXXXX0=0.004537,etcd-XXXXXX1=0.003769,etcd-XXXXXX2=0.004960. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2025-11-17T09:47:22.629542074Z I1117 09:47:22.629455       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:47:25.431268859Z I1117 09:47:25.431205       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:47:27.230198803Z I1117 09:47:27.230117       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:47:33.003253959Z I1117 09:47:33.002715       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:47:37.176967062Z W1117 09:47:37.176905       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX2]
2025-11-17T09:47:49.066377841Z W1117 09:47:49.066319       1 defragcontroller.go:207] cluster is unhealthy: 2 of 3 members are available, XXXXXX2 is unhealthy
2025-11-17T09:47:52.208868259Z W1117 09:47:52.208808       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX2]
2025-11-17T09:48:03.319367423Z I1117 09:48:03.319316       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:48:04.515387379Z I1117 09:48:04.515339       1 request.go:697] Waited for 1.099252304s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:48:05.714952667Z I1117 09:48:05.714829       1 request.go:697] Waited for 1.196690032s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:48:06.715040159Z I1117 09:48:06.714982       1 request.go:697] Waited for 1.196140374s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:48:06.719029939Z I1117 09:48:06.718907       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:48:09.318633933Z I1117 09:48:09.318577       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:48:11.118965982Z I1117 09:48:11.118916       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:48:16.703810312Z I1117 09:48:16.703727       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.4063142857142856 over 5 minutes on "BareMetal"; disk metrics are: etcd-XXXXXX0=0.003951,etcd-XXXXXX1=0.003860,etcd-XXXXXX2=0.003828. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2025-11-17T09:48:18.156112013Z W1117 09:48:18.156049       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX2]
2025-11-17T09:48:24.746165032Z I1117 09:48:24.746090       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:48:26.073036976Z I1117 09:48:26.072890       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 55.51 %, dbSize: 124477440
2025-11-17T09:48:26.073264694Z I1117 09:48:26.073234       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentAttempt' Attempting defrag on member: XXXXXX2, memberID: a9468b9230510600, dbSize: 124477440, dbInUse: 55377920, leader ID: 12197590100771669504
2025-11-17T09:48:26.693052448Z I1117 09:48:26.692918       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentSuccess' etcd member has been defragmented: XXXXXX2, memberID: 12197590100771669504
2025-11-17T09:48:33.193395218Z I1117 09:48:33.192428       1 installer_controller.go:512] "XXXXXX2" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:48:34.383329863Z I1117 09:48:34.383247       1 request.go:697] Waited for 1.161021618s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:48:35.988824814Z I1117 09:48:35.988770       1 installer_controller.go:500] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:48:35.988824814Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:48:35.988824814Z  CurrentRevision: (int32) 11,
2025-11-17T09:48:35.988824814Z  TargetRevision: (int32) 0,
2025-11-17T09:48:35.988824814Z  LastFailedRevision: (int32) 0,
2025-11-17T09:48:35.988824814Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:48:35.988824814Z  LastFailedReason: (string) "",
2025-11-17T09:48:35.988824814Z  LastFailedCount: (int) 0,
2025-11-17T09:48:35.988824814Z  LastFallbackCount: (int) 0,
2025-11-17T09:48:35.988824814Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:48:35.988824814Z }
2025-11-17T09:48:35.988824814Z  because static pod is ready
2025-11-17T09:48:35.997791163Z I1117 09:48:35.997750       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX2" from revision 10 to 11 because static pod is ready
2025-11-17T09:48:36.006945154Z I1117 09:48:36.006910       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:46:27Z","message":"NodeInstallerProgressing: 2 nodes are at revision 10; 1 nodes are at revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 10; 1 nodes are at revision 11\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:48:36.014091334Z I1117 09:48:36.014038       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 10; 0 nodes have achieved new revision 11" to "NodeInstallerProgressing: 2 nodes are at revision 10; 1 nodes are at revision 11",Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 10; 0 nodes have achieved new revision 11\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 10; 1 nodes are at revision 11\nEtcdMembersAvailable: 3 members are available"
2025-11-17T09:48:37.183433452Z I1117 09:48:37.183377       1 request.go:697] Waited for 1.182628305s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:48:38.183669271Z I1117 09:48:38.183528       1 request.go:697] Waited for 1.593620533s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2025-11-17T09:48:39.184019717Z I1117 09:48:39.183915       1 request.go:697] Waited for 1.396390636s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:48:40.383445294Z I1117 09:48:40.383300       1 request.go:697] Waited for 1.39673753s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:48:42.387202339Z I1117 09:48:42.387060       1 installer_controller.go:524] node XXXXXX1 with revision 10 is the oldest and needs new revision 11
2025-11-17T09:48:42.387202339Z I1117 09:48:42.387103       1 installer_controller.go:532] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:48:42.387202339Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:48:42.387202339Z  CurrentRevision: (int32) 10,
2025-11-17T09:48:42.387202339Z  TargetRevision: (int32) 11,
2025-11-17T09:48:42.387202339Z  LastFailedRevision: (int32) 0,
2025-11-17T09:48:42.387202339Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:48:42.387202339Z  LastFailedReason: (string) "",
2025-11-17T09:48:42.387202339Z  LastFailedCount: (int) 0,
2025-11-17T09:48:42.387202339Z  LastFallbackCount: (int) 0,
2025-11-17T09:48:42.387202339Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:48:42.387202339Z }
2025-11-17T09:48:42.399412141Z I1117 09:48:42.399338       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "XXXXXX1" from revision 10 to 11 because node XXXXXX1 with revision 10 is the oldest
2025-11-17T09:48:43.583790598Z I1117 09:48:43.583643       1 request.go:697] Waited for 1.184621947s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-11-XXXXXX2
2025-11-17T09:48:44.783947354Z I1117 09:48:44.783804       1 request.go:697] Waited for 1.196199117s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-11-XXXXXX1
2025-11-17T09:48:44.994388078Z I1117 09:48:44.994281       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-11-XXXXXX1 -n openshift-etcd because it was missing
2025-11-17T09:48:45.983481978Z I1117 09:48:45.983279       1 request.go:697] Waited for 1.195391635s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-11-XXXXXX0
2025-11-17T09:48:46.386744965Z I1117 09:48:46.386666       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:48:46.983523932Z I1117 09:48:46.983382       1 request.go:697] Waited for 1.389368138s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-backup-sa
2025-11-17T09:48:47.984579991Z I1117 09:48:47.984396       1 request.go:697] Waited for 1.196437438s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:48:48.586296931Z I1117 09:48:48.586223       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:48:50.186600125Z I1117 09:48:50.186536       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:49:04.816194088Z I1117 09:49:04.816141       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.06 %, dbSize: 58679296
2025-11-17T09:49:04.816194088Z I1117 09:49:04.816158       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.05 %, dbSize: 58785792
2025-11-17T09:49:04.816194088Z I1117 09:49:04.816162       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.08 %, dbSize: 58769408
2025-11-17T09:49:16.701250040Z I1117 09:49:16.701190       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.2222222222222223 over 5 minutes on "BareMetal"; disk metrics are: etcd-XXXXXX0=0.003818,etcd-XXXXXX1=0.003860,etcd-XXXXXX2=0.004551. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2025-11-17T09:49:20.333385705Z I1117 09:49:20.333323       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:49:23.532045914Z I1117 09:49:23.531981       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:49:25.732598999Z I1117 09:49:25.732528       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:49:32.496197821Z I1117 09:49:32.496140       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:49:35.349898952Z W1117 09:49:35.349668       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX1]
2025-11-17T09:49:50.389387507Z W1117 09:49:50.389278       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX1]
2025-11-17T09:49:58.997227435Z I1117 09:49:58.997091       1 request.go:697] Waited for 1.033155091s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:49:59.997649444Z I1117 09:49:59.997597       1 request.go:697] Waited for 1.194016437s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:50:01.001072709Z I1117 09:50:01.001007       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:50:01.196787481Z I1117 09:50:01.196731       1 request.go:697] Waited for 1.193521795s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:50:02.197429298Z I1117 09:50:02.197370       1 request.go:697] Waited for 1.195125786s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-11-XXXXXX1
2025-11-17T09:50:03.397394407Z I1117 09:50:03.396969       1 request.go:697] Waited for 1.19396175s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-11-XXXXXX1
2025-11-17T09:50:04.400411580Z I1117 09:50:04.400354       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:50:05.596832557Z I1117 09:50:05.596766       1 request.go:697] Waited for 1.141207615s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:50:07.200407221Z I1117 09:50:07.200326       1 installer_controller.go:512] "XXXXXX1" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:50:16.691981986Z I1117 09:50:16.691866       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.3245558009063716 over 5 minutes on "BareMetal"; disk metrics are: etcd-XXXXXX0=0.003764,etcd-XXXXXX1=0.003895,etcd-XXXXXX2=0.004130. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2025-11-17T09:50:18.360627563Z I1117 09:50:18.360572       1 installer_controller.go:500] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:50:18.360627563Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:50:18.360627563Z  CurrentRevision: (int32) 11,
2025-11-17T09:50:18.360627563Z  TargetRevision: (int32) 0,
2025-11-17T09:50:18.360627563Z  LastFailedRevision: (int32) 0,
2025-11-17T09:50:18.360627563Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:50:18.360627563Z  LastFailedReason: (string) "",
2025-11-17T09:50:18.360627563Z  LastFailedCount: (int) 0,
2025-11-17T09:50:18.360627563Z  LastFallbackCount: (int) 0,
2025-11-17T09:50:18.360627563Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:50:18.360627563Z }
2025-11-17T09:50:18.360627563Z  because static pod is ready
2025-11-17T09:50:18.370774074Z I1117 09:50:18.370701       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX1" from revision 10 to 11 because static pod is ready
2025-11-17T09:50:18.372187169Z I1117 09:50:18.372145       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:46:27Z","message":"NodeInstallerProgressing: 1 nodes are at revision 10; 2 nodes are at revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:50:18.379070940Z I1117 09:50:18.378930       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 10; 1 nodes are at revision 11" to "NodeInstallerProgressing: 1 nodes are at revision 10; 2 nodes are at revision 11",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 10; 1 nodes are at revision 11\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11\nEtcdMembersAvailable: 3 members are available"
2025-11-17T09:50:18.457494722Z I1117 09:50:18.457366       1 defragcontroller.go:294] etcd member "XXXXXX2" backend store fragmented: 0.09 %, dbSize: 62525440
2025-11-17T09:50:18.457494722Z I1117 09:50:18.457383       1 defragcontroller.go:294] etcd member "XXXXXX1" backend store fragmented: 0.05 %, dbSize: 62525440
2025-11-17T09:50:18.457494722Z I1117 09:50:18.457387       1 defragcontroller.go:294] etcd member "XXXXXX0" backend store fragmented: 0.03 %, dbSize: 62611456
2025-11-17T09:50:19.157686958Z I1117 09:50:19.157537       1 request.go:697] Waited for 1.088634806s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2025-11-17T09:50:20.357610201Z I1117 09:50:20.357503       1 request.go:697] Waited for 1.796121406s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:50:21.557170736Z I1117 09:50:21.557090       1 request.go:697] Waited for 1.396309697s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-11-XXXXXX1
2025-11-17T09:50:22.557314796Z I1117 09:50:22.557165       1 request.go:697] Waited for 1.397599891s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:50:22.761148064Z I1117 09:50:22.761084       1 installer_controller.go:500] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:50:22.761148064Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:50:22.761148064Z  CurrentRevision: (int32) 11,
2025-11-17T09:50:22.761148064Z  TargetRevision: (int32) 0,
2025-11-17T09:50:22.761148064Z  LastFailedRevision: (int32) 0,
2025-11-17T09:50:22.761148064Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:50:22.761148064Z  LastFailedReason: (string) "",
2025-11-17T09:50:22.761148064Z  LastFailedCount: (int) 0,
2025-11-17T09:50:22.761148064Z  LastFallbackCount: (int) 0,
2025-11-17T09:50:22.761148064Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:50:22.761148064Z }
2025-11-17T09:50:22.761148064Z  because static pod is ready
2025-11-17T09:50:23.756909519Z I1117 09:50:23.756830       1 request.go:697] Waited for 1.396684053s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX0
2025-11-17T09:50:28.160943665Z I1117 09:50:28.160885       1 installer_controller.go:524] node XXXXXX0 with revision 10 is the oldest and needs new revision 11
2025-11-17T09:50:28.160943665Z I1117 09:50:28.160928       1 installer_controller.go:532] "XXXXXX0" moving to (v1.NodeStatus) {
2025-11-17T09:50:28.160943665Z  NodeName: (string) (len=7) "XXXXXX0",
2025-11-17T09:50:28.160943665Z  CurrentRevision: (int32) 10,
2025-11-17T09:50:28.160943665Z  TargetRevision: (int32) 11,
2025-11-17T09:50:28.160943665Z  LastFailedRevision: (int32) 0,
2025-11-17T09:50:28.160943665Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:50:28.160943665Z  LastFailedReason: (string) "",
2025-11-17T09:50:28.160943665Z  LastFailedCount: (int) 0,
2025-11-17T09:50:28.160943665Z  LastFallbackCount: (int) 0,
2025-11-17T09:50:28.160943665Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:50:28.160943665Z }
2025-11-17T09:50:28.170641166Z I1117 09:50:28.170589       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "XXXXXX0" from revision 10 to 11 because node XXXXXX0 with revision 10 is the oldest
2025-11-17T09:50:29.357184684Z I1117 09:50:29.357137       1 request.go:697] Waited for 1.184279862s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-11-XXXXXX2
2025-11-17T09:50:30.556621425Z I1117 09:50:30.556547       1 request.go:697] Waited for 1.597203885s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2025-11-17T09:50:31.168157335Z I1117 09:50:31.168086       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-11-XXXXXX0 -n openshift-etcd because it was missing
2025-11-17T09:50:31.557335455Z I1117 09:50:31.557194       1 request.go:697] Waited for 1.393472233s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX1
2025-11-17T09:50:32.557471353Z I1117 09:50:32.557317       1 request.go:697] Waited for 1.389219207s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:50:32.759629282Z I1117 09:50:32.759565       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:50:33.757060751Z I1117 09:50:33.756991       1 request.go:697] Waited for 1.396045807s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:50:34.757341772Z I1117 09:50:34.757185       1 request.go:697] Waited for 1.395008309s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX1
2025-11-17T09:50:35.159390458Z I1117 09:50:35.159326       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:50:36.960026961Z I1117 09:50:36.959952       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:51:07.424734331Z I1117 09:51:07.424644       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:51:09.821628388Z I1117 09:51:09.821555       1 request.go:697] Waited for 1.139700095s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:51:10.025468432Z I1117 09:51:10.025410       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:51:12.625657858Z I1117 09:51:12.625094       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:51:16.707201153Z I1117 09:51:16.707132       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 3.3333333333333335 over 5 minutes on "BareMetal"; disk metrics are: etcd-XXXXXX0=0.003569,etcd-XXXXXX1=0.003972,etcd-XXXXXX2=0.003930. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2025-11-17T09:51:17.821689364Z I1117 09:51:17.821638       1 request.go:697] Waited for 1.043026689s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:51:18.822005790Z I1117 09:51:18.821903       1 request.go:697] Waited for 1.196607604s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:51:18.825699423Z I1117 09:51:18.825666       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:51:21.309802466Z W1117 09:51:21.309752       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX0]
2025-11-17T09:51:31.688696083Z E1117 09:51:31.688636       1 etcdmemberscontroller.go:81] Unhealthy etcd member found: XXXXXX0, took=, err=create client failure: failed to make etcd client for endpoints [https://XXXXXXXXXXX:2379]: context deadline exceeded
2025-11-17T09:51:31.701717796Z I1117 09:51:31.701662       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX0 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:46:27Z","message":"NodeInstallerProgressing: 1 nodes are at revision 10; 2 nodes are at revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:51:31.716927404Z I1117 09:51:31.716850       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX0 is unhealthy"
2025-11-17T09:51:31.738184616Z I1117 09:51:31.738130       1 status_controller.go:213] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, XXXXXX0 is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:46:27Z","message":"NodeInstallerProgressing: 1 nodes are at revision 10; 2 nodes are at revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:22:17Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX0 is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:19Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2025-11-17T09:51:31.748415676Z I1117 09:51:31.748345       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"14c88512-1544-4457-ad99-ad37b830e151", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11\nEtcdMembersAvailable: 2 of 3 members are available, XXXXXX0 is unhealthy"
2025-11-17T09:51:32.901828188Z I1117 09:51:32.901762       1 request.go:697] Waited for 1.175132035s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2025-11-17T09:51:34.101666346Z I1117 09:51:34.101549       1 request.go:697] Waited for 1.59551586s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2025-11-17T09:51:34.307192198Z I1117 09:51:34.307125       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:51:35.301421269Z I1117 09:51:35.301345       1 request.go:697] Waited for 1.595320589s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX2
2025-11-17T09:51:36.301815040Z I1117 09:51:36.301743       1 request.go:697] Waited for 1.397424046s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:51:36.340606620Z W1117 09:51:36.340536       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX0]
2025-11-17T09:51:37.501561474Z I1117 09:51:37.501503       1 request.go:697] Waited for 1.197263813s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:51:38.501997569Z I1117 09:51:38.501946       1 request.go:697] Waited for 1.39773629s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-XXXXXX0
2025-11-17T09:51:38.505415301Z I1117 09:51:38.505380       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2025-11-17T09:51:44.761602525Z I1117 09:51:44.760715       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:51:45.943604717Z I1117 09:51:45.943550       1 request.go:697] Waited for 1.170971082s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2025-11-17T09:51:46.750220281Z E1117 09:51:46.750160       1 base_controller.go:268] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, XXXXXX0 is unhealthy
2025-11-17T09:51:46.943842145Z I1117 09:51:46.943613       1 request.go:697] Waited for 1.197432574s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX2
2025-11-17T09:51:47.745939835Z I1117 09:51:47.745876       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:51:48.143768037Z I1117 09:51:48.143705       1 request.go:697] Waited for 1.197507354s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX0
2025-11-17T09:51:49.343730277Z I1117 09:51:49.343668       1 request.go:697] Waited for 1.195109929s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-XXXXXX0
2025-11-17T09:51:50.947013647Z I1117 09:51:50.946949       1 installer_controller.go:512] "XXXXXX0" is in transition to 11, but has not made progress because static pod is pending
2025-11-17T09:51:51.379567494Z W1117 09:51:51.378864       1 etcdcli.go:351] UnhealthyEtcdMember found: [XXXXXX0]
