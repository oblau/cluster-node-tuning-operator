2025-11-17T09:21:44.272998979Z I1117 09:21:44.270632       1 cmd.go:230] Using service-serving-cert provided certificates
2025-11-17T09:21:44.272998979Z I1117 09:21:44.270900       1 observer_polling.go:74] Adding reactor for file "/var/run/secrets/serving-cert/tls.crt"
2025-11-17T09:21:44.272998979Z I1117 09:21:44.270963       1 observer_polling.go:74] Adding reactor for file "/var/run/secrets/serving-cert/tls.key"
2025-11-17T09:21:44.272998979Z I1117 09:21:44.271000       1 observer_polling.go:52] Starting from specified content for file "/etc/insights-operator/server.yaml"
2025-11-17T09:21:44.272998979Z I1117 09:21:44.271017       1 observer_polling.go:52] Starting from specified content for file "/var/run/configmaps/service-ca-bundle/service-ca.crt"
2025-11-17T09:21:44.272998979Z I1117 09:21:44.272415       1 observer_polling.go:159] Starting file observer
2025-11-17T09:21:44.272998979Z I1117 09:21:44.272616       1 observer_polling.go:135] File observer successfully synced
2025-11-17T09:21:44.287108162Z W1117 09:21:44.287036       1 builder.go:230] unable to get owner reference (falling back to namespace): replicasets.apps "insights-operator-74dc4c8545" is forbidden: User "system:serviceaccount:openshift-insights:operator" cannot get resource "replicasets" in API group "apps" in the namespace "openshift-insights"
2025-11-17T09:21:44.288358361Z I1117 09:21:44.288045       1 dynamic_serving_content.go:113] "Loaded a new cert/key pair" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2025-11-17T09:21:45.005994347Z I1117 09:21:45.005784       1 requestheader_controller.go:244] Loaded a new request header values for RequestHeaderAuthRequestController
2025-11-17T09:21:45.006601087Z I1117 09:21:45.006354       1 maxinflight.go:140] "Initialized nonMutatingChan" len=400
2025-11-17T09:21:45.006601087Z I1117 09:21:45.006370       1 maxinflight.go:146] "Initialized mutatingChan" len=200
2025-11-17T09:21:45.006601087Z I1117 09:21:45.006404       1 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["executing","readOnly"]
2025-11-17T09:21:45.006601087Z I1117 09:21:45.006421       1 timing_ratio_histogram.go:203] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=["executing","mutating"]
2025-11-17T09:21:45.006601087Z I1117 09:21:45.006431       1 maxinflight.go:117] "Set denominator for readonly requests" limit=400
2025-11-17T09:21:45.006601087Z I1117 09:21:45.006437       1 maxinflight.go:121] "Set denominator for mutating requests" limit=200
2025-11-17T09:21:45.006715069Z I1117 09:21:45.006686       1 config.go:814] Not requested to run hook priority-and-fairness-config-consumer
2025-11-17T09:21:45.009082551Z I1117 09:21:45.008583       1 operator.go:55] Starting insights-operator v0.0.0-XXXXXX+$Format:%H$
2025-11-17T09:21:45.009082551Z I1117 09:21:45.008817       1 config.go:327] Current config: {"report":false,"storagePath":"/var/lib/insights-operator","interval":"2h","endpoint":"https://console.redhat.com/api/ingress/v1/upload","conditionalGathererEndpoint":"https://console.redhat.com/api/gathering/gathering_rules","pull_report":{"endpoint":"https://console.redhat.com/api/insights-results-aggregator/v2/cluster/%s/reports","delay":"60s","timeout":"3000s","min_retry":"30s"},"impersonate":"system:serviceaccount:openshift-insights:gather","enableGlobalObfuscation":false,"ocm":{"scaEndpoint":"https://api.openshift.com/api/accounts_mgmt/v1/certificates","scaInterval":"8h","scaDisabled":false,"clusterTransferEndpoint":"https://api.openshift.com/api/accounts_mgmt/v1/cluster_transfers/","clusterTransferInterval":"12h"},"disableInsightsAlerts":false,"processingStatusEndpoint":"https://console.redhat.com/api/insights-results-aggregator/v2/cluster/%s/request/%s/status","reportEndpointTechPreview":"https://console.redhat.com/api/insights-results-aggregator/v2/cluster/%s/request/%s/report"}
2025-11-17T09:21:45.009700732Z I1117 09:21:45.009421       1 reflector.go:287] Starting reflector *v1.FeatureGate (10m0s) from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:21:45.009700732Z I1117 09:21:45.009435       1 reflector.go:323] Listing and watching *v1.FeatureGate from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:21:45.010410235Z W1117 09:21:45.010080       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2025-11-17T09:21:45.010410235Z W1117 09:21:45.010092       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2025-11-17T09:21:45.010410235Z I1117 09:21:45.010251       1 simple_featuregate_reader.go:171] Starting feature-gate-detector
2025-11-17T09:21:45.010410235Z I1117 09:21:45.010381       1 reflector.go:287] Starting reflector *v1.ClusterVersion (10m0s) from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:21:45.010410235Z I1117 09:21:45.010387       1 reflector.go:323] Listing and watching *v1.ClusterVersion from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:21:45.010547265Z I1117 09:21:45.010516       1 genericapiserver.go:524] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
2025-11-17T09:21:45.013500838Z I1117 09:21:45.013355       1 operator.go:113] FeatureGates initialized: knownFeatureGates=[AdminNetworkPolicy AdmissionWebhookMatchConditions AlibabaPlatform AutomatedEtcdBackup AzureWorkloadIdentity BuildCSIVolumes CSIDriverSharedResource CloudDualStackNodeIPs DynamicResourceAllocation EventedPLEG ExternalCloudProvider ExternalCloudProviderAzure ExternalCloudProviderExternal ExternalCloudProviderGCP GCPLabelsTags GatewayAPI InsightsConfigAPI MachineAPIOperatorDisableMachineHealthCheckController MachineAPIProviderOpenStack MaxUnavailableStatefulSet NetworkLiveMigration NodeSwap OpenShiftPodSecurityAdmission PrivateHostedZoneAWS RetroactiveDefaultStorageClass RouteExternalCertificate SigstoreImageVerification VSphereStaticIPs ValidatingAdmissionPolicy]
2025-11-17T09:21:45.013500838Z I1117 09:21:45.013412       1 secretconfigobserver.go:215] Configuration set: enabled=false endpoint=https://console.redhat.com/api/ingress/v1/upload conditional_gatherer_endpoint=https://console.redhat.com/api/gathering/gathering_rules interval=2h0m0s token=false reportEndpoint=https://console.redhat.com/api/insights-results-aggregator/v2/cluster/%s/reports initialPollingDelay=1m0s minRetryTime=30s pollingTimeout=50m0s processingStatusEndpoint=https://console.redhat.com/api/insights-results-aggregator/v2/cluster/%s/request/%s/status
2025-11-17T09:21:45.013500838Z I1117 09:21:45.013419       1 secretconfigobserver.go:136] Refreshing configuration from cluster pull secret
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013521       1 event.go:298] Event(v1.ObjectReference{Kind:"Namespace", Namespace:"openshift-insights", Name:"openshift-insights", UID:"", APIVersion:"v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'FeatureGatesInitialized' FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AlibabaPlatform", "AzureWorkloadIdentity", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "PrivateHostedZoneAWS"}, Disabled:[]v1.FeatureGateName{"AdminNetworkPolicy", "AdmissionWebhookMatchConditions", "AutomatedEtcdBackup", "CSIDriverSharedResource", "DynamicResourceAllocation", "EventedPLEG", "ExternalCloudProvider", "ExternalCloudProviderGCP", "GCPLabelsTags", "GatewayAPI", "InsightsConfigAPI", "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack", "MaxUnavailableStatefulSet", "NetworkLiveMigration", "NodeSwap", "OpenShiftPodSecurityAdmission", "RetroactiveDefaultStorageClass", "RouteExternalCertificate", "SigstoreImageVerification", "VSphereStaticIPs", "ValidatingAdmissionPolicy"}}
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013614       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013629       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013713       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013725       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013745       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013750       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013847       1 reflector.go:287] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013854       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013875       1 reflector.go:287] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013882       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:21:45.016356517Z I1117 09:21:45.013933       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-insights.svc\" [serving] validServingFor=[metrics.openshift-insights.svc,metrics.openshift-insights.svc.cluster.local] issuer=\"openshift-service-serving-signer@1763371220\" (2025-11-17 09:20:34 +0000 UTC to 2027-11-17 09:20:35 +0000 UTC (now=2025-11-17 09:21:45.013885266 +0000 UTC))"
2025-11-17T09:21:45.016356517Z I1117 09:21:45.014046       1 reflector.go:287] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:21:45.016356517Z I1117 09:21:45.014053       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:21:45.016356517Z I1117 09:21:45.014156       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2025-11-17T09:21:45.016356517Z I1117 09:21:45.014276       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1763371304\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1763371304\" (2025-11-17 08:21:44 +0000 UTC to 2026-11-17 08:21:44 +0000 UTC (now=2025-11-17 09:21:45.014257727 +0000 UTC))"
2025-11-17T09:21:45.016356517Z I1117 09:21:45.014319       1 secure_serving.go:210] Serving securely on [::]:8443
2025-11-17T09:21:45.016356517Z I1117 09:21:45.014338       1 genericapiserver.go:672] [graceful-termination] waiting for shutdown to be initiated
2025-11-17T09:21:45.016356517Z I1117 09:21:45.014351       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2025-11-17T09:21:45.020351472Z E1117 09:21:45.017700       1 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"openshift-insights.1878c0dbf2695b69", GenerateName:"", Namespace:"openshift-insights", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Namespace", Namespace:"openshift-insights", Name:"openshift-insights", UID:"", APIVersion:"v1", ResourceVersion:"", FieldPath:""}, Reason:"FeatureGatesInitialized", Message:"FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{\"AlibabaPlatform\", \"AzureWorkloadIdentity\", \"BuildCSIVolumes\", \"CloudDualStackNodeIPs\", \"ExternalCloudProviderAzure\", \"ExternalCloudProviderExternal\", \"PrivateHostedZoneAWS\"}, Disabled:[]v1.FeatureGateName{\"AdminNetworkPolicy\", \"AdmissionWebhookMatchConditions\", \"AutomatedEtcdBackup\", \"CSIDriverSharedResource\", \"DynamicResourceAllocation\", \"EventedPLEG\", \"ExternalCloudProvider\", \"ExternalCloudProviderGCP\", \"GCPLabelsTags\", \"GatewayAPI\", \"InsightsConfigAPI\", \"MachineAPIOperatorDisableMachineHealthCheckController\", \"MachineAPIProviderOpenStack\", \"MaxUnavailableStatefulSet\", \"NetworkLiveMigration\", \"NodeSwap\", \"OpenShiftPodSecurityAdmission\", \"RetroactiveDefaultStorageClass\", \"RouteExternalCertificate\", \"SigstoreImageVerification\", \"VSphereStaticIPs\", \"ValidatingAdmissionPolicy\"}}", Source:v1.EventSource{Component:"openshift-insights-operator", Host:""}, FirstTimestamp:time.Date(2025, time.November, 17, 9, 21, 45, 12910953, time.Local), LastTimestamp:time.Date(2025, time.November, 17, 9, 21, 45, 12910953, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:serviceaccount:openshift-insights:operator" cannot create resource "events" in API group "" in the namespace "openshift-insights"' (will not retry!)
2025-11-17T09:21:45.020351472Z I1117 09:21:45.018312       1 secretconfigobserver.go:248] Found cloud.openshift.com token
2025-11-17T09:21:45.020351472Z I1117 09:21:45.018329       1 secretconfigobserver.go:203] Configuration updated: enabled=true endpoint=https://console.redhat.com/api/ingress/v1/upload conditional_gatherer_endpoint=https://console.redhat.com/api/gathering/gathering_rules interval=2h0m0s token=true reportEndpoint=https://console.redhat.com/api/insights-results-aggregator/v2/cluster/%s/reports initialPollingDelay=1m0s minRetryTime=30s pollingTimeout=50m0s processingStatusEndpoint=https://console.redhat.com/api/insights-results-aggregator/v2/cluster/%s/request/%s/status
2025-11-17T09:21:45.020351472Z I1117 09:21:45.018335       1 secretconfigobserver.go:162] Refreshing configuration from cluster secret
2025-11-17T09:21:45.020351472Z I1117 09:21:45.019577       1 secretconfigobserver.go:119] support secret does not exist
2025-11-17T09:21:45.020351472Z I1117 09:21:45.020053       1 secretconfigobserver.go:136] Refreshing configuration from cluster pull secret
2025-11-17T09:21:45.023433412Z I1117 09:21:45.023400       1 secretconfigobserver.go:248] Found cloud.openshift.com token
2025-11-17T09:21:45.023433412Z I1117 09:21:45.023416       1 secretconfigobserver.go:162] Refreshing configuration from cluster secret
2025-11-17T09:21:45.023618317Z I1117 09:21:45.023588       1 recorder.go:155] Pruning old reports every 4h19m13s, max age is 288h0m0s
2025-11-17T09:21:45.024454156Z E1117 09:21:45.024427       1 operator.go:281] Couldn't get Insights Operator Pod to detect its status. Error: pods "insights-operator-74dc4c8545-rzwbg" is forbidden: User "system:serviceaccount:openshift-insights:gather" cannot get resource "pods" in API group "" in the namespace "openshift-insights": RBAC: clusterrole.rbac.authorization.k8s.io "cluster-reader" not found
2025-11-17T09:21:45.025387214Z I1117 09:21:45.024712       1 secretconfigobserver.go:119] support secret does not exist
2025-11-17T09:21:45.025387214Z E1117 09:21:45.025218       1 operator.go:281] Couldn't get Insights Operator Pod to detect its status. Error: pods "insights-operator-74dc4c8545-rzwbg" is forbidden: User "system:serviceaccount:openshift-insights:gather" cannot get resource "pods" in API group "" in the namespace "openshift-insights": RBAC: clusterrole.rbac.authorization.k8s.io "cluster-reader" not found
2025-11-17T09:21:45.115492933Z I1117 09:21:45.115437       1 shared_informer.go:341] caches populated
2025-11-17T09:21:45.115492933Z I1117 09:21:45.115467       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-11-17T09:21:45.115529642Z I1117 09:21:45.115483       1 shared_informer.go:341] caches populated
2025-11-17T09:21:45.115529642Z I1117 09:21:45.115499       1 shared_informer.go:341] caches populated
2025-11-17T09:21:45.115529642Z I1117 09:21:45.115513       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-11-17T09:21:45.115547462Z I1117 09:21:45.115503       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
2025-11-17T09:21:45.115786493Z I1117 09:21:45.115760       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:28 +0000 UTC to 2025-11-18 09:01:28 +0000 UTC (now=2025-11-17 09:21:45.115714474 +0000 UTC))"
2025-11-17T09:21:45.116215164Z I1117 09:21:45.116189       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-insights.svc\" [serving] validServingFor=[metrics.openshift-insights.svc,metrics.openshift-insights.svc.cluster.local] issuer=\"openshift-service-serving-signer@1763371220\" (2025-11-17 09:20:34 +0000 UTC to 2027-11-17 09:20:35 +0000 UTC (now=2025-11-17 09:21:45.116171578 +0000 UTC))"
2025-11-17T09:21:45.116599409Z I1117 09:21:45.116578       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1763371304\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1763371304\" (2025-11-17 08:21:44 +0000 UTC to 2026-11-17 08:21:44 +0000 UTC (now=2025-11-17 09:21:45.116562973 +0000 UTC))"
2025-11-17T09:21:45.116712186Z I1117 09:21:45.116696       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2025-11-17 08:23:44 +0000 UTC to 2035-11-15 08:23:44 +0000 UTC (now=2025-11-17 09:21:45.116682531 +0000 UTC))"
2025-11-17T09:21:45.116730571Z I1117 09:21:45.116721       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:29 +0000 UTC to 2025-11-18 09:01:29 +0000 UTC (now=2025-11-17 09:21:45.116709512 +0000 UTC))"
2025-11-17T09:21:45.116782491Z I1117 09:21:45.116768       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:29 +0000 UTC to 2026-11-17 09:01:29 +0000 UTC (now=2025-11-17 09:21:45.11672775 +0000 UTC))"
2025-11-17T09:21:45.116806685Z I1117 09:21:45.116792       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:29 +0000 UTC to 2026-11-17 09:01:29 +0000 UTC (now=2025-11-17 09:21:45.116780107 +0000 UTC))"
2025-11-17T09:21:45.116824911Z I1117 09:21:45.116812       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:27 +0000 UTC to 2035-11-15 09:01:27 +0000 UTC (now=2025-11-17 09:21:45.116801986 +0000 UTC))"
2025-11-17T09:21:45.116844691Z I1117 09:21:45.116832       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:28 +0000 UTC to 2025-11-18 09:01:28 +0000 UTC (now=2025-11-17 09:21:45.116821906 +0000 UTC))"
2025-11-17T09:21:45.117255571Z I1117 09:21:45.117240       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-insights.svc\" [serving] validServingFor=[metrics.openshift-insights.svc,metrics.openshift-insights.svc.cluster.local] issuer=\"openshift-service-serving-signer@1763371220\" (2025-11-17 09:20:34 +0000 UTC to 2027-11-17 09:20:35 +0000 UTC (now=2025-11-17 09:21:45.117225902 +0000 UTC))"
2025-11-17T09:21:45.117690684Z I1117 09:21:45.117652       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1763371304\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1763371304\" (2025-11-17 08:21:44 +0000 UTC to 2026-11-17 08:21:44 +0000 UTC (now=2025-11-17 09:21:45.117634208 +0000 UTC))"
2025-11-17T09:22:05.032852477Z E1117 09:22:05.031833       1 operator.go:281] Couldn't get Insights Operator Pod to detect its status. Error: pods "insights-operator-74dc4c8545-rzwbg" is forbidden: User "system:serviceaccount:openshift-insights:gather" cannot get resource "pods" in API group "" in the namespace "openshift-insights": RBAC: clusterrole.rbac.authorization.k8s.io "cluster-reader" not found
2025-11-17T09:22:25.028374244Z E1117 09:22:25.028275       1 operator.go:281] Couldn't get Insights Operator Pod to detect its status. Error: pods "insights-operator-74dc4c8545-rzwbg" is forbidden: User "system:serviceaccount:openshift-insights:gather" cannot get resource "pods" in API group "" in the namespace "openshift-insights": RBAC: clusterrole.rbac.authorization.k8s.io "cluster-reader" not found
2025-11-17T09:22:45.030371206Z I1117 09:22:45.030303       1 operator.go:288] The last pod state is unhealthy
2025-11-17T09:23:05.029863017Z I1117 09:23:05.029778       1 operator.go:288] The last pod state is unhealthy
2025-11-17T09:23:25.031463807Z I1117 09:23:25.031322       1 operator.go:288] The last pod state is unhealthy
2025-11-17T09:23:45.030611002Z I1117 09:23:45.030546       1 operator.go:288] The last pod state is unhealthy
2025-11-17T09:23:45.253314503Z I1117 09:23:45.253251       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2025-11-17 08:23:44 +0000 UTC to 2035-11-15 08:23:44 +0000 UTC (now=2025-11-17 09:23:45.252605657 +0000 UTC))"
2025-11-17T09:23:45.253400164Z I1117 09:23:45.253389       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:29 +0000 UTC to 2025-11-18 09:01:29 +0000 UTC (now=2025-11-17 09:23:45.253360327 +0000 UTC))"
2025-11-17T09:23:45.253438067Z I1117 09:23:45.253430       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:29 +0000 UTC to 2026-11-17 09:01:29 +0000 UTC (now=2025-11-17 09:23:45.253416848 +0000 UTC))"
2025-11-17T09:23:45.253478683Z I1117 09:23:45.253471       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:29 +0000 UTC to 2026-11-17 09:01:29 +0000 UTC (now=2025-11-17 09:23:45.253452436 +0000 UTC))"
2025-11-17T09:23:45.253512315Z I1117 09:23:45.253505       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:27 +0000 UTC to 2035-11-15 09:01:27 +0000 UTC (now=2025-11-17 09:23:45.253492555 +0000 UTC))"
2025-11-17T09:23:45.253545693Z I1117 09:23:45.253538       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1763371225\" [] issuer=\"kubelet-signer\" (2025-11-17 09:20:25 +0000 UTC to 2025-11-18 09:01:29 +0000 UTC (now=2025-11-17 09:23:45.253525604 +0000 UTC))"
2025-11-17T09:23:45.253585543Z I1117 09:23:45.253573       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1763371229\" [] issuer=\"<self>\" (2025-11-17 09:20:28 +0000 UTC to 2026-11-17 09:20:29 +0000 UTC (now=2025-11-17 09:23:45.253558328 +0000 UTC))"
2025-11-17T09:23:45.253619579Z I1117 09:23:45.253612       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2025-11-17 09:01:28 +0000 UTC to 2025-11-18 09:01:28 +0000 UTC (now=2025-11-17 09:23:45.253599866 +0000 UTC))"
2025-11-17T09:23:45.254206510Z I1117 09:23:45.254188       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-insights.svc\" [serving] validServingFor=[metrics.openshift-insights.svc,metrics.openshift-insights.svc.cluster.local] issuer=\"openshift-service-serving-signer@1763371220\" (2025-11-17 09:20:34 +0000 UTC to 2027-11-17 09:20:35 +0000 UTC (now=2025-11-17 09:23:45.254153912 +0000 UTC))"
2025-11-17T09:23:45.254668634Z I1117 09:23:45.254653       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1763371304\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1763371304\" (2025-11-17 08:21:44 +0000 UTC to 2026-11-17 08:21:44 +0000 UTC (now=2025-11-17 09:23:45.254620376 +0000 UTC))"
2025-11-17T09:24:05.029805411Z I1117 09:24:05.029757       1 operator.go:288] The last pod state is unhealthy
2025-11-17T09:24:25.030436435Z I1117 09:24:25.030297       1 operator.go:288] The last pod state is unhealthy
2025-11-17T09:24:45.034176308Z I1117 09:24:45.034128       1 operator.go:288] The last pod state is unhealthy
2025-11-17T09:26:05.027386106Z E1117 09:26:05.027338       1 operator.go:281] Couldn't get Insights Operator Pod to detect its status. Error: the server was unable to return a response in the time allotted, but may still be processing the request (get pods insights-operator-74dc4c8545-rzwbg)
2025-11-17T09:26:10.029205398Z I1117 09:26:10.029073       1 with_retry.go:234] Got a Retry-After 5s response for attempt 1 to https://172.30.0.1:443/api/v1/namespaces/openshift-insights/pods/insights-operator-74dc4c8545-rzwbg
2025-11-17T09:26:45.025829933Z I1117 09:26:45.025593       1 secretconfigobserver.go:136] Refreshing configuration from cluster pull secret
2025-11-17T09:27:04.978321941Z I1117 09:27:04.977200       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.ClusterVersion total 14 items received
2025-11-17T09:27:04.978321941Z I1117 09:27:04.977597       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.FeatureGate total 5 items received
2025-11-17T09:27:04.978321941Z I1117 09:27:04.977750       1 reflector.go:788] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: Watch close - *v1.ConfigMap total 6 items received
2025-11-17T09:27:04.978321941Z I1117 09:27:04.977953       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 6 items received
2025-11-17T09:27:04.978321941Z I1117 09:27:04.978116       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 6 items received
2025-11-17T09:27:10.038232142Z E1117 09:27:10.038147       1 operator.go:281] Couldn't get Insights Operator Pod to detect its status. Error: the server was unable to return a response in the time allotted, but may still be processing the request (get pods insights-operator-74dc4c8545-rzwbg)
2025-11-17T09:27:10.043158320Z E1117 09:27:10.043102       1 operator.go:281] Couldn't get Insights Operator Pod to detect its status. Error: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-insights/pods/insights-operator-74dc4c8545-rzwbg": context deadline exceeded
2025-11-17T09:27:10.043158320Z I1117 09:27:10.043140       1 operator.go:222] Unable to check insights-operator pod status. Setting initial delay to 12m0.975780609s
2025-11-17T09:27:10.043318071Z I1117 09:27:10.043283       1 controllerstatus.go:77] name=insightsuploader healthy=true reason= message=
2025-11-17T09:27:10.043365888Z I1117 09:27:10.043335       1 insightsuploader.go:94] Reporting status periodically to https://console.redhat.com/api/ingress/v1/upload every 2h0m0s, starting in 12m0s
2025-11-17T09:27:10.043365888Z I1117 09:27:10.043300       1 controllerstatus.go:77] name=insightsreport healthy=true reason= message=
2025-11-17T09:27:10.043372931Z I1117 09:27:10.043364       1 insightsreport.go:295] Starting report retriever
2025-11-17T09:27:10.043389225Z I1117 09:27:10.043372       1 insightsreport.go:297] Insights analysis reports will be downloaded from the https://console.redhat.com/api/insights-results-aggregator/v2/cluster/%s/reports endpoint with a delay of 1m0s
2025-11-17T09:27:12.988702490Z I1117 09:27:12.987976       1 controller.go:116] Initializing last reported time to 0001-01-01T00:00:00Z
2025-11-17T09:27:12.988702490Z I1117 09:27:12.988022       1 controller.go:200] Source periodic-clusterconfig *controllerstatus.Simple is not ready
2025-11-17T09:27:12.988702490Z I1117 09:27:12.988034       1 controller.go:200] Source periodic-conditional *controllerstatus.Simple is not ready
2025-11-17T09:27:12.988702490Z I1117 09:27:12.988039       1 controller.go:200] Source periodic-workloads *controllerstatus.Simple is not ready
2025-11-17T09:27:12.988702490Z I1117 09:27:12.988070       1 controller.go:419] The operator is still being initialized
2025-11-17T09:27:12.988702490Z I1117 09:27:12.988078       1 controller.go:444] The operator is healthy
2025-11-17T09:27:12.994594966Z I1117 09:27:12.993202       1 secretconfigobserver.go:248] Found cloud.openshift.com token
2025-11-17T09:27:12.994594966Z I1117 09:27:12.993244       1 secretconfigobserver.go:162] Refreshing configuration from cluster secret
2025-11-17T09:27:13.018601108Z I1117 09:27:13.017851       1 secretconfigobserver.go:119] support secret does not exist
2025-11-17T09:27:13.111808482Z W1117 09:27:13.111738       1 operator.go:260] started
2025-11-17T09:27:13.112604077Z I1117 09:27:13.112561       1 sca.go:99] Pulling SCA certificates from https://api.openshift.com/api/accounts_mgmt/v1/certificates. Next check is in 8h0m0s
2025-11-17T09:27:13.112734493Z I1117 09:27:13.112715       1 cluster_transfer.go:78] checking the availability of cluster transfer. Next check is in 12h0m0s
2025-11-17T09:27:13.143362182Z I1117 09:27:13.141956       1 controller.go:200] Source periodic-clusterconfig *controllerstatus.Simple is not ready
2025-11-17T09:27:13.143362182Z I1117 09:27:13.141991       1 controller.go:200] Source periodic-conditional *controllerstatus.Simple is not ready
2025-11-17T09:27:13.143362182Z I1117 09:27:13.141999       1 controller.go:200] Source periodic-workloads *controllerstatus.Simple is not ready
2025-11-17T09:27:13.143362182Z I1117 09:27:13.142009       1 controller.go:200] Source scaController *sca.Controller is not ready
2025-11-17T09:27:13.143362182Z I1117 09:27:13.142025       1 controller.go:200] Source clusterTransferController *clustertransfer.Controller is not ready
2025-11-17T09:27:13.143362182Z I1117 09:27:13.142048       1 controller.go:419] The operator is still being initialized
2025-11-17T09:27:13.143362182Z I1117 09:27:13.142052       1 controller.go:444] The operator is healthy
2025-11-17T09:27:13.143362182Z I1117 09:27:13.142090       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:27:13.158327172Z I1117 09:27:13.155003       1 prometheus_rules.go:83] Prometheus rules successfully created
2025-11-17T09:27:13.818139905Z I1117 09:27:13.817151       1 cluster_transfer.go:97] no available accepted cluster transfer
2025-11-17T09:27:13.818139905Z I1117 09:27:13.817174       1 controllerstatus.go:77] name=clusterTransferController healthy=true reason=NoClusterTransfer message=no available cluster transfer
2025-11-17T09:27:13.996095581Z I1117 09:27:13.994279       1 reflector.go:443] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.ClusterVersion closed with: too old resource version: 14051 (14231)
2025-11-17T09:27:13.996095581Z I1117 09:27:13.995555       1 reflector.go:443] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.FeatureGate closed with: too old resource version: 14027 (14231)
2025-11-17T09:27:14.017987110Z I1117 09:27:14.017633       1 reflector.go:443] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap closed with: too old resource version: 14055 (14231)
2025-11-17T09:27:14.017987110Z I1117 09:27:14.017781       1 reflector.go:443] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap closed with: too old resource version: 14055 (14231)
2025-11-17T09:27:14.020067355Z I1117 09:27:14.019662       1 reflector.go:443] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: watch of *v1.ConfigMap closed with: too old resource version: 14055 (14231)
2025-11-17T09:27:14.233819684Z I1117 09:27:14.232433       1 sca.go:141] etc-pki-entitlement secret successfully updated
2025-11-17T09:27:14.233819684Z I1117 09:27:14.232463       1 controllerstatus.go:77] name=scaController healthy=true reason=Updated message=SCA certs successfully updated in the etc-pki-entitlement secret
2025-11-17T09:27:15.066412875Z I1117 09:27:15.065738       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172
2025-11-17T09:27:15.209134353Z I1117 09:27:15.209084       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:27:15.265329975Z I1117 09:27:15.264658       1 reflector.go:323] Listing and watching *v1.ClusterVersion from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:27:15.294340274Z I1117 09:27:15.293829       1 reflector.go:323] Listing and watching *v1.FeatureGate from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:27:15.463219243Z I1117 09:27:15.463159       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:27:20.068354539Z I1117 09:27:20.067438       1 with_retry.go:234] Got a Retry-After 5s response for attempt 1 to https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=14055
2025-11-17T09:29:13.151212719Z I1117 09:29:13.151131       1 controller.go:200] Source periodic-clusterconfig *controllerstatus.Simple is not ready
2025-11-17T09:29:13.151212719Z I1117 09:29:13.151155       1 controller.go:200] Source periodic-conditional *controllerstatus.Simple is not ready
2025-11-17T09:29:13.151212719Z I1117 09:29:13.151160       1 controller.go:200] Source periodic-workloads *controllerstatus.Simple is not ready
2025-11-17T09:29:13.151212719Z I1117 09:29:13.151181       1 controller.go:419] The operator is still being initialized
2025-11-17T09:29:13.151212719Z I1117 09:29:13.151185       1 controller.go:444] The operator is healthy
2025-11-17T09:29:13.151281538Z I1117 09:29:13.151262       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:31:28.134509541Z I1117 09:31:28.133478       1 reflector.go:788] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: Watch close - *v1.ConfigMap total 4 items received
2025-11-17T09:31:28.143189614Z I1117 09:31:28.138698       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 4 items received
2025-11-17T09:31:28.143189614Z I1117 09:31:28.138921       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 4 items received
2025-11-17T09:31:28.150896227Z I1117 09:31:28.149527       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.FeatureGate total 4 items received
2025-11-17T09:31:28.169323657Z I1117 09:31:28.165960       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.ClusterVersion total 16 items received
2025-11-17T09:32:13.018527255Z I1117 09:32:13.018469       1 secretconfigobserver.go:136] Refreshing configuration from cluster pull secret
2025-11-17T09:32:13.118991981Z E1117 09:32:13.118940       1 controller.go:284] Unable to write cluster operator status: the server was unable to return a response in the time allotted, but may still be processing the request (get clusteroperators.config.openshift.io insights)
2025-11-17T09:32:50.485796770Z I1117 09:32:50.484779       1 reflector.go:443] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.ClusterVersion closed with: too old resource version: 18945 (19024)
2025-11-17T09:32:50.535905081Z I1117 09:32:50.535807       1 reflector.go:443] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.FeatureGate closed with: too old resource version: 18968 (19024)
2025-11-17T09:32:50.597847227Z I1117 09:32:50.597773       1 reflector.go:443] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap closed with: too old resource version: 18961 (19024)
2025-11-17T09:32:50.598808991Z I1117 09:32:50.598015       1 reflector.go:443] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: watch of *v1.ConfigMap closed with: too old resource version: 18961 (19024)
2025-11-17T09:32:50.611654095Z I1117 09:32:50.610944       1 reflector.go:443] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap closed with: too old resource version: 18961 (19024)
2025-11-17T09:32:50.718656966Z I1117 09:32:50.717678       1 secretconfigobserver.go:248] Found cloud.openshift.com token
2025-11-17T09:32:50.718656966Z I1117 09:32:50.717740       1 secretconfigobserver.go:162] Refreshing configuration from cluster secret
2025-11-17T09:32:51.429929713Z I1117 09:32:51.429686       1 reflector.go:323] Listing and watching *v1.FeatureGate from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:32:51.599189520Z I1117 09:32:51.599131       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:32:51.625773728Z I1117 09:32:51.625085       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:32:52.010198437Z I1117 09:32:52.009398       1 reflector.go:323] Listing and watching *v1.ClusterVersion from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:32:52.164413462Z I1117 09:32:52.160655       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172
2025-11-17T09:32:52.164413462Z W1117 09:32:52.161276       1 reflector.go:533] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: failed to list *v1.ConfigMap: Get "https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=18961": dial tcp 172.30.0.1:443: connect: connection refused
2025-11-17T09:32:52.164413462Z E1117 09:32:52.161366       1 reflector.go:148] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=18961": dial tcp 172.30.0.1:443: connect: connection refused
2025-11-17T09:32:53.983097633Z I1117 09:32:53.982946       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172
2025-11-17T09:32:53.983657459Z W1117 09:32:53.983617       1 reflector.go:533] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: failed to list *v1.ConfigMap: Get "https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=18961": dial tcp 172.30.0.1:443: connect: connection refused
2025-11-17T09:32:53.983657459Z E1117 09:32:53.983649       1 reflector.go:148] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=18961": dial tcp 172.30.0.1:443: connect: connection refused
2025-11-17T09:32:55.718844818Z I1117 09:32:55.718751       1 with_retry.go:234] Got a Retry-After 5s response for attempt 1 to https://172.30.0.1:443/api/v1/namespaces/openshift-config/secrets/support
2025-11-17T09:32:55.748725146Z I1117 09:32:55.748657       1 secretconfigobserver.go:119] support secret does not exist
2025-11-17T09:32:56.439554665Z I1117 09:32:56.439494       1 with_retry.go:234] Got a Retry-After 5s response for attempt 1 to https://172.30.0.1:443/apis/config.openshift.io/v1/featuregates?resourceVersion=18968
2025-11-17T09:32:56.612563800Z I1117 09:32:56.610038       1 with_retry.go:234] Got a Retry-After 5s response for attempt 1 to https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=18961
2025-11-17T09:32:56.636156980Z I1117 09:32:56.634902       1 with_retry.go:234] Got a Retry-After 5s response for attempt 1 to https://172.30.0.1:443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=18961
2025-11-17T09:32:57.035335734Z I1117 09:32:57.031920       1 with_retry.go:234] Got a Retry-After 5s response for attempt 1 to https://172.30.0.1:443/apis/config.openshift.io/v1/clusterversions?resourceVersion=18945
2025-11-17T09:32:59.845498982Z I1117 09:32:59.845344       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172
2025-11-17T09:33:13.119500813Z I1117 09:33:13.119409       1 controller.go:200] Source periodic-clusterconfig *controllerstatus.Simple is not ready
2025-11-17T09:33:13.119500813Z I1117 09:33:13.119447       1 controller.go:200] Source periodic-conditional *controllerstatus.Simple is not ready
2025-11-17T09:33:13.119500813Z I1117 09:33:13.119452       1 controller.go:200] Source periodic-workloads *controllerstatus.Simple is not ready
2025-11-17T09:33:13.119564049Z I1117 09:33:13.119499       1 controller.go:444] The operator is healthy
2025-11-17T09:35:18.118512748Z I1117 09:35:18.118429       1 with_retry.go:234] Got a Retry-After 5s response for attempt 1 to https://172.30.0.1:443/apis/config.openshift.io/v1/clusteroperators/insights
2025-11-17T09:35:32.371949558Z I1117 09:35:32.371504       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.ClusterVersion total 2 items received
2025-11-17T09:35:32.376708053Z I1117 09:35:32.376626       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.FeatureGate total 2 items received
2025-11-17T09:35:32.385131078Z I1117 09:35:32.384026       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 2 items received
2025-11-17T09:35:32.392099554Z I1117 09:35:32.391948       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 2 items received
2025-11-17T09:35:32.393347190Z I1117 09:35:32.392221       1 reflector.go:788] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: Watch close - *v1.ConfigMap total 2 items received
2025-11-17T09:36:01.683361683Z I1117 09:36:01.681197       1 reflector.go:443] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.ClusterVersion closed with: too old resource version: 21048 (21068)
2025-11-17T09:36:01.690831026Z I1117 09:36:01.690732       1 reflector.go:443] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.FeatureGate closed with: too old resource version: 21049 (21068)
2025-11-17T09:36:01.781354488Z I1117 09:36:01.780906       1 reflector.go:443] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap closed with: too old resource version: 21028 (21068)
2025-11-17T09:36:01.781354488Z I1117 09:36:01.781026       1 reflector.go:443] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: watch of *v1.ConfigMap closed with: too old resource version: 21028 (21068)
2025-11-17T09:36:01.782135743Z I1117 09:36:01.781785       1 reflector.go:443] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap closed with: too old resource version: 21028 (21068)
2025-11-17T09:36:02.657142066Z I1117 09:36:02.655373       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:36:02.684328340Z I1117 09:36:02.682573       1 reflector.go:323] Listing and watching *v1.FeatureGate from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:36:03.107552934Z I1117 09:36:03.101351       1 reflector.go:323] Listing and watching *v1.ClusterVersion from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:36:03.107685130Z I1117 09:36:03.107663       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:36:03.290895686Z I1117 09:36:03.290854       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172
2025-11-17T09:36:04.153111270Z I1117 09:36:04.148570       1 controller.go:200] Source periodic-workloads *controllerstatus.Simple is not ready
2025-11-17T09:36:04.153111270Z I1117 09:36:04.148595       1 controller.go:200] Source periodic-clusterconfig *controllerstatus.Simple is not ready
2025-11-17T09:36:04.153111270Z I1117 09:36:04.148600       1 controller.go:200] Source periodic-conditional *controllerstatus.Simple is not ready
2025-11-17T09:36:04.153111270Z I1117 09:36:04.148619       1 controller.go:444] The operator is healthy
2025-11-17T09:36:04.153111270Z I1117 09:36:04.148662       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:36:42.702936474Z I1117 09:36:42.702857       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 0 items received
2025-11-17T09:36:42.707348198Z I1117 09:36:42.706935       1 reflector.go:788] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: Watch close - *v1.ConfigMap total 0 items received
2025-11-17T09:36:42.707348198Z I1117 09:36:42.707224       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 0 items received
2025-11-17T09:36:42.752699129Z I1117 09:36:42.752650       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.FeatureGate total 0 items received
2025-11-17T09:36:42.772388291Z I1117 09:36:42.770616       1 reflector.go:443] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.FeatureGate closed with: too old resource version: 21068 (21716)
2025-11-17T09:36:42.772388291Z I1117 09:36:42.770746       1 reflector.go:443] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap closed with: too old resource version: 21100 (21306)
2025-11-17T09:36:42.773348058Z I1117 09:36:42.772900       1 reflector.go:443] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap closed with: too old resource version: 21100 (21306)
2025-11-17T09:36:42.773348058Z I1117 09:36:42.772983       1 reflector.go:443] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: watch of *v1.ConfigMap closed with: too old resource version: 21100 (21306)
2025-11-17T09:36:42.775369890Z I1117 09:36:42.774251       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.ClusterVersion total 0 items received
2025-11-17T09:36:42.802540413Z I1117 09:36:42.801706       1 reflector.go:443] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.ClusterVersion closed with: too old resource version: 21068 (21716)
2025-11-17T09:36:44.551913215Z I1117 09:36:44.551863       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:36:44.603781816Z I1117 09:36:44.603689       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172
2025-11-17T09:36:44.794043647Z I1117 09:36:44.793871       1 reflector.go:323] Listing and watching *v1.ConfigMap from k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206
2025-11-17T09:36:45.870842264Z I1117 09:36:45.870791       1 reflector.go:323] Listing and watching *v1.ClusterVersion from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:36:45.929445529Z I1117 09:36:45.928443       1 reflector.go:323] Listing and watching *v1.FeatureGate from github.com/openshift/client-go/config/informers/externalversions/factory.go:101
2025-11-17T09:37:13.119618769Z I1117 09:37:13.119507       1 controller.go:200] Source periodic-clusterconfig *controllerstatus.Simple is not ready
2025-11-17T09:37:13.119618769Z I1117 09:37:13.119536       1 controller.go:200] Source periodic-conditional *controllerstatus.Simple is not ready
2025-11-17T09:37:13.119618769Z I1117 09:37:13.119541       1 controller.go:200] Source periodic-workloads *controllerstatus.Simple is not ready
2025-11-17T09:37:13.119618769Z I1117 09:37:13.119571       1 controller.go:444] The operator is healthy
2025-11-17T09:37:13.119690148Z I1117 09:37:13.119659       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:37:55.749515463Z I1117 09:37:55.749425       1 secretconfigobserver.go:136] Refreshing configuration from cluster pull secret
2025-11-17T09:37:55.755728741Z I1117 09:37:55.755695       1 secretconfigobserver.go:248] Found cloud.openshift.com token
2025-11-17T09:37:55.755747391Z I1117 09:37:55.755741       1 secretconfigobserver.go:162] Refreshing configuration from cluster secret
2025-11-17T09:37:55.758409492Z I1117 09:37:55.757760       1 secretconfigobserver.go:119] support secret does not exist
2025-11-17T09:39:11.019533700Z I1117 09:39:11.019396       1 insightsuploader.go:129] Nothing to report since 0001-01-01T00:00:00Z
2025-11-17T09:39:11.019533700Z I1117 09:39:11.019405       1 periodic.go:204] Running clusterconfig gatherer
2025-11-17T09:39:11.019533700Z I1117 09:39:11.019526       1 tasks_processing.go:45] number of XXXXXXs: 48
2025-11-17T09:39:11.019615784Z I1117 09:39:11.019595       1 tasks_processing.go:69] XXXXXX 47 listening for tasks.
2025-11-17T09:39:11.019615784Z I1117 09:39:11.019603       1 tasks_processing.go:71] XXXXXX 47 working on machine_configs task.
2025-11-17T09:39:11.019638379Z I1117 09:39:11.019614       1 tasks_processing.go:69] XXXXXX 23 listening for tasks.
2025-11-17T09:39:11.019638379Z I1117 09:39:11.019633       1 tasks_processing.go:69] XXXXXX 0 listening for tasks.
2025-11-17T09:39:11.019644896Z I1117 09:39:11.019637       1 tasks_processing.go:69] XXXXXX 1 listening for tasks.
2025-11-17T09:39:11.019644896Z I1117 09:39:11.019641       1 tasks_processing.go:69] XXXXXX 2 listening for tasks.
2025-11-17T09:39:11.019650415Z I1117 09:39:11.019644       1 tasks_processing.go:69] XXXXXX 3 listening for tasks.
2025-11-17T09:39:11.019650415Z I1117 09:39:11.019648       1 tasks_processing.go:69] XXXXXX 4 listening for tasks.
2025-11-17T09:39:11.019655962Z I1117 09:39:11.019652       1 tasks_processing.go:69] XXXXXX 5 listening for tasks.
2025-11-17T09:39:11.019661475Z I1117 09:39:11.019655       1 tasks_processing.go:69] XXXXXX 6 listening for tasks.
2025-11-17T09:39:11.019661475Z I1117 09:39:11.019659       1 tasks_processing.go:69] XXXXXX 7 listening for tasks.
2025-11-17T09:39:11.019666992Z I1117 09:39:11.019662       1 tasks_processing.go:69] XXXXXX 8 listening for tasks.
2025-11-17T09:39:11.019672005Z I1117 09:39:11.019665       1 tasks_processing.go:69] XXXXXX 9 listening for tasks.
2025-11-17T09:39:11.019672005Z I1117 09:39:11.019669       1 tasks_processing.go:69] XXXXXX 10 listening for tasks.
2025-11-17T09:39:11.019677335Z I1117 09:39:11.019673       1 tasks_processing.go:69] XXXXXX 11 listening for tasks.
2025-11-17T09:39:11.019682616Z I1117 09:39:11.019676       1 tasks_processing.go:69] XXXXXX 12 listening for tasks.
2025-11-17T09:39:11.019682616Z I1117 09:39:11.019680       1 tasks_processing.go:69] XXXXXX 13 listening for tasks.
2025-11-17T09:39:11.019696865Z I1117 09:39:11.019688       1 tasks_processing.go:69] XXXXXX 14 listening for tasks.
2025-11-17T09:39:11.019703257Z I1117 09:39:11.019696       1 tasks_processing.go:69] XXXXXX 15 listening for tasks.
2025-11-17T09:39:11.019710006Z I1117 09:39:11.019703       1 tasks_processing.go:69] XXXXXX 20 listening for tasks.
2025-11-17T09:39:11.019731346Z I1117 09:39:11.019715       1 tasks_processing.go:69] XXXXXX 21 listening for tasks.
2025-11-17T09:39:11.019731346Z I1117 09:39:11.019709       1 tasks_processing.go:69] XXXXXX 35 listening for tasks.
2025-11-17T09:39:11.019741215Z I1117 09:39:11.019735       1 tasks_processing.go:69] XXXXXX 17 listening for tasks.
2025-11-17T09:39:11.019749233Z I1117 09:39:11.019725       1 tasks_processing.go:69] XXXXXX 22 listening for tasks.
2025-11-17T09:39:11.019749233Z I1117 09:39:11.019731       1 tasks_processing.go:69] XXXXXX 19 listening for tasks.
2025-11-17T09:39:11.019762150Z I1117 09:39:11.019748       1 tasks_processing.go:69] XXXXXX 16 listening for tasks.
2025-11-17T09:39:11.019762150Z I1117 09:39:11.019750       1 tasks_processing.go:69] XXXXXX 24 listening for tasks.
2025-11-17T09:39:11.019762150Z I1117 09:39:11.019757       1 tasks_processing.go:69] XXXXXX 18 listening for tasks.
2025-11-17T09:39:11.019770946Z I1117 09:39:11.019757       1 tasks_processing.go:69] XXXXXX 29 listening for tasks.
2025-11-17T09:39:11.019770946Z I1117 09:39:11.019763       1 tasks_processing.go:69] XXXXXX 25 listening for tasks.
2025-11-17T09:39:11.019779290Z I1117 09:39:11.019767       1 tasks_processing.go:69] XXXXXX 30 listening for tasks.
2025-11-17T09:39:11.019779290Z I1117 09:39:11.019769       1 tasks_processing.go:69] XXXXXX 26 listening for tasks.
2025-11-17T09:39:11.019802077Z I1117 09:39:11.019775       1 tasks_processing.go:69] XXXXXX 28 listening for tasks.
2025-11-17T09:39:11.019802077Z I1117 09:39:11.019781       1 tasks_processing.go:69] XXXXXX 41 listening for tasks.
2025-11-17T09:39:11.019809920Z I1117 09:39:11.019775       1 tasks_processing.go:69] XXXXXX 27 listening for tasks.
2025-11-17T09:39:11.019817744Z I1117 09:39:11.019806       1 tasks_processing.go:69] XXXXXX 34 listening for tasks.
2025-11-17T09:39:11.019826091Z I1117 09:39:11.019785       1 tasks_processing.go:69] XXXXXX 32 listening for tasks.
2025-11-17T09:39:11.019826091Z I1117 09:39:11.019791       1 tasks_processing.go:69] XXXXXX 36 listening for tasks.
2025-11-17T09:39:11.019833429Z I1117 09:39:11.019825       1 tasks_processing.go:69] XXXXXX 43 listening for tasks.
2025-11-17T09:39:11.019833429Z I1117 09:39:11.019791       1 tasks_processing.go:69] XXXXXX 31 listening for tasks.
2025-11-17T09:39:11.019833429Z I1117 09:39:11.019798       1 tasks_processing.go:69] XXXXXX 37 listening for tasks.
2025-11-17T09:39:11.019842151Z I1117 09:39:11.019799       1 tasks_processing.go:69] XXXXXX 33 listening for tasks.
2025-11-17T09:39:11.019842151Z I1117 09:39:11.019807       1 tasks_processing.go:69] XXXXXX 44 listening for tasks.
2025-11-17T09:39:11.019842151Z I1117 09:39:11.019807       1 tasks_processing.go:69] XXXXXX 45 listening for tasks.
2025-11-17T09:39:11.019842151Z I1117 09:39:11.019784       1 tasks_processing.go:69] XXXXXX 38 listening for tasks.
2025-11-17T09:39:11.019842151Z I1117 09:39:11.019813       1 tasks_processing.go:69] XXXXXX 42 listening for tasks.
2025-11-17T09:39:11.019842151Z I1117 09:39:11.019817       1 tasks_processing.go:69] XXXXXX 39 listening for tasks.
2025-11-17T09:39:11.019851192Z I1117 09:39:11.019822       1 tasks_processing.go:69] XXXXXX 40 listening for tasks.
2025-11-17T09:39:11.019851192Z I1117 09:39:11.019825       1 tasks_processing.go:69] XXXXXX 46 listening for tasks.
2025-11-17T09:39:11.019950766Z I1117 09:39:11.019917       1 tasks_processing.go:71] XXXXXX 46 working on feature_gates task.
2025-11-17T09:39:11.019950766Z I1117 09:39:11.019931       1 tasks_processing.go:71] XXXXXX 23 working on openshift_sdn_logs task.
2025-11-17T09:39:11.020011515Z I1117 09:39:11.019993       1 tasks_processing.go:71] XXXXXX 4 working on kube_controller_manager_logs task.
2025-11-17T09:39:11.020021529Z I1117 09:39:11.020009       1 tasks_processing.go:71] XXXXXX 1 working on certificate_signing_requests task.
2025-11-17T09:39:11.020021529Z I1117 09:39:11.019921       1 tasks_processing.go:71] XXXXXX 19 working on machine_config_pools task.
2025-11-17T09:39:11.020062314Z I1117 09:39:11.020050       1 tasks_processing.go:71] XXXXXX 5 working on schedulers task.
2025-11-17T09:39:11.020108769Z I1117 09:39:11.020090       1 tasks_processing.go:71] XXXXXX 0 working on openshift_logging task.
2025-11-17T09:39:11.020108769Z I1117 09:39:11.020101       1 tasks_processing.go:71] XXXXXX 16 working on config_maps task.
2025-11-17T09:39:11.020249806Z I1117 09:39:11.020206       1 tasks_processing.go:71] XXXXXX 7 working on mutating_webhook_configurations task.
2025-11-17T09:39:11.020249806Z I1117 09:39:11.020234       1 tasks_processing.go:71] XXXXXX 11 working on image_pruners task.
2025-11-17T09:39:11.020362648Z I1117 09:39:11.020088       1 tasks_processing.go:71] XXXXXX 32 working on openshift_apiserver_operator_logs task.
2025-11-17T09:39:11.020572851Z I1117 09:39:11.020535       1 tasks_processing.go:71] XXXXXX 2 working on container_runtime_configs task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.020750       1 tasks_processing.go:71] XXXXXX 30 working on image_registries task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.019991       1 tasks_processing.go:71] XXXXXX 10 working on overlapping_namespace_uids task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.020788       1 tasks_processing.go:71] XXXXXX 3 working on machine_autoscalers task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.020817       1 tasks_processing.go:71] XXXXXX 35 working on sap_license_management_logs task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.020846       1 tasks_processing.go:71] XXXXXX 36 working on sap_datahubs task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.020990       1 tasks_processing.go:71] XXXXXX 6 working on metrics task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.020218       1 tasks_processing.go:71] XXXXXX 15 working on machine_healthchecks task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.021098       1 tasks_processing.go:71] XXXXXX 13 working on storage_classes task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.021144       1 tasks_processing.go:71] XXXXXX 12 working on openshift_authentication_logs task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.021224       1 tasks_processing.go:71] XXXXXX 41 working on operators_pods_and_events task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.021227       1 tasks_processing.go:71] XXXXXX 43 working on olm_operators task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.021428       1 tasks_processing.go:71] XXXXXX 24 working on oauths task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.021721       1 tasks_processing.go:71] XXXXXX 26 working on operators task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.021752       1 tasks_processing.go:71] XXXXXX 31 working on ceph_cluster task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.021795       1 tasks_processing.go:71] XXXXXX 18 working on authentication task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.021870       1 tasks_processing.go:71] XXXXXX 14 working on aggregated_monitoring_cr_names task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.022097       1 tasks_processing.go:71] XXXXXX 28 working on install_plans task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.022174       1 tasks_processing.go:71] XXXXXX 20 working on jaegers task.
2025-11-17T09:39:11.022341268Z I1117 09:39:11.022209       1 tasks_processing.go:71] XXXXXX 29 working on scheduler_logs task.
2025-11-17T09:39:11.022435125Z I1117 09:39:11.022416       1 tasks_processing.go:71] XXXXXX 8 working on proxies task.
2025-11-17T09:39:11.022643822Z I1117 09:39:11.022610       1 tasks_processing.go:71] XXXXXX 9 working on openshift_sdn_controller_logs task.
2025-11-17T09:39:11.022770963Z I1117 09:39:11.022750       1 tasks_processing.go:71] XXXXXX 25 working on machines task.
2025-11-17T09:39:11.023192146Z I1117 09:39:11.023117       1 tasks_processing.go:71] XXXXXX 17 working on tsdb_status task.
2025-11-17T09:39:11.023209350Z I1117 09:39:11.023200       1 tasks_processing.go:71] XXXXXX 21 working on host_subnets task.
2025-11-17T09:39:11.023409300Z I1117 09:39:11.023119       1 tasks_processing.go:71] XXXXXX 27 working on nodes task.
2025-11-17T09:39:11.023409300Z I1117 09:39:11.023375       1 tasks_processing.go:71] XXXXXX 45 working on sap_config task.
2025-11-17T09:39:11.023472436Z I1117 09:39:11.023457       1 tasks_processing.go:71] XXXXXX 22 working on crds task.
2025-11-17T09:39:11.023763440Z I1117 09:39:11.023734       1 tasks_processing.go:71] XXXXXX 33 working on networks task.
2025-11-17T09:39:11.023778451Z I1117 09:39:11.023763       1 tasks_processing.go:71] XXXXXX 37 working on silenced_alerts task.
2025-11-17T09:39:11.023807022Z I1117 09:39:11.023788       1 tasks_processing.go:71] XXXXXX 44 working on image task.
2025-11-17T09:39:11.023885429Z I1117 09:39:11.023865       1 tasks_processing.go:71] XXXXXX 42 working on dvo_metrics task.
2025-11-17T09:39:11.024045564Z I1117 09:39:11.023905       1 tasks_processing.go:71] XXXXXX 38 working on ingress_certificates task.
2025-11-17T09:39:11.024092887Z I1117 09:39:11.024070       1 tasks_processing.go:71] XXXXXX 34 working on cost_management_metrics_configs task.
2025-11-17T09:39:11.024332874Z I1117 09:39:11.024276       1 tasks_processing.go:71] XXXXXX 39 working on pdbs task.
2025-11-17T09:39:11.024371118Z I1117 09:39:11.024305       1 tasks_processing.go:71] XXXXXX 40 working on service_accounts task.
2025-11-17T09:39:11.048507398Z I1117 09:39:11.048449       1 tasks_processing.go:71] XXXXXX 0 working on machine_sets task.
2025-11-17T09:39:11.048716582Z E1117 09:39:11.048675       1 gather_most_recent_metrics.go:98] Unable to retrieve most recent metrics: Get "https://prometheus-k8s.openshift-monitoring.svc:9091/federate?match%5B%5D=cluster_installer&match%5B%5D=namespace%3Acontainer_cpu_usage_seconds_total%3Asum_rate&match%5B%5D=namespace%3Acontainer_memory_usage_bytes%3Asum&match%5B%5D=vsphere_node_hw_version_total&match%5B%5D=virt_platform&match%5B%5D=console_helm_installs_total&match%5B%5D=console_helm_upgrades_total&match%5B%5D=console_helm_uninstalls_total&match%5B%5D=openshift_apps_deploymentconfigs_strategy_total&match%5B%5D=etcd_server_slow_apply_total&match%5B%5D=etcd_server_slow_read_indexes_total&match%5B%5D=haproxy_exporter_server_threshold": dial tcp 172.30.76.10:9091: connect: connection refused
2025-11-17T09:39:11.048729738Z I1117 09:39:11.048718       1 gather.go:183] gatherer "clusterconfig" function "openshift_logging" took 28.337479ms to process 0 records
2025-11-17T09:39:11.048741855Z I1117 09:39:11.048735       1 gather.go:183] gatherer "clusterconfig" function "host_subnets" took 25.482352ms to process 0 records
2025-11-17T09:39:11.048768162Z E1117 09:39:11.048746       1 gather_prometheus_tsdb_status.go:49] Unable to tsdb status: Get "https://prometheus-k8s.openshift-monitoring.svc:9091/api/v1/status/tsdb": dial tcp 172.30.76.10:9091: connect: connection refused
2025-11-17T09:39:11.048790396Z E1117 09:39:11.048744       1 gather.go:146] gatherer "clusterconfig" function "metrics" failed with the error: Get "https://prometheus-k8s.openshift-monitoring.svc:9091/federate?match%5B%5D=cluster_installer&match%5B%5D=namespace%3Acontainer_cpu_usage_seconds_total%3Asum_rate&match%5B%5D=namespace%3Acontainer_memory_usage_bytes%3Asum&match%5B%5D=vsphere_node_hw_version_total&match%5B%5D=virt_platform&match%5B%5D=console_helm_installs_total&match%5B%5D=console_helm_upgrades_total&match%5B%5D=console_helm_uninstalls_total&match%5B%5D=openshift_apps_deploymentconfigs_strategy_total&match%5B%5D=etcd_server_slow_apply_total&match%5B%5D=etcd_server_slow_read_indexes_total&match%5B%5D=haproxy_exporter_server_threshold": dial tcp 172.30.76.10:9091: connect: connection refused
2025-11-17T09:39:11.048790396Z I1117 09:39:11.048777       1 gather.go:183] gatherer "clusterconfig" function "metrics" took 27.707598ms to process 0 records
2025-11-17T09:39:11.048813160Z I1117 09:39:11.048798       1 gather.go:183] gatherer "clusterconfig" function "jaegers" took 26.54468ms to process 0 records
2025-11-17T09:39:11.048823779Z E1117 09:39:11.048809       1 gather.go:146] gatherer "clusterconfig" function "tsdb_status" failed with the error: Get "https://prometheus-k8s.openshift-monitoring.svc:9091/api/v1/status/tsdb": dial tcp 172.30.76.10:9091: connect: connection refused
2025-11-17T09:39:11.048831996Z I1117 09:39:11.048820       1 gather.go:183] gatherer "clusterconfig" function "tsdb_status" took 25.630404ms to process 0 records
2025-11-17T09:39:11.048842711Z I1117 09:39:11.048836       1 tasks_processing.go:71] XXXXXX 17 working on validating_webhook_configurations task.
2025-11-17T09:39:11.048874331Z I1117 09:39:11.048854       1 gather_sap_vsystem_iptables_logs.go:60] SAP resources weren't found
2025-11-17T09:39:11.048896370Z I1117 09:39:11.048880       1 gather.go:183] gatherer "clusterconfig" function "sap_license_management_logs" took 28.036331ms to process 0 records
2025-11-17T09:39:11.049011936Z I1117 09:39:11.048989       1 gather.go:183] gatherer "clusterconfig" function "sap_datahubs" took 28.126505ms to process 0 records
2025-11-17T09:39:11.050514018Z I1117 09:39:11.050474       1 tasks_processing.go:71] XXXXXX 20 working on openshift_machine_api_events task.
2025-11-17T09:39:11.050542074Z I1117 09:39:11.050513       1 tasks_processing.go:71] XXXXXX 45 working on container_images task.
2025-11-17T09:39:11.050967161Z I1117 09:39:11.050916       1 tasks_processing.go:71] XXXXXX 36 working on active_alerts task.
2025-11-17T09:39:11.050967161Z I1117 09:39:11.050933       1 tasks_processing.go:71] XXXXXX 6 working on support_secret task.
2025-11-17T09:39:11.050967161Z I1117 09:39:11.050947       1 tasks_processing.go:71] XXXXXX 21 working on version task.
2025-11-17T09:39:11.051169341Z I1117 09:39:11.051144       1 gather.go:183] gatherer "clusterconfig" function "sap_config" took 27.118764ms to process 0 records
2025-11-17T09:39:11.051169341Z I1117 09:39:11.051165       1 gather.go:183] gatherer "clusterconfig" function "support_secret" took 1.796s to process 0 records
2025-11-17T09:39:11.051181446Z I1117 09:39:11.051175       1 gather.go:183] gatherer "clusterconfig" function "cost_management_metrics_configs" took 26.928374ms to process 0 records
2025-11-17T09:39:11.051204168Z I1117 09:39:11.051193       1 tasks_processing.go:71] XXXXXX 6 working on node_logs task.
2025-11-17T09:39:11.051259743Z I1117 09:39:11.051242       1 tasks_processing.go:71] XXXXXX 35 working on infrastructures task.
2025-11-17T09:39:11.051337337Z I1117 09:39:11.051320       1 tasks_processing.go:71] XXXXXX 31 working on pod_network_connectivity_checks task.
2025-11-17T09:39:11.051693662Z I1117 09:39:11.051495       1 tasks_processing.go:71] XXXXXX 34 working on cluster_apiserver task.
2025-11-17T09:39:11.051693662Z I1117 09:39:11.051519       1 gather.go:183] gatherer "clusterconfig" function "ceph_cluster" took 29.553896ms to process 0 records
2025-11-17T09:39:11.053161635Z I1117 09:39:11.052093       1 tasks_processing.go:71] XXXXXX 7 working on monitoring_persistent_volumes task.
2025-11-17T09:39:11.053161635Z E1117 09:39:11.052457       1 gather_silenced_alerts.go:52] Unable to retrieve silenced alerts: Get "https://alertmanager-main.openshift-monitoring.svc:9094/api/v2/alerts?active=false&inhibited=false&silenced=true": dial tcp 172.30.145.226:9094: connect: connection refused
2025-11-17T09:39:11.054386282Z I1117 09:39:11.054052       1 recorder.go:70] Recording config/mutatingwebhookconfigurations/machine-api with fingerprint=13c119315b9220835dbce9c8d5e85b488d00db828f7ae14d4de4db274df2fa4e
2025-11-17T09:39:11.054535676Z I1117 09:39:11.054517       1 recorder.go:70] Recording config/mutatingwebhookconfigurations/machine-api-metal3-remediation with fingerprint=7cfb006659a0f26320ad9e12b6b3221d50ca890d7ac35e5e6abc5c8123dc8d4c
2025-11-17T09:39:11.054583546Z I1117 09:39:11.054567       1 gather.go:183] gatherer "clusterconfig" function "mutating_webhook_configurations" took 31.848102ms to process 2 records
2025-11-17T09:39:11.054721599Z E1117 09:39:11.054508       1 gather_active_alerts.go:64] Unable to retrieve most recent alerts: Get "https://alertmanager-main.openshift-monitoring.svc:9094/api/v2/alerts?active=true": dial tcp 172.30.145.226:9094: connect: connection refused
2025-11-17T09:39:11.054721599Z I1117 09:39:11.054686       1 tasks_processing.go:71] XXXXXX 37 working on sap_pods task.
2025-11-17T09:39:11.054778954Z E1117 09:39:11.054607       1 gather.go:146] gatherer "clusterconfig" function "silenced_alerts" failed with the error: Get "https://alertmanager-main.openshift-monitoring.svc:9094/api/v2/alerts?active=false&inhibited=false&silenced=true": dial tcp 172.30.145.226:9094: connect: connection refused
2025-11-17T09:39:11.054823619Z I1117 09:39:11.054808       1 gather.go:183] gatherer "clusterconfig" function "silenced_alerts" took 28.721995ms to process 0 records
2025-11-17T09:39:11.054863532Z I1117 09:39:11.054852       1 gather.go:183] gatherer "clusterconfig" function "storage_classes" took 31.763209ms to process 0 records
2025-11-17T09:39:11.054905229Z E1117 09:39:11.054883       1 gather.go:146] gatherer "clusterconfig" function "active_alerts" failed with the error: Get "https://alertmanager-main.openshift-monitoring.svc:9094/api/v2/alerts?active=true": dial tcp 172.30.145.226:9094: connect: connection refused
2025-11-17T09:39:11.054936712Z I1117 09:39:11.054926       1 gather.go:183] gatherer "clusterconfig" function "active_alerts" took 3.707965ms to process 0 records
2025-11-17T09:39:11.054985503Z I1117 09:39:11.054963       1 tasks_processing.go:71] XXXXXX 36 working on netnamespaces task.
2025-11-17T09:39:11.055307866Z I1117 09:39:11.055264       1 tasks_processing.go:71] XXXXXX 13 working on ingress task.
2025-11-17T09:39:11.058926849Z I1117 09:39:11.058867       1 tasks_processing.go:71] XXXXXX 30 working on storage_cluster task.
2025-11-17T09:39:11.060640532Z I1117 09:39:11.060556       1 gather_logs.go:132] no pods in openshift-sdn namespace were found
2025-11-17T09:39:11.060640532Z W1117 09:39:11.060604       1 gather_dvo_metrics.go:115] No DVO metrics gathered
2025-11-17T09:39:11.062456333Z I1117 09:39:11.062422       1 gather_machine_sets.go:86] error during anonymizing machineset: unable to find field '[spec template spec providerSpec value projectID]'
2025-11-17T09:39:11.063011249Z I1117 09:39:11.062976       1 gather_machine_sets.go:86] error during anonymizing machineset: unable to find field '[spec template spec providerSpec value region]'
2025-11-17T09:39:11.063640833Z I1117 09:39:11.063615       1 gather_machine_sets.go:86] error during anonymizing machineset: unable to find field '[spec template spec providerSpec value placement availabilityZone]'
2025-11-17T09:39:11.063692025Z I1117 09:39:11.063680       1 gather_machine_sets.go:86] error during anonymizing machineset: unable to find field '[spec template spec providerSpec value placement region]'
2025-11-17T09:39:11.063891724Z I1117 09:39:11.063863       1 gather_machine_sets.go:98] error during anonymizing machineset: unable to find service accounts false <nil>
2025-11-17T09:39:11.063926817Z I1117 09:39:11.063765       1 gather_logs.go:132] no pods in openshift-sdn namespace were found
2025-11-17T09:39:11.063952656Z I1117 09:39:11.062772       1 recorder.go:70] Recording config/clusteroperator/imageregistry.operator.openshift.io/config/cluster with fingerprint=ef67efb5dc88e6563c04c0f5bea4d5e356b40685df7e65deb7e6d43a162d538c
2025-11-17T09:39:11.063999101Z I1117 09:39:11.063984       1 gather.go:183] gatherer "clusterconfig" function "image_registries" took 38.086242ms to process 1 records
2025-11-17T09:39:11.064047479Z I1117 09:39:11.064022       1 gather.go:183] gatherer "clusterconfig" function "machine_autoscalers" took 39.69126ms to process 0 records
2025-11-17T09:39:11.064117955Z I1117 09:39:11.064098       1 gather.go:183] gatherer "clusterconfig" function "container_runtime_configs" took 39.989568ms to process 0 records
2025-11-17T09:39:11.064151501Z I1117 09:39:11.064140       1 gather.go:183] gatherer "clusterconfig" function "openshift_sdn_controller_logs" took 37.930682ms to process 0 records
2025-11-17T09:39:11.064185303Z I1117 09:39:11.064173       1 gather.go:183] gatherer "clusterconfig" function "dvo_metrics" took 36.741335ms to process 0 records
2025-11-17T09:39:11.064382968Z I1117 09:39:11.064360       1 recorder.go:70] Recording config/proxy with fingerprint=7849eb2a0fa8665502aa580bbc3dd64ff3e6d3e934fa32ef305c7d3322c992eb
2025-11-17T09:39:11.064436028Z I1117 09:39:11.064423       1 gather.go:183] gatherer "clusterconfig" function "proxies" took 38.270724ms to process 1 records
2025-11-17T09:39:11.064507003Z I1117 09:39:11.064483       1 tasks_processing.go:74] XXXXXX 9 stopped.
2025-11-17T09:39:11.064507003Z I1117 09:39:11.064488       1 tasks_processing.go:74] XXXXXX 8 stopped.
2025-11-17T09:39:11.064507003Z I1117 09:39:11.064498       1 tasks_processing.go:74] XXXXXX 42 stopped.
2025-11-17T09:39:11.064519835Z I1117 09:39:11.064507       1 tasks_processing.go:74] XXXXXX 3 stopped.
2025-11-17T09:39:11.064519835Z I1117 09:39:11.064510       1 tasks_processing.go:74] XXXXXX 2 stopped.
2025-11-17T09:39:11.064519835Z I1117 09:39:11.064387       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement availabilityZone]'
2025-11-17T09:39:11.064545747Z I1117 09:39:11.064530       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement region]'
2025-11-17T09:39:11.064554207Z I1117 09:39:11.064545       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[metadata labels machine.openshift.io/region]'
2025-11-17T09:39:11.064598710Z I1117 09:39:11.064584       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement availabilityZone]'
2025-11-17T09:39:11.064598710Z I1117 09:39:11.064593       1 tasks_processing.go:74] XXXXXX 46 stopped.
2025-11-17T09:39:11.064608800Z I1117 09:39:11.064596       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement region]'
2025-11-17T09:39:11.064608800Z I1117 09:39:11.064603       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[metadata labels machine.openshift.io/region]'
2025-11-17T09:39:11.064623827Z I1117 09:39:11.064617       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement availabilityZone]'
2025-11-17T09:39:11.064631487Z I1117 09:39:11.064624       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement region]'
2025-11-17T09:39:11.064702108Z I1117 09:39:11.064686       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[metadata labels machine.openshift.io/region]'
2025-11-17T09:39:11.064712771Z I1117 09:39:11.064705       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement availabilityZone]'
2025-11-17T09:39:11.064720847Z I1117 09:39:11.064712       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement region]'
2025-11-17T09:39:11.064727936Z I1117 09:39:11.064719       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[metadata labels machine.openshift.io/region]'
2025-11-17T09:39:11.064737674Z I1117 09:39:11.064731       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement availabilityZone]'
2025-11-17T09:39:11.064745790Z I1117 09:39:11.064739       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[spec providerSpec value placement region]'
2025-11-17T09:39:11.064753303Z I1117 09:39:11.064745       1 gather_machines.go:83] error during anonymizing machine: unable to find field '[metadata labels machine.openshift.io/region]'
2025-11-17T09:39:11.064850474Z I1117 09:39:11.064836       1 recorder.go:70] Recording config/featuregate with fingerprint=13657b48b6381c02da39b67d455b23a9cb51b34ba290c54229cfd37b8720d72c
2025-11-17T09:39:11.064889361Z I1117 09:39:11.064875       1 gather.go:183] gatherer "clusterconfig" function "feature_gates" took 41.823467ms to process 1 records
2025-11-17T09:39:11.065008042Z I1117 09:39:11.064982       1 tasks_processing.go:74] XXXXXX 27 stopped.
2025-11-17T09:39:11.065455673Z I1117 09:39:11.065426       1 recorder.go:70] Recording config/node/XXXXXX0 with fingerprint=ea5969c34e940e31cc9349542eb35c32e81b53d86887d2cf83ce4cbe1017882e
2025-11-17T09:39:11.065609480Z I1117 09:39:11.065591       1 recorder.go:70] Recording config/node/XXXXXX1 with fingerprint=3844b853fae4c05ba81b8447709f2606533f17979404f1a91e0a9f200a4afe27
2025-11-17T09:39:11.065734976Z I1117 09:39:11.065722       1 recorder.go:70] Recording config/node/XXXXXX2 with fingerprint=b26f80fbb220f00621eca7d7e98fe1127b04d4fdd3e7683a334de1c3fac4d6b0
2025-11-17T09:39:11.065869367Z I1117 09:39:11.065852       1 recorder.go:70] Recording config/node/XXXXXX0 with fingerprint=26b0d337b4b5c9dc83191aaf85f134bc9cde3ac0c05670b378b6708bf3e8e740
2025-11-17T09:39:11.066014227Z I1117 09:39:11.066000       1 recorder.go:70] Recording config/node/XXXXXX1 with fingerprint=cd3e5bd60b69f769dd29d3bead766de95fc77038589aa77bbb80bf5da89c9a00
2025-11-17T09:39:11.066060734Z I1117 09:39:11.066048       1 gather.go:183] gatherer "clusterconfig" function "nodes" took 39.256972ms to process 5 records
2025-11-17T09:39:11.066091674Z I1117 09:39:11.066081       1 gather.go:183] gatherer "clusterconfig" function "sap_pods" took 7.975178ms to process 0 records
2025-11-17T09:39:11.066192542Z I1117 09:39:11.066172       1 tasks_processing.go:74] XXXXXX 37 stopped.
2025-11-17T09:39:11.066229049Z I1117 09:39:11.066218       1 recorder.go:70] Recording config/schedulers/cluster with fingerprint=67849dc5b9fe40d3f8f2c98d0c3219200a62e4de26179329fec1a2df1e65112a
2025-11-17T09:39:11.066261508Z I1117 09:39:11.066249       1 gather.go:183] gatherer "clusterconfig" function "schedulers" took 42.883138ms to process 1 records
2025-11-17T09:39:11.066358643Z I1117 09:39:11.066266       1 tasks_processing.go:74] XXXXXX 5 stopped.
2025-11-17T09:39:11.066465627Z I1117 09:39:11.066448       1 tasks_processing.go:74] XXXXXX 11 stopped.
2025-11-17T09:39:11.066489491Z I1117 09:39:11.066452       1 recorder.go:70] Recording config/clusteroperator/imageregistry.operator.openshift.io/imagepruner/cluster with fingerprint=754cf8910513bd105abb7ab3717306d884b552ae37c549282d1d52358fde8cb3
2025-11-17T09:39:11.066521410Z I1117 09:39:11.066511       1 gather.go:183] gatherer "clusterconfig" function "image_pruners" took 42.75993ms to process 1 records
2025-11-17T09:39:11.067172999Z I1117 09:39:11.067141       1 tasks_processing.go:74] XXXXXX 33 stopped.
2025-11-17T09:39:11.067940211Z I1117 09:39:11.067896       1 recorder.go:70] Recording config/network with fingerprint=e0f896755c7e48086b155bccba84c64d783caac4af18db562a5fbcbd5f22a79d
2025-11-17T09:39:11.067940211Z I1117 09:39:11.067930       1 gather.go:183] gatherer "clusterconfig" function "networks" took 39.368627ms to process 1 records
2025-11-17T09:39:11.068145734Z I1117 09:39:11.068122       1 tasks_processing.go:74] XXXXXX 18 stopped.
2025-11-17T09:39:11.068411905Z I1117 09:39:11.068389       1 recorder.go:70] Recording config/authentication with fingerprint=3fed0313e19c0f204d21b6d81319d6cfe5b50d070fd78b80f7037d964826a064
2025-11-17T09:39:11.068438525Z I1117 09:39:11.068425       1 gather.go:183] gatherer "clusterconfig" function "authentication" took 41.517649ms to process 1 records
2025-11-17T09:39:11.068446717Z I1117 09:39:11.068437       1 gather.go:183] gatherer "clusterconfig" function "olm_operators" took 42.345971ms to process 0 records
2025-11-17T09:39:11.068454190Z E1117 09:39:11.068445       1 gather.go:146] gatherer "clusterconfig" function "pod_network_connectivity_checks" failed with the error: the server could not find the requested resource (get podnetworkconnectivitychecks.controlplane.operator.openshift.io)
2025-11-17T09:39:11.068461146Z I1117 09:39:11.068456       1 gather.go:183] gatherer "clusterconfig" function "pod_network_connectivity_checks" took 12.259799ms to process 0 records
2025-11-17T09:39:11.068599546Z I1117 09:39:11.068582       1 recorder.go:70] Recording config/image with fingerprint=3cdb6ab2645e8d64b3476eb7a9dc0b2ba26241b2d70f37297468caee6b7dd6e0
2025-11-17T09:39:11.068609758Z I1117 09:39:11.068595       1 gather.go:183] gatherer "clusterconfig" function "image" took 39.953517ms to process 1 records
2025-11-17T09:39:11.068609758Z I1117 09:39:11.068607       1 gather.go:183] gatherer "clusterconfig" function "netnamespaces" took 8.801893ms to process 0 records
2025-11-17T09:39:11.068650946Z I1117 09:39:11.068623       1 tasks_processing.go:74] XXXXXX 44 stopped.
2025-11-17T09:39:11.068659168Z I1117 09:39:11.068649       1 tasks_processing.go:74] XXXXXX 31 stopped.
2025-11-17T09:39:11.068659168Z I1117 09:39:11.068657       1 tasks_processing.go:74] XXXXXX 36 stopped.
2025-11-17T09:39:11.068764226Z I1117 09:39:11.068742       1 tasks_processing.go:74] XXXXXX 0 stopped.
2025-11-17T09:39:11.068833760Z I1117 09:39:11.068820       1 recorder.go:70] Recording machinesets/openshift-machine-api/hlxcl51-5vf88-XXXXXX-0 with fingerprint=d7c708875de52ffcd05403676bbc6d7926a0355a43f4bd2cb9090e136f451424
2025-11-17T09:39:11.068842221Z I1117 09:39:11.068831       1 gather.go:183] gatherer "clusterconfig" function "machine_sets" took 15.452626ms to process 1 records
2025-11-17T09:39:11.068882392Z I1117 09:39:11.068855       1 tasks_processing.go:74] XXXXXX 43 stopped.
2025-11-17T09:39:11.068909545Z I1117 09:39:11.068897       1 gather.go:183] gatherer "clusterconfig" function "openshift_sdn_logs" took 44.006582ms to process 0 records
2025-11-17T09:39:11.069007352Z I1117 09:39:11.068984       1 tasks_processing.go:74] XXXXXX 19 stopped.
2025-11-17T09:39:11.069007352Z I1117 09:39:11.068982       1 tasks_processing.go:74] XXXXXX 23 stopped.
2025-11-17T09:39:11.069180104Z I1117 09:39:11.069151       1 recorder.go:70] Recording config/machineconfigpools/XXXXXX with fingerprint=4cecb8fd4c4806f208f2133bd72010708c56e8d2f1d1102b8ed15f561e31266e
2025-11-17T09:39:11.071299445Z I1117 09:39:11.071231       1 recorder.go:70] Recording config/machineconfigpools/XXXXXX with fingerprint=57e26a500631e1cf928347c1c11b243d295a35f7409c01b789f1cf3fdf83f917
2025-11-17T09:39:11.071299445Z I1117 09:39:11.071264       1 gather.go:183] gatherer "clusterconfig" function "machine_config_pools" took 44.437759ms to process 2 records
2025-11-17T09:39:11.071442344Z I1117 09:39:11.071415       1 tasks_processing.go:74] XXXXXX 24 stopped.
2025-11-17T09:39:11.071630892Z I1117 09:39:11.071598       1 recorder.go:70] Recording config/oauth with fingerprint=5e84b79f16b07b55ecb04465d4df311a131ab55d70d053c76f2c3b19f1cd6e41
2025-11-17T09:39:11.071630892Z I1117 09:39:11.071617       1 gather.go:183] gatherer "clusterconfig" function "oauths" took 43.114857ms to process 1 records
2025-11-17T09:39:11.071771159Z I1117 09:39:11.071748       1 tasks_processing.go:74] XXXXXX 25 stopped.
2025-11-17T09:39:11.071771159Z I1117 09:39:11.071758       1 recorder.go:70] Recording config/machines/openshift-machine-api/hlxcl51-5vf88-XXXXXX-0 with fingerprint=1c61b222d827db58fc1a8b8a9aa41d386b186b0e92ea57999f9c78b4ffac19ff
2025-11-17T09:39:11.071874939Z I1117 09:39:11.071849       1 recorder.go:70] Recording config/machines/openshift-machine-api/hlxcl51-5vf88-XXXXXX-1 with fingerprint=cf745956c3baf1fe19bb681b05d9ad57a6e455e5431706b3dc3b44576547318d
2025-11-17T09:39:11.071953685Z I1117 09:39:11.071937       1 recorder.go:70] Recording config/machines/openshift-machine-api/hlxcl51-5vf88-XXXXXX-2 with fingerprint=84b5927a55a8456262947a180df5c43b449b3f1bd5f5b87b561040ac9e3c4544
2025-11-17T09:39:11.071985699Z I1117 09:39:11.071966       1 gather_cluster_version.go:118] Found 1 unhealthy pods in openshift-cluster-version
2025-11-17T09:39:11.072064540Z I1117 09:39:11.072045       1 recorder.go:70] Recording config/machines/openshift-machine-api/hlxcl51-5vf88-XXXXXX-0-csjwc with fingerprint=5409ce9cc23322a1857f297d3fa641c3f5aedb98cb52e73521e7cfaad1f21165
2025-11-17T09:39:11.072189257Z I1117 09:39:11.072146       1 recorder.go:70] Recording config/machines/openshift-machine-api/hlxcl51-5vf88-XXXXXX-0-qk9w5 with fingerprint=e613e761c245e5fa4a30dde44a7eab34b8da10be18da6526c591f28e11753ca5
2025-11-17T09:39:11.072189257Z I1117 09:39:11.072159       1 gather.go:183] gatherer "clusterconfig" function "machines" took 41.989173ms to process 5 records
2025-11-17T09:39:11.072434850Z I1117 09:39:11.072411       1 recorder.go:70] Recording config/pdbs/openshift-apiserver/openshift-apiserver-pdb with fingerprint=fa7a8946efa0875ac2fdfbf9c5d1829957ec1c69bc321afff0ebac0cd9d9a75e
2025-11-17T09:39:11.072459357Z I1117 09:39:11.072436       1 recorder.go:70] Recording config/pdbs/openshift-cluster-storage-operator/csi-snapshot-controller-pdb with fingerprint=85ed704720ba4a94258deec23ba68b95d4d68b4a61379bbe942aca8d4e76f0e7
2025-11-17T09:39:11.072459357Z I1117 09:39:11.072449       1 recorder.go:70] Recording config/pdbs/openshift-cluster-storage-operator/csi-snapshot-webhook-pdb with fingerprint=b0f861f4090142d8467924e0eb6d07dc5fb87ce947d8183f2439acbb97c81f96
2025-11-17T09:39:11.072476721Z I1117 09:39:11.072470       1 recorder.go:70] Recording config/pdbs/openshift-console/console with fingerprint=b4f70db1577cb02b924141f2b5b78d93c158adc6de78d03fdad815f3560b19a8
2025-11-17T09:39:11.072506607Z I1117 09:39:11.072485       1 recorder.go:70] Recording config/pdbs/openshift-console/downloads with fingerprint=3d01ebf23d2e76143240f28278f020e466daf560dae8c4041a6e7609372a2beb
2025-11-17T09:39:11.072514810Z I1117 09:39:11.072504       1 recorder.go:70] Recording config/pdbs/openshift-etcd/etcd-guard-pdb with fingerprint=be846dfd3c044cbdde7b7400088dbd1a71cd5189bad0f0b971d9ac0fe7a8cd6f
2025-11-17T09:39:11.072539029Z I1117 09:39:11.072522       1 recorder.go:70] Recording config/pdbs/openshift-ingress/router-default with fingerprint=7a7323f6b0a22fb4438f2f0fc4a6af9c84f03862340bdeb673c3593bc47e6b06
2025-11-17T09:39:11.072548260Z I1117 09:39:11.072538       1 recorder.go:70] Recording config/pdbs/openshift-kube-apiserver/kube-apiserver-guard-pdb with fingerprint=a5a1820bdf7b902b6a289edadd3d55243e2f87d98f6194698b90b542bd52dac6
2025-11-17T09:39:11.072558803Z I1117 09:39:11.072554       1 recorder.go:70] Recording config/pdbs/openshift-kube-controller-manager/kube-controller-manager-guard-pdb with fingerprint=8756022aeb4ec247a0256933bceab37d1d4cf3603b8f88c353ffe9a00446e8fd
2025-11-17T09:39:11.072577625Z I1117 09:39:11.072569       1 recorder.go:70] Recording config/pdbs/openshift-kube-scheduler/openshift-kube-scheduler-guard-pdb with fingerprint=e3bd9b8e73dcdc86bee625cc88a2d033cf3ab549080a6d5f8833f8e98fcab97c
2025-11-17T09:39:11.072609557Z I1117 09:39:11.072589       1 recorder.go:70] Recording config/pdbs/openshift-monitoring/alertmanager-main with fingerprint=f5b36064d734639f029e5aaa35dbd1efe5298933e70b9d979421d39da02f01b4
2025-11-17T09:39:11.072619422Z I1117 09:39:11.072611       1 recorder.go:70] Recording config/pdbs/openshift-monitoring/monitoring-plugin with fingerprint=50ed5e5602276453313decd716a42cd8a9f4545f3bcfa8e650f9fd4d3d2b54f7
2025-11-17T09:39:11.072645615Z I1117 09:39:11.072630       1 recorder.go:70] Recording config/pdbs/openshift-monitoring/prometheus-k8s with fingerprint=bfa85d990b4135d18bde3cc47f14e56b6e435ef1b03ebe60e0000e95b36c6145
2025-11-17T09:39:11.072655919Z I1117 09:39:11.072650       1 recorder.go:70] Recording config/pdbs/openshift-monitoring/prometheus-operator-admission-webhook with fingerprint=051ed20002d38a5f10be5a38f33c1c792ae161a32d7b905593e2f381ba23b224
2025-11-17T09:39:11.072670808Z I1117 09:39:11.072663       1 recorder.go:70] Recording config/pdbs/openshift-oauth-apiserver/oauth-apiserver-pdb with fingerprint=9e7330b12485d78cc1a6fa66e92563ddc583bfdd3d8c15685c728584b09b0a0e
2025-11-17T09:39:11.072697421Z I1117 09:39:11.072679       1 recorder.go:70] Recording config/pdbs/openshift-operator-lifecycle-manager/packageserver-pdb with fingerprint=56d164dbc39a4a61e4f51cfe72a1ab26edc962a04fcd3e81d10bb19004c0a569
2025-11-17T09:39:11.072697421Z I1117 09:39:11.072691       1 gather.go:183] gatherer "clusterconfig" function "pdbs" took 40.582426ms to process 16 records
2025-11-17T09:39:11.072877214Z I1117 09:39:11.072854       1 recorder.go:70] Recording config/apiserver with fingerprint=43a81c1cb223a27ca6020fc3fb61cf4e87d220737d26cbbe138af5ddb80538a8
2025-11-17T09:39:11.072889757Z I1117 09:39:11.072872       1 gather.go:183] gatherer "clusterconfig" function "cluster_apiserver" took 13.459801ms to process 1 records
2025-11-17T09:39:11.073019590Z I1117 09:39:11.072991       1 recorder.go:70] Recording config/validatingwebhookconfigurations/alertmanagerconfigs.openshift.io with fingerprint=2f6d7fa6a78cb493021594a78e5069f4b100461d802caef224f5dd264f0e8b8b
2025-11-17T09:39:11.073055944Z I1117 09:39:11.073037       1 tasks_processing.go:74] XXXXXX 39 stopped.
2025-11-17T09:39:11.073055944Z I1117 09:39:11.073047       1 recorder.go:70] Recording config/validatingwebhookconfigurations/autoscaling.openshift.io with fingerprint=c719ad0972417c519625c5ef18788e73e4d1c783a88274b9d5a25828f73ee122
2025-11-17T09:39:11.073055944Z I1117 09:39:11.073053       1 tasks_processing.go:74] XXXXXX 34 stopped.
2025-11-17T09:39:11.073105975Z I1117 09:39:11.073088       1 recorder.go:70] Recording config/validatingwebhookconfigurations/baremetal-operator-validating-webhook-configuration with fingerprint=89f4543a2a6d1bcf9be64f64c1b31ce3dcb7864b16649eae4609aebdc55e3beb
2025-11-17T09:39:11.073140086Z I1117 09:39:11.073125       1 recorder.go:70] Recording config/validatingwebhookconfigurations/cluster-baremetal-validating-webhook-configuration with fingerprint=fe2ce48f5b0bfa5ccc97b74bdf617da1697a811f483883ff31861eab8cd8e3b8
2025-11-17T09:39:11.073140086Z I1117 09:39:11.073135       1 tasks_processing.go:74] XXXXXX 17 stopped.
2025-11-17T09:39:11.073174779Z I1117 09:39:11.073160       1 recorder.go:70] Recording config/validatingwebhookconfigurations/controlplanemachineset.machine.openshift.io with fingerprint=a2891533657f842c335d20f008863f903f94e7b7394332fe63f5323eb95cbf8c
2025-11-17T09:39:11.073224427Z I1117 09:39:11.073207       1 recorder.go:70] Recording config/validatingwebhookconfigurations/machine-api with fingerprint=309c8fce095dd1d219dddc03ac6b8f306d6a652b45ff19f02c862a43f4d6bad7
2025-11-17T09:39:11.073268254Z I1117 09:39:11.073253       1 recorder.go:70] Recording config/validatingwebhookconfigurations/machine-api-metal3-remediation with fingerprint=24d88fc0ec6e04d3b89c10fac827b433c70d060ccd7f4e3ec6c00eb3b96a5d36
2025-11-17T09:39:11.073315874Z I1117 09:39:11.073293       1 recorder.go:70] Recording config/validatingwebhookconfigurations/multus.openshift.io with fingerprint=bc8043e9efdde610cc4486cd781948ffeccd4ee31d442a1dfba4fec29c429351
2025-11-17T09:39:11.073355555Z I1117 09:39:11.073336       1 recorder.go:70] Recording config/validatingwebhookconfigurations/performance-addon-operator with fingerprint=d455c36637cc872095fdd6e7e4671c95156b9013afd5bc879f51a7fcdbd5b688
2025-11-17T09:39:11.073386566Z I1117 09:39:11.073369       1 recorder.go:70] Recording config/validatingwebhookconfigurations/prometheusrules.openshift.io with fingerprint=0923ae19c2e181417c26d1a18d1ae92d4cc5e378d20b9ccfc68541f06a325ed3
2025-11-17T09:39:11.073417541Z I1117 09:39:11.073401       1 recorder.go:70] Recording config/validatingwebhookconfigurations/snapshot.storage.k8s.io with fingerprint=eace05617f27c01487f9dccdfca3e1d0cbf0763a16bf811621d5ed1368ef9378
2025-11-17T09:39:11.073417541Z I1117 09:39:11.073411       1 gather.go:183] gatherer "clusterconfig" function "validating_webhook_configurations" took 16.265443ms to process 11 records
2025-11-17T09:39:11.073425965Z I1117 09:39:11.073420       1 gather.go:183] gatherer "clusterconfig" function "monitoring_persistent_volumes" took 13.016163ms to process 0 records
2025-11-17T09:39:11.073451618Z I1117 09:39:11.073427       1 tasks_processing.go:74] XXXXXX 7 stopped.
2025-11-17T09:39:11.073451618Z I1117 09:39:11.073436       1 recorder.go:70] Recording config/namespaces_with_overlapping_uids with fingerprint=4f53cda18c2baa0c0354bb5f9a3ecbe5ed12ab4d8e11ba873c2f11161202b945
2025-11-17T09:39:11.073451618Z I1117 09:39:11.073443       1 tasks_processing.go:74] XXXXXX 10 stopped.
2025-11-17T09:39:11.073480559Z I1117 09:39:11.073461       1 gather.go:183] gatherer "clusterconfig" function "overlapping_namespace_uids" took 44.635114ms to process 1 records
2025-11-17T09:39:11.073693520Z I1117 09:39:11.073657       1 tasks_processing.go:74] XXXXXX 35 stopped.
2025-11-17T09:39:11.074255814Z I1117 09:39:11.074225       1 recorder.go:70] Recording config/infrastructure with fingerprint=b5ee13ee022282f225e199cda470708b78bc5f0ae766cd7edb8ef1fc45dd96fd
2025-11-17T09:39:11.074255814Z I1117 09:39:11.074247       1 gather.go:183] gatherer "clusterconfig" function "infrastructures" took 14.355669ms to process 1 records
2025-11-17T09:39:11.074268475Z I1117 09:39:11.074257       1 gather.go:183] gatherer "clusterconfig" function "storage_cluster" took 7.534294ms to process 0 records
2025-11-17T09:39:11.074354975Z I1117 09:39:11.074332       1 tasks_processing.go:74] XXXXXX 30 stopped.
2025-11-17T09:39:11.074354975Z I1117 09:39:11.074350       1 recorder.go:70] Recording config/machinehealthchecks/openshift-machine-api/machine-api-termination-handler with fingerprint=8d7713ec62e74ab838c0d92fa26308c6139b43f2dc5d06e3e75e0061f5425df9
2025-11-17T09:39:11.074366572Z I1117 09:39:11.074361       1 gather.go:183] gatherer "clusterconfig" function "machine_healthchecks" took 50.148686ms to process 1 records
2025-11-17T09:39:11.074496395Z I1117 09:39:11.074475       1 recorder.go:70] Recording events/openshift-machine-api with fingerprint=cc69d6d426732f2e74eb835dab836c457c6bb4baf2022ae62c4836627df7235e
2025-11-17T09:39:11.074496395Z I1117 09:39:11.074489       1 gather.go:183] gatherer "clusterconfig" function "openshift_machine_api_events" took 22.928771ms to process 1 records
2025-11-17T09:39:11.074567772Z I1117 09:39:11.074550       1 tasks_processing.go:74] XXXXXX 15 stopped.
2025-11-17T09:39:11.074567772Z I1117 09:39:11.074562       1 tasks_processing.go:74] XXXXXX 20 stopped.
2025-11-17T09:39:11.074645594Z I1117 09:39:11.074630       1 tasks_processing.go:74] XXXXXX 22 stopped.
2025-11-17T09:39:11.075086025Z I1117 09:39:11.075062       1 recorder.go:70] Recording config/crd/volumesnapshots.snapshot.storage.k8s.io with fingerprint=e691271a27c2af7e59982cb161d22fac72e25e29328b7c9628c5788b86501ad6
2025-11-17T09:39:11.075393311Z I1117 09:39:11.075368       1 recorder.go:70] Recording config/crd/volumesnapshotcontents.snapshot.storage.k8s.io with fingerprint=b4f42e8821f1dc553a9931ff87168b0d38076f90ff2cf7f3005e7dd855fb8160
2025-11-17T09:39:11.075393311Z I1117 09:39:11.075383       1 gather.go:183] gatherer "clusterconfig" function "crds" took 50.08363ms to process 2 records
2025-11-17T09:39:11.075503198Z I1117 09:39:11.075485       1 tasks_processing.go:74] XXXXXX 13 stopped.
2025-11-17T09:39:11.075571468Z I1117 09:39:11.075552       1 recorder.go:70] Recording config/ingress with fingerprint=af7879218aab64ffe52ddb9a4053e06f3fa60e2fc9782408bb903e846c2150a9
2025-11-17T09:39:11.075579183Z I1117 09:39:11.075573       1 gather.go:183] gatherer "clusterconfig" function "ingress" took 19.207899ms to process 1 records
2025-11-17T09:39:11.075602804Z I1117 09:39:11.075588       1 gather.go:183] gatherer "clusterconfig" function "aggregated_monitoring_cr_names" took 52.74343ms to process 0 records
2025-11-17T09:39:11.075608929Z I1117 09:39:11.075600       1 tasks_processing.go:74] XXXXXX 14 stopped.
2025-11-17T09:39:11.076113450Z I1117 09:39:11.076087       1 tasks_processing.go:74] XXXXXX 1 stopped.
2025-11-17T09:39:11.076131858Z I1117 09:39:11.076113       1 gather.go:183] gatherer "clusterconfig" function "certificate_signing_requests" took 56.058321ms to process 0 records
2025-11-17T09:39:11.080002102Z I1117 09:39:11.079950       1 tasks_processing.go:74] XXXXXX 38 stopped.
2025-11-17T09:39:11.080191763Z I1117 09:39:11.080157       1 recorder.go:70] Recording aggregated/ingress_controllers_certs with fingerprint=7fb12ae25b301351b095adfad7caafdc6bae53eef64d1a2c55f072d3f2d9db87
2025-11-17T09:39:11.080202004Z I1117 09:39:11.080189       1 gather.go:183] gatherer "clusterconfig" function "ingress_certificates" took 56.014432ms to process 1 records
2025-11-17T09:39:11.080337293Z I1117 09:39:11.080307       1 tasks_processing.go:74] XXXXXX 21 stopped.
2025-11-17T09:39:11.080546277Z I1117 09:39:11.080518       1 recorder.go:70] Recording config/version with fingerprint=9e4073be81db6a9f850b7282f8e580326aeed1846b5aa7e89f1dd2427bc8efca
2025-11-17T09:39:11.080546277Z I1117 09:39:11.080532       1 recorder.go:70] Recording config/id with fingerprint=d70950b940fb95ce7b3ae505f0eab56ee97bc642d63b84c3d8351ce7cdfadc5a
2025-11-17T09:39:11.082480182Z I1117 09:39:11.082445       1 recorder.go:70] Recording config/pod/openshift-cluster-version/cluster-version-operator-6bccd5fdcc-kpbx4 with fingerprint=356616086c8c0fe724f1c4bd7e5e45b38c2f7b5c2769b9955b4e095e9d998cd4
2025-11-17T09:39:11.082542804Z I1117 09:39:11.082528       1 recorder.go:70] Recording events/openshift-cluster-version with fingerprint=c245f41b752577b6be811845ab189b1a9621ccf90a28d66d5d0b6557e5c3ac8d
2025-11-17T09:39:11.082549752Z I1117 09:39:11.082543       1 gather.go:183] gatherer "clusterconfig" function "version" took 29.238842ms to process 4 records
2025-11-17T09:39:11.082583153Z E1117 09:39:11.082564       1 gather.go:146] gatherer "clusterconfig" function "config_maps" failed with the error: configmaps "cluster-monitoring-config" not found
2025-11-17T09:39:11.082583153Z E1117 09:39:11.082579       1 gather.go:146] gatherer "clusterconfig" function "config_maps" failed with the error: configmaps "gateway-mode-config" not found
2025-11-17T09:39:11.082608145Z I1117 09:39:11.082587       1 tasks_processing.go:74] XXXXXX 16 stopped.
2025-11-17T09:39:11.082618667Z I1117 09:39:11.082613       1 recorder.go:70] Recording config/configmaps/openshift-config/admin-kubeconfig-client-ca/ca-bundle.crt with fingerprint=34cdf5d36d45a9e3ee7c2be31546e3d937ae4ebf144ac07b0210c06a1610a94d
2025-11-17T09:39:11.082644555Z I1117 09:39:11.082631       1 recorder.go:70] Recording config/configmaps/openshift-config/etcd-ca-bundle/ca-bundle.crt with fingerprint=34cdf5d36d45a9e3ee7c2be31546e3d937ae4ebf144ac07b0210c06a1610a94d
2025-11-17T09:39:11.082682055Z W1117 09:39:11.082655       1 gather.go:161] issue recording gatherer "clusterconfig" function "config_maps" result "config/configmaps/openshift-config/etcd-ca-bundle/ca-bundle.crt" because of the warning: warning: the record with the same fingerprint "34cdf5d36d45a9e3ee7c2be31546e3d937ae4ebf144ac07b0210c06a1610a94d" was already recorded at path "config/configmaps/openshift-config/admin-kubeconfig-client-ca/ca-bundle.crt", recording another one with a different path "config/configmaps/openshift-config/etcd-ca-bundle/ca-bundle.crt"
2025-11-17T09:39:11.082705542Z I1117 09:39:11.082691       1 recorder.go:70] Recording config/configmaps/openshift-config/etcd-metric-serving-ca/ca-bundle.crt with fingerprint=34cdf5d36d45a9e3ee7c2be31546e3d937ae4ebf144ac07b0210c06a1610a94d
2025-11-17T09:39:11.082726920Z W1117 09:39:11.082711       1 gather.go:161] issue recording gatherer "clusterconfig" function "config_maps" result "config/configmaps/openshift-config/etcd-metric-serving-ca/ca-bundle.crt" because of the warning: warning: the record with the same fingerprint "34cdf5d36d45a9e3ee7c2be31546e3d937ae4ebf144ac07b0210c06a1610a94d" was already recorded at path "config/configmaps/openshift-config/etcd-ca-bundle/ca-bundle.crt", recording another one with a different path "config/configmaps/openshift-config/etcd-metric-serving-ca/ca-bundle.crt"
2025-11-17T09:39:11.082748058Z I1117 09:39:11.082732       1 recorder.go:70] Recording config/configmaps/openshift-config/etcd-serving-ca/ca-bundle.crt with fingerprint=34cdf5d36d45a9e3ee7c2be31546e3d937ae4ebf144ac07b0210c06a1610a94d
2025-11-17T09:39:11.082767152Z W1117 09:39:11.082751       1 gather.go:161] issue recording gatherer "clusterconfig" function "config_maps" result "config/configmaps/openshift-config/etcd-serving-ca/ca-bundle.crt" because of the warning: warning: the record with the same fingerprint "34cdf5d36d45a9e3ee7c2be31546e3d937ae4ebf144ac07b0210c06a1610a94d" was already recorded at path "config/configmaps/openshift-config/etcd-metric-serving-ca/ca-bundle.crt", recording another one with a different path "config/configmaps/openshift-config/etcd-serving-ca/ca-bundle.crt"
2025-11-17T09:39:11.082820795Z I1117 09:39:11.082802       1 recorder.go:70] Recording config/configmaps/openshift-config/initial-kube-apiserver-server-ca/ca-bundle.crt with fingerprint=b115cabb75d3a5abd871b8626a1e9e91f4b87698e6f540c0d58abddc29c34240
2025-11-17T09:39:11.082877167Z I1117 09:39:11.082865       1 recorder.go:70] Recording config/configmaps/openshift-config/kube-root-ca.crt/ca.crt with fingerprint=8f83ef1ecfe5c8bf1cdc6211109047b8218a3e3824396ed4c00cdb684c75b209
2025-11-17T09:39:11.082885585Z I1117 09:39:11.082877       1 recorder.go:70] Recording config/configmaps/openshift-config/openshift-install-manifests/invoker with fingerprint=54b4c107bb91b5685caa4cbae1931790e802b3dfe96a2c5c4d05bee411b9f8be
2025-11-17T09:39:11.082893250Z I1117 09:39:11.082885       1 recorder.go:70] Recording config/configmaps/openshift-config/openshift-install-manifests/version with fingerprint=9eeb16073cf22c23f86fe31589bb0dab8c8cd1a1ea516d61caa1d08f4ec0d114
2025-11-17T09:39:11.082913179Z I1117 09:39:11.082901       1 recorder.go:70] Recording config/configmaps/openshift-config/openshift-service-ca.crt/service-ca.crt with fingerprint=34cdf5d36d45a9e3ee7c2be31546e3d937ae4ebf144ac07b0210c06a1610a94d
2025-11-17T09:39:11.082921076Z W1117 09:39:11.082914       1 gather.go:161] issue recording gatherer "clusterconfig" function "config_maps" result "config/configmaps/openshift-config/openshift-service-ca.crt/service-ca.crt" because of the warning: warning: the record with the same fingerprint "34cdf5d36d45a9e3ee7c2be31546e3d937ae4ebf144ac07b0210c06a1610a94d" was already recorded at path "config/configmaps/openshift-config/etcd-serving-ca/ca-bundle.crt", recording another one with a different path "config/configmaps/openshift-config/openshift-service-ca.crt/service-ca.crt"
2025-11-17T09:39:11.082945881Z I1117 09:39:11.082933       1 recorder.go:70] Recording config/configmaps/kube-system/cluster-config-v1/install-config with fingerprint=e4b05e016eb5b2c76368a46d2d7ab51b1f46a7cd06fc825be0157079df94e6d8
2025-11-17T09:39:11.082953765Z I1117 09:39:11.082945       1 gather.go:183] gatherer "clusterconfig" function "config_maps" took 60.306684ms to process 10 records
2025-11-17T09:39:11.096631854Z I1117 09:39:11.096566       1 tasks_processing.go:74] XXXXXX 32 stopped.
2025-11-17T09:39:11.096631854Z I1117 09:39:11.096590       1 gather.go:183] gatherer "clusterconfig" function "openshift_apiserver_operator_logs" took 76.19977ms to process 0 records
2025-11-17T09:39:11.130799023Z I1117 09:39:11.130740       1 tasks_processing.go:74] XXXXXX 12 stopped.
2025-11-17T09:39:11.130799023Z I1117 09:39:11.130764       1 gather.go:183] gatherer "clusterconfig" function "openshift_authentication_logs" took 109.571653ms to process 0 records
2025-11-17T09:39:11.148981642Z I1117 09:39:11.148907       1 tasks_processing.go:74] XXXXXX 45 stopped.
2025-11-17T09:39:11.149265968Z I1117 09:39:11.149230       1 recorder.go:70] Recording config/pod/openshift-cloud-controller-manager-operator/cluster-cloud-controller-manager-operator-596cb57f55-6mp8n with fingerprint=ba10856b45cb11b6de2bd3453cf0b50c5003f83f046dd5213b77f99f81cc7100
2025-11-17T09:39:11.149413971Z I1117 09:39:11.149388       1 recorder.go:70] Recording config/pod/openshift-cluster-machine-approver/machine-approver-6ffcd5ff8c-q5zdk with fingerprint=200768e093b4f3688cb9ec385d102225c8dd285ccdaad559f15ed4dadb856746
2025-11-17T09:39:11.149524815Z I1117 09:39:11.149498       1 recorder.go:70] Recording config/pod/openshift-cluster-node-tuning-operator/cluster-node-tuning-operator-85f658c4d5-5h6rh with fingerprint=a537f0d2dc515767e5e9e420a4426c31210d937521028589ddd5f1961d5de20b
2025-11-17T09:39:11.149608823Z I1117 09:39:11.149584       1 recorder.go:70] Recording config/pod/openshift-cluster-storage-operator/csi-snapshot-controller-6ff5fc4db6-htx72 with fingerprint=f84be2c52ff85ba640a710765472361a747f252be80c4eaae3e54f323378a129
2025-11-17T09:39:11.149730007Z I1117 09:39:11.149710       1 recorder.go:70] Recording config/pod/openshift-ingress-operator/ingress-operator-66db58ffbb-hlkc5 with fingerprint=4f4428d68f98c49daf707b0f6572677a2bccde22db0d11f18ccbee8f02a77b83
2025-11-17T09:39:11.149959159Z I1117 09:39:11.149933       1 recorder.go:70] Recording config/pod/openshift-kube-apiserver/kube-apiserver-XXXXXX2 with fingerprint=e0f17df199d9b5a67de8fb7e45fc38a38f40bc7b44450b493ae18f5731704792
2025-11-17T09:39:11.150196408Z I1117 09:39:11.150167       1 recorder.go:70] Recording config/pod/openshift-kube-controller-manager/kube-controller-manager-XXXXXX1 with fingerprint=a8160d50ac8272dac82d835a967bfcd4928e6eb22dada311b38b99eb7578e69c
2025-11-17T09:39:11.150401649Z I1117 09:39:11.150373       1 recorder.go:70] Recording config/pod/openshift-kube-controller-manager/kube-controller-manager-XXXXXX2 with fingerprint=b3f07ae01378567b5710f423411d9d836cbd3c54cbac4ee9e93b486e2ca3dd0d
2025-11-17T09:39:11.150594342Z I1117 09:39:11.150556       1 recorder.go:70] Recording config/pod/openshift-kube-scheduler/openshift-kube-scheduler-XXXXXX2 with fingerprint=003a5d0fe1e728a00bde0ab59b1c4388a2cc02f28c5281c66cabcc3d6f399388
2025-11-17T09:39:11.150755054Z I1117 09:39:11.150722       1 recorder.go:70] Recording config/pod/openshift-machine-api/cluster-autoscaler-operator-577bc44bdf-8ssjj with fingerprint=a8d011febc02e162432387295baf53b0416b3d979ea86cba70090c2f44535305
2025-11-17T09:39:11.150829746Z I1117 09:39:11.150810       1 recorder.go:70] Recording config/pod/openshift-machine-api/control-plane-machine-set-operator-54989cfc99-766pz with fingerprint=2751215ecabee7ff7a5065c0177907b2d31930d0268641af546e8dd4366630ad
2025-11-17T09:39:11.151060110Z I1117 09:39:11.151015       1 recorder.go:70] Recording config/pod/openshift-machine-api/machine-api-controllers-57fbbf9557-pdr5j with fingerprint=8828e5b1476a5523ac64b7795a5b48c8fd6ba2c7ccb2a376c12f57d27d84eb5a
2025-11-17T09:39:11.151158678Z I1117 09:39:11.151139       1 recorder.go:70] Recording config/pod/openshift-machine-api/machine-api-operator-5fb84845c4-2h8bz with fingerprint=64ebe348cdf45a0897c15f8006bede80d6bf881368a144653310d02f8dd15e50
2025-11-17T09:39:11.151254966Z I1117 09:39:11.151227       1 recorder.go:70] Recording config/pod/openshift-machine-config-operator/kube-rbac-proxy-crio-XXXXXX0 with fingerprint=ffef4e184b33c82cbf4ba6e710e45190607959505e42823f9d06b95a7a7be293
2025-11-17T09:39:11.151357766Z I1117 09:39:11.151332       1 recorder.go:70] Recording config/pod/openshift-machine-config-operator/kube-rbac-proxy-crio-XXXXXX1 with fingerprint=590ccc28e5956430500c91993c9b9100c9335a2dcee28d1f74ca1fd0d8bb2863
2025-11-17T09:39:11.151437233Z I1117 09:39:11.151416       1 recorder.go:70] Recording config/pod/openshift-machine-config-operator/kube-rbac-proxy-crio-XXXXXX2 with fingerprint=0bf2a316db536275cebf6dd91a35794c05f9e287b83d3a56deebceb508468067
2025-11-17T09:39:11.151537424Z I1117 09:39:11.151508       1 recorder.go:70] Recording config/pod/openshift-machine-config-operator/kube-rbac-proxy-crio-XXXXXX0 with fingerprint=8a940e12068da7f206ade3d33c1ccb667c1317802ad15be1a4a6db65472142a1
2025-11-17T09:39:11.151603613Z I1117 09:39:11.151577       1 recorder.go:70] Recording config/pod/openshift-machine-config-operator/kube-rbac-proxy-crio-XXXXXX1 with fingerprint=4701740683071cccc0a0d6976e6aab99c3fd5e2441748cac2420936d03df2f6e
2025-11-17T09:39:11.151703850Z I1117 09:39:11.151676       1 recorder.go:70] Recording config/pod/openshift-marketplace/marketplace-operator-69f959b844-8hlkw with fingerprint=de6940a7733cad3d23b5401feb4505327681a9d80813056f094e4c004c13a16a
2025-11-17T09:39:11.151818861Z I1117 09:39:11.151793       1 recorder.go:70] Recording config/pod/openshift-multus/multus-mlhj6 with fingerprint=7b5cc9fbaf5dda33c9b6f282d0a8a2eed3f54421f55b8a4daa3647f968e3f846
2025-11-17T09:39:11.151925493Z I1117 09:39:11.151903       1 recorder.go:70] Recording config/pod/openshift-multus/multus-n4nxn with fingerprint=d9fa8ccaf6a0aebec6b97f1fdf50a30d898a69d5f00e0c5330d769d74456bbc5
2025-11-17T09:39:11.152086200Z I1117 09:39:11.152052       1 recorder.go:70] Recording config/pod/openshift-network-node-identity/network-node-identity-7hmhp with fingerprint=509ff7f4ea39510ce45c88281b702209c9c9ad349fd96fe5e4edaf53a1143bd1
2025-11-17T09:39:11.152185053Z I1117 09:39:11.152166       1 recorder.go:70] Recording config/pod/openshift-network-node-identity/network-node-identity-qbhkd with fingerprint=466294ff712ba3965b6066d8d209053edf301a99e90bff432c4dc642fcb45bd5
2025-11-17T09:39:11.152295575Z I1117 09:39:11.152265       1 recorder.go:70] Recording config/pod/openshift-operator-lifecycle-manager/package-server-manager-d8bd45c9-pvjwf with fingerprint=f4bd38eced283b2607e6b2c3c2c4aaba9fb2d0f350acdbb3737132a1b36aa3af
2025-11-17T09:39:11.152764799Z I1117 09:39:11.152733       1 recorder.go:70] Recording config/running_containers with fingerprint=0d9474e3cd931c7506932eca3d3a0b7a50e68da3370e9995c96cf22713f456cc
2025-11-17T09:39:11.152764799Z I1117 09:39:11.152751       1 gather.go:183] gatherer "clusterconfig" function "container_images" took 98.360454ms to process 25 records
2025-11-17T09:39:11.163764594Z I1117 09:39:11.163726       1 tasks_processing.go:74] XXXXXX 4 stopped.
2025-11-17T09:39:11.163815813Z I1117 09:39:11.163789       1 recorder.go:70] Recording config/pod/openshift-kube-controller-manager/logs/kube-controller-manager-XXXXXX1/errors.log with fingerprint=0c8e97788c6252142a04733c52bde27280f3a9b2b20d43794f6621a5436aba8e
2025-11-17T09:39:11.163857066Z I1117 09:39:11.163841       1 gather.go:183] gatherer "clusterconfig" function "kube_controller_manager_logs" took 143.710889ms to process 1 records
2025-11-17T09:39:11.168586770Z I1117 09:39:11.168538       1 tasks_processing.go:74] XXXXXX 29 stopped.
2025-11-17T09:39:11.168586770Z I1117 09:39:11.168566       1 recorder.go:70] Recording config/pod/openshift-kube-scheduler/logs/openshift-kube-scheduler-XXXXXX1/messages.log with fingerprint=5508c8e688a71f4e7607dded5c784c3a07b742d20a761994c0ca16653592c80b
2025-11-17T09:39:11.168586770Z I1117 09:39:11.168577       1 recorder.go:70] Recording config/pod/openshift-kube-scheduler/logs/openshift-kube-scheduler-XXXXXX2/messages.log with fingerprint=a1503dfe1b4166270d18869bdde8e29b283758d4fc0b5e18821397d113c30112
2025-11-17T09:39:11.168613473Z I1117 09:39:11.168605       1 gather.go:183] gatherer "clusterconfig" function "scheduler_logs" took 146.31043ms to process 2 records
2025-11-17T09:39:11.182123843Z I1117 09:39:11.182074       1 tasks_processing.go:74] XXXXXX 47 stopped.
2025-11-17T09:39:11.182470999Z I1117 09:39:11.182446       1 recorder.go:70] Recording config/machineconfigs/00-XXXXXX with fingerprint=9f15634d6e7c75a9497e08d95bc2fd18c7e7ceb258d9266eeaa00be721411f6c
2025-11-17T09:39:11.182749460Z I1117 09:39:11.182729       1 recorder.go:70] Recording config/machineconfigs/00-XXXXXX with fingerprint=a6102a214fcc23ad502acbf00b1b3d850ac9a71ee4702334d5f1fc5f944c46e1
2025-11-17T09:39:11.182810468Z I1117 09:39:11.182791       1 recorder.go:70] Recording config/machineconfigs/01-XXXXXX-container-runtime with fingerprint=97fc74f533ab2d69100be00c1bac40736c2462b445ded6873651a0905f641183
2025-11-17T09:39:11.182895909Z I1117 09:39:11.182877       1 recorder.go:70] Recording config/machineconfigs/01-XXXXXX-kubelet with fingerprint=f363157d4e0a44bc2cc322a6fd1a5a500a14fe51021d816b83650436ccca388c
2025-11-17T09:39:11.182939226Z I1117 09:39:11.182925       1 recorder.go:70] Recording config/machineconfigs/01-XXXXXX-container-runtime with fingerprint=4d397c9fb9b4556717ca15d7db27bf6c4c2893d94b4401de8932a8331ee93de2
2025-11-17T09:39:11.183043182Z I1117 09:39:11.183016       1 recorder.go:70] Recording config/machineconfigs/01-XXXXXX-kubelet with fingerprint=a3de0634e4574b7ae5b544a2319b0759b4566f88c887cb224502fe36fb6a06a2
2025-11-17T09:39:11.183064024Z I1117 09:39:11.183049       1 recorder.go:70] Recording config/machineconfigs/50-XXXXXXs-chrony-configuration with fingerprint=3dbcca93628593108771f1d938c346fd7fafd12babf1cbbb47fc4ae50ce1ac7d
2025-11-17T09:39:11.183098277Z I1117 09:39:11.183085       1 recorder.go:70] Recording config/machineconfigs/50-XXXXXXs-chrony-configuration with fingerprint=cf6773693d62485ddc7544e3838ce9d99c99cf5651e0597395e5b50c696e3c34
2025-11-17T09:39:11.183136108Z I1117 09:39:11.183122       1 recorder.go:70] Recording config/machineconfigs/97-XXXXXX-generated-kubelet with fingerprint=ebc2177b394d03084185a944c6e2aaf7910f2f0f9c2bff67d83f587fe93ac3ea
2025-11-17T09:39:11.183172663Z I1117 09:39:11.183159       1 recorder.go:70] Recording config/machineconfigs/97-XXXXXX-generated-kubelet with fingerprint=69044e3bdeff81c18b8e5e9026471f180b63dc07ba1dc372d9266fe4376a343c
2025-11-17T09:39:11.183208804Z I1117 09:39:11.183195       1 recorder.go:70] Recording config/machineconfigs/98-XXXXXX-generated-kubelet with fingerprint=0b39563eed7df5b4c5250b5cd11183e6d95d9060e493413edf957d01e7e64e8c
2025-11-17T09:39:11.183239473Z I1117 09:39:11.183226       1 recorder.go:70] Recording config/machineconfigs/98-XXXXXX-generated-kubelet with fingerprint=5dbd48b573dbd98505a4eee9aafabea571cfa6cf59bf7235f8b26bb7b03af469
2025-11-17T09:39:11.183266628Z I1117 09:39:11.183253       1 recorder.go:70] Recording config/machineconfigs/99-assisted-installer-XXXXXX-ssh with fingerprint=dc7db77c2e881d29c648dd392aedd6bf0426d4c43f4e5b9eebd197051cc4372b
2025-11-17T09:39:11.183363503Z I1117 09:39:11.183327       1 recorder.go:70] Recording config/machineconfigs/99-XXXXXX-generated-registries with fingerprint=b0793ca79c8d49f993a00db5aead70c9c4d25380e03071f4666d15e814c61378
2025-11-17T09:39:11.183389203Z I1117 09:39:11.183376       1 recorder.go:70] Recording config/machineconfigs/99-XXXXXX-ssh with fingerprint=7dfbc0dfd44fd1f5e8ce55b5913d1d5b809f104219d33d70ddcc27515fec77d5
2025-11-17T09:39:11.183425944Z I1117 09:39:11.183414       1 recorder.go:70] Recording config/machineconfigs/99-XXXXXX-generated-registries with fingerprint=7dcb65b222a1843bef0899f9362b797aa19f50327d1d532b35f68eff33a0fe37
2025-11-17T09:39:11.183452485Z I1117 09:39:11.183441       1 recorder.go:70] Recording config/machineconfigs/99-XXXXXX-ssh with fingerprint=7213926649d3923486d186037be3736b0f3ebcc548678f75a3850bbcf69e03c8
2025-11-17T09:39:11.183799087Z I1117 09:39:11.183774       1 recorder.go:70] Recording config/machineconfigs/rendered-XXXXXX-0a819a2a78bbc6d7b05226fcfa282c1f with fingerprint=de4b373f19a01bfaa9853ba566d129f3f271a04d1bfa527c955ceecb9f0628fc
2025-11-17T09:39:11.184139278Z I1117 09:39:11.184113       1 recorder.go:70] Recording config/machineconfigs/rendered-XXXXXX-5187170d5ec13041c4aa3af7746bf70d with fingerprint=25fe6cbb815714eb45be239d82f42892b65002e301598f13f46805fb14ca2015
2025-11-17T09:39:11.184467340Z I1117 09:39:11.184443       1 recorder.go:70] Recording config/machineconfigs/rendered-XXXXXX-9ea4404e175496104b2d06c8688c8092 with fingerprint=e3d24edd25ee7c7af5dd3b77c6ddc7fe9e15a9bc9609719f98b3693f4b4e552d
2025-11-17T09:39:11.184761000Z I1117 09:39:11.184741       1 recorder.go:70] Recording config/machineconfigs/rendered-XXXXXX-aaae2bf7120cca1ce72bbcccc91fb935 with fingerprint=fecc6dd8266fe701ffd1014f09182bc7a3e17a9f7ad1d5e9c0769901f18a2271
2025-11-17T09:39:11.184761000Z I1117 09:39:11.184752       1 gather.go:183] gatherer "clusterconfig" function "machine_configs" took 162.449392ms to process 21 records
2025-11-17T09:39:11.202463017Z I1117 09:39:11.202407       1 tasks_processing.go:74] XXXXXX 6 stopped.
2025-11-17T09:39:11.202570259Z I1117 09:39:11.202549       1 recorder.go:70] Recording config/node/logs/XXXXXX0.log with fingerprint=43eb1f6ac053930af5d0eaa3ddbe37f4b66705da9d1732fa928aff2e5e7e1054
2025-11-17T09:39:11.202720534Z I1117 09:39:11.202692       1 recorder.go:70] Recording config/node/logs/XXXXXX1.log with fingerprint=1cb61cf2d1f217e92232a1dd6f417f56a416b5ceeea1870452b25f2e3767950f
2025-11-17T09:39:11.202777695Z I1117 09:39:11.202760       1 recorder.go:70] Recording config/node/logs/XXXXXX2.log with fingerprint=b6ebd80a40d7bbbda15463868919b9fe8f3de4dfc6cd40d20c9ba2a841cbd961
2025-11-17T09:39:11.202786996Z I1117 09:39:11.202775       1 gather.go:183] gatherer "clusterconfig" function "node_logs" took 151.191819ms to process 3 records
2025-11-17T09:39:11.226179173Z I1117 09:39:11.226098       1 request.go:628] Waited for 114.338801ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cloud-controller-manager-operator/serviceaccounts?limit=1000
2025-11-17T09:39:11.273180840Z I1117 09:39:11.273124       1 request.go:628] Waited for 144.051645ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-cluster-machine-approver/installplans?limit=500
2025-11-17T09:39:11.299192541Z I1117 09:39:11.299135       1 request.go:628] Waited for 143.581581ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods
2025-11-17T09:39:11.317398415Z I1117 09:39:11.317351       1 request.go:628] Waited for 153.646853ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/ingress.operator.openshift.io/v1/namespaces/openshift-ingress-operator/dnsrecords/default
2025-11-17T09:39:11.318888068Z I1117 09:39:11.318862       1 gather_cluster_operators.go:181] Unable to get dnsrecords.ingress.operator.openshift.io resource due to: dnsrecords.ingress.operator.openshift.io "default" is forbidden: User "system:serviceaccount:openshift-insights:gather" cannot get resource "dnsrecords" in API group "ingress.operator.openshift.io" in the namespace "openshift-ingress-operator"
2025-11-17T09:39:11.426071949Z I1117 09:39:11.425992       1 request.go:628] Waited for 197.075894ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cloud-credential-operator/serviceaccounts?limit=1000
2025-11-17T09:39:11.473806095Z I1117 09:39:11.473733       1 request.go:628] Waited for 196.881698ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-cluster-node-tuning-operator/installplans?limit=500
2025-11-17T09:39:11.498590861Z I1117 09:39:11.498470       1 request.go:628] Waited for 195.828083ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/events
2025-11-17T09:39:11.517237758Z I1117 09:39:11.517176       1 request.go:628] Waited for 198.249046ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1/insightsoperators/cluster
2025-11-17T09:39:11.625505608Z I1117 09:39:11.625442       1 request.go:628] Waited for 194.75989ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cloud-network-config-controller/serviceaccounts?limit=1000
2025-11-17T09:39:11.673768501Z I1117 09:39:11.673626       1 request.go:628] Waited for 196.090082ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-cluster-samples-operator/installplans?limit=500
2025-11-17T09:39:11.722953494Z I1117 09:39:11.722892       1 gather_cluster_operators.go:181] Unable to get podnetworkconnectivitychecks.controlplane.operator.openshift.io resource due to: name is required
2025-11-17T09:39:11.825864230Z I1117 09:39:11.825704       1 request.go:628] Waited for 137.725397ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cluster-csi-drivers/serviceaccounts?limit=1000
2025-11-17T09:39:11.873572950Z I1117 09:39:11.873514       1 request.go:628] Waited for 183.863868ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-cluster-storage-operator/installplans?limit=500
2025-11-17T09:39:11.899334135Z I1117 09:39:11.899270       1 request.go:628] Waited for 196.053247ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console-operator/events
2025-11-17T09:39:11.917656152Z I1117 09:39:11.917609       1 request.go:628] Waited for 194.625301ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster
2025-11-17T09:39:12.026097547Z I1117 09:39:12.025951       1 request.go:628] Waited for 197.483395ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cluster-machine-approver/serviceaccounts?limit=1000
2025-11-17T09:39:12.073018420Z I1117 09:39:12.072876       1 request.go:628] Waited for 196.833932ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-cluster-version/installplans?limit=500
2025-11-17T09:39:12.098795242Z I1117 09:39:12.098738       1 request.go:628] Waited for 194.831958ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console/pods
2025-11-17T09:39:12.118247558Z I1117 09:39:12.118193       1 request.go:628] Waited for 196.36817ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster
2025-11-17T09:39:12.121597724Z I1117 09:39:12.121556       1 gather_cluster_operators.go:181] Unable to get podnetworkconnectivitychecks.controlplane.operator.openshift.io resource due to: name is required
2025-11-17T09:39:12.225635403Z I1117 09:39:12.225510       1 request.go:628] Waited for 196.604107ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cluster-node-tuning-operator/serviceaccounts?limit=1000
2025-11-17T09:39:12.273409912Z I1117 09:39:12.273355       1 request.go:628] Waited for 197.691394ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-config/installplans?limit=500
2025-11-17T09:39:12.299234766Z I1117 09:39:12.299089       1 request.go:628] Waited for 196.246505ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console/events
2025-11-17T09:39:12.317781286Z I1117 09:39:12.317725       1 request.go:628] Waited for 196.095005ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1/kubestorageversionmigrators/cluster
2025-11-17T09:39:12.425890741Z I1117 09:39:12.425841       1 request.go:628] Waited for 197.569046ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cluster-samples-operator/serviceaccounts?limit=1000
2025-11-17T09:39:12.473698909Z I1117 09:39:12.473650       1 request.go:628] Waited for 196.762511ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-config-managed/installplans?limit=500
2025-11-17T09:39:12.498419513Z I1117 09:39:12.498368       1 request.go:628] Waited for 195.156858ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods
2025-11-17T09:39:12.522459541Z I1117 09:39:12.522409       1 request.go:628] Waited for 201.625017ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/network.operator.openshift.io/v1/namespaces/openshift-ovn-kubernetes/operatorpkis/ovn
2025-11-17T09:39:12.524084719Z I1117 09:39:12.523999       1 gather_cluster_operators.go:181] Unable to get operatorpkis.network.operator.openshift.io resource due to: operatorpkis.network.operator.openshift.io "ovn" is forbidden: User "system:serviceaccount:openshift-insights:gather" cannot get resource "operatorpkis" in API group "network.operator.openshift.io" in the namespace "openshift-ovn-kubernetes"
2025-11-17T09:39:12.626091129Z I1117 09:39:12.626013       1 request.go:628] Waited for 195.310883ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cluster-storage-operator/serviceaccounts?limit=1000
2025-11-17T09:39:12.673451928Z I1117 09:39:12.673400       1 request.go:628] Waited for 195.938676ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-config-operator/installplans?limit=500
2025-11-17T09:39:12.698841201Z I1117 09:39:12.698757       1 request.go:628] Waited for 173.964616ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/events
2025-11-17T09:39:12.717929222Z I1117 09:39:12.717866       1 request.go:628] Waited for 193.732639ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/network.operator.openshift.io/v1/namespaces/openshift-ovn-kubernetes/operatorpkis/signer
2025-11-17T09:39:12.719278475Z I1117 09:39:12.719232       1 gather_cluster_operators.go:181] Unable to get operatorpkis.network.operator.openshift.io resource due to: operatorpkis.network.operator.openshift.io "signer" is forbidden: User "system:serviceaccount:openshift-insights:gather" cannot get resource "operatorpkis" in API group "network.operator.openshift.io" in the namespace "openshift-ovn-kubernetes"
2025-11-17T09:39:12.826086927Z I1117 09:39:12.826005       1 request.go:628] Waited for 85.840113ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-cluster-version/serviceaccounts?limit=1000
2025-11-17T09:39:12.873755036Z I1117 09:39:12.873686       1 request.go:628] Waited for 133.514606ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-console/installplans?limit=500
2025-11-17T09:39:12.899270230Z I1117 09:39:12.899209       1 request.go:628] Waited for 153.562655ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-user-workload-monitoring/pods
2025-11-17T09:39:12.917701317Z I1117 09:39:12.917645       1 request.go:628] Waited for 198.342164ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/network.operator.openshift.io/v1/namespaces/openshift-network-node-identity/operatorpkis/network-node-identity
2025-11-17T09:39:12.918833463Z I1117 09:39:12.918807       1 gather_cluster_operators.go:181] Unable to get operatorpkis.network.operator.openshift.io resource due to: operatorpkis.network.operator.openshift.io "network-node-identity" is forbidden: User "system:serviceaccount:openshift-insights:gather" cannot get resource "operatorpkis" in API group "network.operator.openshift.io" in the namespace "openshift-network-node-identity"
2025-11-17T09:39:13.026395477Z I1117 09:39:13.026338       1 request.go:628] Waited for 197.694306ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config/serviceaccounts?limit=1000
2025-11-17T09:39:13.073199559Z I1117 09:39:13.073134       1 request.go:628] Waited for 196.905909ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-console-operator/installplans?limit=500
2025-11-17T09:39:13.099225860Z I1117 09:39:13.099174       1 request.go:628] Waited for 197.597054ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-user-workload-monitoring/events
2025-11-17T09:39:13.102477856Z I1117 09:39:13.102451       1 gather_cluster_operator_pods_and_events.go:117] Found 41 pods with 96 containers
2025-11-17T09:39:13.102477856Z I1117 09:39:13.102466       1 gather_cluster_operator_pods_and_events.go:231] Maximum buffer size: 87381 bytes
2025-11-17T09:39:13.102526455Z I1117 09:39:13.102509       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-openshift container oauth-openshift-76f996bb9d-9lf69 pod in namespace openshift-authentication (previous: false).
2025-11-17T09:39:13.116035499Z I1117 09:39:13.115971       1 controller.go:200] Source periodic-clusterconfig *controllerstatus.Simple is not ready
2025-11-17T09:39:13.116035499Z I1117 09:39:13.115990       1 controller.go:200] Source periodic-conditional *controllerstatus.Simple is not ready
2025-11-17T09:39:13.116035499Z I1117 09:39:13.115995       1 controller.go:200] Source periodic-workloads *controllerstatus.Simple is not ready
2025-11-17T09:39:13.116073790Z I1117 09:39:13.116037       1 controller.go:444] The operator is healthy
2025-11-17T09:39:13.116081615Z I1117 09:39:13.116078       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:39:13.117819948Z I1117 09:39:13.117796       1 request.go:628] Waited for 198.913264ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1/networks/cluster
2025-11-17T09:39:13.226271243Z I1117 09:39:13.226221       1 request.go:628] Waited for 196.206857ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/serviceaccounts?limit=1000
2025-11-17T09:39:13.273668640Z I1117 09:39:13.273624       1 request.go:628] Waited for 197.438244ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-console-user-settings/installplans?limit=500
2025-11-17T09:39:13.298518300Z I1117 09:39:13.298474       1 request.go:628] Waited for 195.890153ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-authentication/pods/oauth-openshift-76f996bb9d-9lf69/log?container=oauth-openshift&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:13.305695399Z I1117 09:39:13.305669       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-openshift container oauth-openshift-76f996bb9d-cdbwh pod in namespace openshift-authentication (previous: false).
2025-11-17T09:39:13.317808086Z I1117 09:39:13.317776       1 request.go:628] Waited for 196.527939ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1/openshiftapiservers/cluster
2025-11-17T09:39:13.322539392Z I1117 09:39:13.322515       1 gather_cluster_operators.go:181] Unable to get podnetworkconnectivitychecks.controlplane.operator.openshift.io resource due to: name is required
2025-11-17T09:39:13.425496323Z I1117 09:39:13.425402       1 request.go:628] Waited for 196.381089ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config-operator/serviceaccounts?limit=1000
2025-11-17T09:39:13.473276122Z I1117 09:39:13.473220       1 request.go:628] Waited for 196.927515ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-controller-manager/installplans?limit=500
2025-11-17T09:39:13.498875368Z I1117 09:39:13.498828       1 request.go:628] Waited for 193.017771ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-authentication/pods/oauth-openshift-76f996bb9d-cdbwh/log?container=oauth-openshift&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:13.505910433Z I1117 09:39:13.505856       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-openshift container oauth-openshift-76f996bb9d-wbblj pod in namespace openshift-authentication (previous: false).
2025-11-17T09:39:13.518225993Z I1117 09:39:13.518179       1 request.go:628] Waited for 195.557482ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1/openshiftcontrollermanagers/cluster
2025-11-17T09:39:13.626074799Z I1117 09:39:13.626009       1 request.go:628] Waited for 197.361224ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console/serviceaccounts?limit=1000
2025-11-17T09:39:13.675208250Z I1117 09:39:13.675155       1 request.go:628] Waited for 198.319279ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-controller-manager-operator/installplans?limit=500
2025-11-17T09:39:13.699450492Z I1117 09:39:13.699402       1 request.go:628] Waited for 193.427283ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-authentication/pods/oauth-openshift-76f996bb9d-wbblj/log?container=oauth-openshift&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:13.707901885Z I1117 09:39:13.707857       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-openshift container oauth-openshift-786998b97f-5d599 pod in namespace openshift-authentication (previous: false).
2025-11-17T09:39:13.718057757Z I1117 09:39:13.718003       1 request.go:628] Waited for 195.774607ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/samples.operator.openshift.io/v1/configs/cluster
2025-11-17T09:39:13.825797336Z I1117 09:39:13.825735       1 request.go:628] Waited for 196.432026ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console-operator/serviceaccounts?limit=1000
2025-11-17T09:39:13.875053476Z I1117 09:39:13.874996       1 request.go:628] Waited for 188.579748ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-dns/installplans?limit=500
2025-11-17T09:39:13.899330378Z I1117 09:39:13.899278       1 request.go:628] Waited for 191.237653ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-authentication/pods/oauth-openshift-786998b97f-5d599/log?container=oauth-openshift&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:13.902073419Z I1117 09:39:13.902049       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:13.902116046Z I1117 09:39:13.902106       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-openshift container oauth-openshift-7d4fc55645-tq662 pod in namespace openshift-authentication (previous: false).
2025-11-17T09:39:13.918246808Z I1117 09:39:13.918209       1 request.go:628] Waited for 195.79379ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1/servicecas/cluster
2025-11-17T09:39:14.026056693Z I1117 09:39:14.025951       1 request.go:628] Waited for 197.603184ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console-user-settings/serviceaccounts?limit=1000
2025-11-17T09:39:14.073382733Z I1117 09:39:14.073330       1 request.go:628] Waited for 194.920372ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-dns-operator/installplans?limit=500
2025-11-17T09:39:14.099036396Z I1117 09:39:14.098970       1 request.go:628] Waited for 196.745796ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-authentication/pods/oauth-openshift-7d4fc55645-tq662/log?container=oauth-openshift&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:14.101542683Z I1117 09:39:14.101509       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:14.101583444Z I1117 09:39:14.101565       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for authentication-operator container authentication-operator-f4cdf7bdc-t2h4s pod in namespace openshift-authentication-operator (previous: true).
2025-11-17T09:39:14.118355730Z I1117 09:39:14.117644       1 request.go:628] Waited for 196.449982ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1/storages/cluster
2025-11-17T09:39:14.123616716Z I1117 09:39:14.123592       1 tasks_processing.go:74] XXXXXX 26 stopped.
2025-11-17T09:39:14.123748140Z I1117 09:39:14.123720       1 recorder.go:70] Recording config/clusteroperator/authentication with fingerprint=99f17084f85da3913f21cc15b43cf12d0573c1a9dd8becec59bf9d5b2062a827
2025-11-17T09:39:14.123894147Z I1117 09:39:14.123875       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/authentication/cluster with fingerprint=c5a73b418be5ad8c91d2bbb85b9eede278811924642b143ab7f8f5e7452d9a3c
2025-11-17T09:39:14.123940750Z I1117 09:39:14.123923       1 recorder.go:70] Recording config/clusteroperator/baremetal with fingerprint=0a0e09446109e5ca7dfe0a84f086f9088e377ce474c7d4059c21999b678b472b
2025-11-17T09:39:14.123992598Z I1117 09:39:14.123980       1 recorder.go:70] Recording config/clusteroperator/cloud-controller-manager with fingerprint=7a2803605be8000f18c812ca9751651e502ed0ea0830bfb9b56227b4ad02f6c4
2025-11-17T09:39:14.124097649Z I1117 09:39:14.124078       1 recorder.go:70] Recording config/clusteroperator/cloud-credential with fingerprint=edc68932b77e78a6d50719011382ed425f422144074b6d40eabe16bf056725a6
2025-11-17T09:39:14.124097649Z I1117 09:39:14.124094       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/cloudcredential/cluster with fingerprint=46c3957079df4e4653a808e6f850abac72c8b1276aa6de05929cc8264f4a1d93
2025-11-17T09:39:14.124144350Z I1117 09:39:14.124131       1 recorder.go:70] Recording config/clusteroperator/cluster-autoscaler with fingerprint=f8df7e6a8016e1d15b48fa067118a147834789f8faf5c88e5a6d096b7d3aa9c5
2025-11-17T09:39:14.124168207Z I1117 09:39:14.124156       1 recorder.go:70] Recording config/clusteroperator/config-operator with fingerprint=feb231527f6740be9f879afdc96d3d0d6740a8cb688936e0df52c37a3658c0ea
2025-11-17T09:39:14.124175594Z I1117 09:39:14.124171       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/config/cluster with fingerprint=7196eb59efe1f5998354aed5372544e9c59c3c8135aea7e3914ebf8f3330364b
2025-11-17T09:39:14.124225847Z I1117 09:39:14.124213       1 recorder.go:70] Recording config/clusteroperator/console with fingerprint=dc399ee76a0b5e669c3033d4e6c94950830ba0e88741d84a9fba5e840864fac7
2025-11-17T09:39:14.124242603Z I1117 09:39:14.124230       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/console/cluster with fingerprint=76f580ac116eeb2b958159e2ef1d1c2a54ad465982c97bf6f2f5a8a90121710a
2025-11-17T09:39:14.124268852Z I1117 09:39:14.124256       1 recorder.go:70] Recording config/clusteroperator/control-plane-machine-set with fingerprint=1565719179735b51bbb563ec8d4bb8b235da0f194ac138e19033af0d73642186
2025-11-17T09:39:14.124317217Z I1117 09:39:14.124303       1 recorder.go:70] Recording config/clusteroperator/csi-snapshot-controller with fingerprint=a350ea3f8f7aa7f297f3ddc35a0976b91bc2a57f80eb8b7115ff00c8c34adfa9
2025-11-17T09:39:14.124326876Z I1117 09:39:14.124321       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/csisnapshotcontroller/cluster with fingerprint=5adc514f4b63e2f1ecc68bf6f9c0af70c5eea04522a49524e102721b1c41f80e
2025-11-17T09:39:14.124370949Z I1117 09:39:14.124358       1 recorder.go:70] Recording config/clusteroperator/dns with fingerprint=2e688fe43d0cbdc03b757bcb8c5beb66491a01e3805d70a21a70ce2ddc80d0ba
2025-11-17T09:39:14.124393632Z I1117 09:39:14.124382       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/dns/default with fingerprint=9e7b4ce029030d3d8c3b49af92c556acdcc415000b40d3f969dbdc42c432b47f
2025-11-17T09:39:14.124422220Z I1117 09:39:14.124410       1 recorder.go:70] Recording config/clusteroperator/etcd with fingerprint=a58fc6e04e1e8f80759d8cbcccd13a9aa495904e100ce8203305b754d140e4e0
2025-11-17T09:39:14.124458038Z I1117 09:39:14.124446       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/etcd/cluster with fingerprint=603093945db725701bfbd9047aedf09ef4843154d12b99ef271b70145b5f25b2
2025-11-17T09:39:14.124486881Z I1117 09:39:14.124475       1 recorder.go:70] Recording config/clusteroperator/image-registry with fingerprint=6f27b7bb064eacb4ac73cb023061aee0cdd819b5075efe56bbc8daf7c41f67f0
2025-11-17T09:39:14.124526695Z I1117 09:39:14.124515       1 recorder.go:70] Recording config/clusteroperator/ingress with fingerprint=6817ab899761d8be1a68a2750c0e79abe9a1ba12f15b333300b108e30bac0961
2025-11-17T09:39:14.124554139Z I1117 09:39:14.124542       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/ingresscontroller/openshift-ingress-operator/default with fingerprint=455daaf32e33a6556175eb303aa83b1f6d9be33e53bc1256508e7a02cd423d03
2025-11-17T09:39:14.124600909Z I1117 09:39:14.124588       1 recorder.go:70] Recording config/clusteroperator/insights with fingerprint=2257ee6048819f3ef4c395c483bdac7cfd65d20cd3b7ec66a6c9bf105fe9a2c8
2025-11-17T09:39:14.124617212Z I1117 09:39:14.124605       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/insightsoperator/cluster with fingerprint=e5ff11d57817f84a678f6fa9565af55bd1120227c16a21933637ab62675a6d70
2025-11-17T09:39:14.124650758Z I1117 09:39:14.124638       1 recorder.go:70] Recording config/clusteroperator/kube-apiserver with fingerprint=55d53bbacf5cc1417508ae0f5aa285a9dd6fed150f3f9624d96c269d210e2f07
2025-11-17T09:39:14.124761239Z I1117 09:39:14.124742       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/kubeapiserver/cluster with fingerprint=8a7e85021cfe5b9c05b2275a1df47382399fedb811b5ff5772db585f376a3ee3
2025-11-17T09:39:14.124801538Z I1117 09:39:14.124788       1 recorder.go:70] Recording config/clusteroperator/kube-controller-manager with fingerprint=2671394cf9ecbe3415656bedcff04df746f5f8ade27076a7b3ca11302b2833ec
2025-11-17T09:39:14.124864992Z I1117 09:39:14.124852       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/kubecontrollermanager/cluster with fingerprint=d237faf5bd7c555a994b5bd651fc2c0a9e4bbe199e4eedda010f0ab5808578f6
2025-11-17T09:39:14.124904585Z I1117 09:39:14.124892       1 recorder.go:70] Recording config/clusteroperator/kube-scheduler with fingerprint=c34c4074a1d9f4f617ff6c6283baa0835a2fe96fef15d5f5855fc57dc442418f
2025-11-17T09:39:14.124983595Z I1117 09:39:14.124969       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/kubescheduler/cluster with fingerprint=4c19e5391383b10e290707cbddf1ea79a8359bfe7867869021d7e62d674cb34c
2025-11-17T09:39:14.125014837Z I1117 09:39:14.125002       1 recorder.go:70] Recording config/clusteroperator/kube-storage-version-migrator with fingerprint=ef78f6ab779c46c128e22ff7e5307ed9a54e22e7a94407a68ca2b19adb502580
2025-11-17T09:39:14.125029213Z I1117 09:39:14.125018       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/kubestorageversionmigrator/cluster with fingerprint=9351181aa7e6ada41ef581ab31e13516c6b934cc95710154bafb2eb222cb58db
2025-11-17T09:39:14.125072718Z I1117 09:39:14.125060       1 recorder.go:70] Recording config/clusteroperator/machine-api with fingerprint=ee11cb814f64a50a18183c58683fe52739b2aa95b8440ca666b01f7461abf04d
2025-11-17T09:39:14.125094435Z I1117 09:39:14.125083       1 recorder.go:70] Recording config/clusteroperator/machine-approver with fingerprint=d314b7f2bd955f5bd5387ade2495f3a05d86b3af03d94495013dce197a01f109
2025-11-17T09:39:14.125135456Z I1117 09:39:14.125123       1 recorder.go:70] Recording config/clusteroperator/machine-config with fingerprint=2aa19d72a139157e60be886f85417c5aa8070875ea1c71f172ae503d5f1b626c
2025-11-17T09:39:14.125160565Z I1117 09:39:14.125148       1 recorder.go:70] Recording config/clusteroperator/marketplace with fingerprint=c61b59f528aabe7ec482431bb2abd17a92a13830bdc8d503543a735ad1988947
2025-11-17T09:39:14.125187746Z I1117 09:39:14.125175       1 recorder.go:70] Recording config/clusteroperator/monitoring with fingerprint=6577d58169196cf11f573c4e71df1b66b1e19bcd3ac600cdb07d2f7fb7b93f02
2025-11-17T09:39:14.125296282Z I1117 09:39:14.125281       1 recorder.go:70] Recording config/clusteroperator/network with fingerprint=4144aaba29294e689ecfbf1a22a8853c25c46422f8af92177c035f105d31ebe2
2025-11-17T09:39:14.125338605Z I1117 09:39:14.125325       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/network/cluster with fingerprint=062a1dfe1e47463e98e4233479e06110316bdb70f95b060b586bee57228a4dfb
2025-11-17T09:39:14.125364586Z I1117 09:39:14.125352       1 recorder.go:70] Recording config/clusteroperator/node-tuning with fingerprint=2558ea3068f9c2b0028ac510f1c8d17ee3f4906df221d3821bbe1504c8693ab4
2025-11-17T09:39:14.125393747Z I1117 09:39:14.125382       1 recorder.go:70] Recording config/clusteroperator/openshift-apiserver with fingerprint=882ea7c077f9182fe8fe9dbfc64f80d469e9c843dbf92aa83399f5803c37370d
2025-11-17T09:39:14.125426394Z I1117 09:39:14.125414       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/openshiftapiserver/cluster with fingerprint=8f97a2fa569e5ca812d71af8cc346a2dce63d3b340de6ce0bceb7d4c1b2360f7
2025-11-17T09:39:14.125451111Z I1117 09:39:14.125439       1 recorder.go:70] Recording config/clusteroperator/openshift-controller-manager with fingerprint=5d6320259eff6bec8f5c722e242d499af294fea781c53213cdefd333832ea6b4
2025-11-17T09:39:14.125479615Z I1117 09:39:14.125467       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/openshiftcontrollermanager/cluster with fingerprint=37d05acceda45ce963172d0e862bb5a46c9342bc1bfc35e895c0fb2daedaeb55
2025-11-17T09:39:14.125503155Z I1117 09:39:14.125491       1 recorder.go:70] Recording config/clusteroperator/openshift-samples with fingerprint=3900646bc071739b23989331a5464bf61188097f96f5a804982fb8e0396c18a7
2025-11-17T09:39:14.125509138Z I1117 09:39:14.125504       1 recorder.go:70] Recording config/clusteroperator/samples.operator.openshift.io/config/cluster with fingerprint=439a4284281b8dcef0621bb14ba23e2175a28a50613f86b33a171c49689474fd
2025-11-17T09:39:14.125537098Z I1117 09:39:14.125526       1 recorder.go:70] Recording config/clusteroperator/operator-lifecycle-manager with fingerprint=506deae546869499523dafbff917d0c467e1a25b76f8b1382c6f3c2ba8871a95
2025-11-17T09:39:14.125577900Z I1117 09:39:14.125566       1 recorder.go:70] Recording config/clusteroperator/operator-lifecycle-manager-catalog with fingerprint=bbcf499c31deeae899bc038801967b144e1fde9787267c8b8611ed0ac7bdd7c0
2025-11-17T09:39:14.125602742Z I1117 09:39:14.125591       1 recorder.go:70] Recording config/clusteroperator/operator-lifecycle-manager-packageserver with fingerprint=0250f3164c446efbf0ee88d779db73f40e50c1e86cb3502b57c4e3c15da832cd
2025-11-17T09:39:14.125630741Z I1117 09:39:14.125618       1 recorder.go:70] Recording config/clusteroperator/service-ca with fingerprint=509bc88401621ac58e3ec1010eb11941ab3daeca6d01517126b88b7e634fe94d
2025-11-17T09:39:14.125636879Z I1117 09:39:14.125632       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/serviceca/cluster with fingerprint=812f7edc2cdb30e61e7f2b29454357a40b1a507a4b0c2b7729193b67f0e3b4aa
2025-11-17T09:39:14.125666806Z I1117 09:39:14.125655       1 recorder.go:70] Recording config/clusteroperator/storage with fingerprint=b146e97e15220ba28d9614fb818557832603c5a612acff68fa098dc4850d38a0
2025-11-17T09:39:14.125672884Z I1117 09:39:14.125668       1 recorder.go:70] Recording config/clusteroperator/operator.openshift.io/storage/cluster with fingerprint=8e480f8c1ce1b39baac42d8ec780c57c2592929ae0c801b61ffad49ba13f33ad
2025-11-17T09:39:14.125693329Z I1117 09:39:14.125678       1 gather.go:183] gatherer "clusterconfig" function "operators" took 3.101843697s to process 52 records
2025-11-17T09:39:14.225697286Z I1117 09:39:14.225648       1 request.go:628] Waited for 196.417066ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-controller-manager/serviceaccounts?limit=1000
2025-11-17T09:39:14.273103839Z I1117 09:39:14.273063       1 request.go:628] Waited for 197.075304ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-etcd/installplans?limit=500
2025-11-17T09:39:14.298781166Z I1117 09:39:14.298751       1 request.go:628] Waited for 197.095498ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-authentication-operator/pods/authentication-operator-f4cdf7bdc-t2h4s/log?container=authentication-operator&limitBytes=87381&previous=true&tailLines=100&timestamps=true
2025-11-17T09:39:14.306970296Z I1117 09:39:14.306942       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for authentication-operator container authentication-operator-f4cdf7bdc-t2h4s pod in namespace openshift-authentication-operator (previous: false).
2025-11-17T09:39:14.426129439Z I1117 09:39:14.426085       1 request.go:628] Waited for 192.522132ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-controller-manager-operator/serviceaccounts?limit=1000
2025-11-17T09:39:14.473615218Z I1117 09:39:14.473575       1 request.go:628] Waited for 197.165173ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-etcd-operator/installplans?limit=500
2025-11-17T09:39:14.498644543Z I1117 09:39:14.498604       1 request.go:628] Waited for 191.508291ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-authentication-operator/pods/authentication-operator-f4cdf7bdc-t2h4s/log?container=authentication-operator&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:14.507805827Z I1117 09:39:14.507768       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for router container router-default-668949fb46-52qxp pod in namespace openshift-ingress (previous: false).
2025-11-17T09:39:14.626258398Z I1117 09:39:14.626214       1 request.go:628] Waited for 196.631226ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-dns/serviceaccounts?limit=1000
2025-11-17T09:39:14.673167109Z I1117 09:39:14.673067       1 request.go:628] Waited for 196.454528ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-host-network/installplans?limit=500
2025-11-17T09:39:14.699104372Z I1117 09:39:14.699034       1 request.go:628] Waited for 191.13571ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ingress/pods/router-default-668949fb46-52qxp/log?container=router&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:14.720952760Z I1117 09:39:14.720913       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for router container router-default-668949fb46-d6qkr pod in namespace openshift-ingress (previous: false).
2025-11-17T09:39:14.825939146Z I1117 09:39:14.825878       1 request.go:628] Waited for 196.856203ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-dns-operator/serviceaccounts?limit=1000
2025-11-17T09:39:14.873698934Z I1117 09:39:14.873652       1 request.go:628] Waited for 196.604384ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-image-registry/installplans?limit=500
2025-11-17T09:39:14.898790867Z I1117 09:39:14.898733       1 request.go:628] Waited for 177.62366ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ingress/pods/router-default-668949fb46-d6qkr/log?container=router&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:14.912659154Z I1117 09:39:14.912620       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for fix-audit-permissions container apiserver-795447c5f-5lfvc pod in namespace openshift-oauth-apiserver (previous: true).
2025-11-17T09:39:15.025863040Z I1117 09:39:15.025723       1 request.go:628] Waited for 195.056951ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts?limit=1000
2025-11-17T09:39:15.073205424Z I1117 09:39:15.073150       1 request.go:628] Waited for 196.467482ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-infra/installplans?limit=500
2025-11-17T09:39:15.099316151Z I1117 09:39:15.099244       1 request.go:628] Waited for 186.439485ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-5lfvc/log?container=fix-audit-permissions&limitBytes=87381&previous=true&tailLines=100&timestamps=true
2025-11-17T09:39:15.103664968Z I1117 09:39:15.103638       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for apiserver-795447c5f-5lfvc pod in namespace openshift-oauth-apiserver for failing operator fix-audit-permissions (previous: true): "previous terminated container \"fix-audit-permissions\" in pod \"apiserver-795447c5f-5lfvc\" not found"
2025-11-17T09:39:15.103664968Z I1117 09:39:15.103659       1 gather_cluster_operator_pods_and_events.go:276] Error: "previous terminated container \"fix-audit-permissions\" in pod \"apiserver-795447c5f-5lfvc\" not found"
2025-11-17T09:39:15.103682091Z I1117 09:39:15.103669       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-apiserver container apiserver-795447c5f-5lfvc pod in namespace openshift-oauth-apiserver (previous: true).
2025-11-17T09:39:15.226402672Z I1117 09:39:15.226139       1 request.go:628] Waited for 197.121525ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/serviceaccounts?limit=1000
2025-11-17T09:39:15.272929435Z I1117 09:39:15.272878       1 request.go:628] Waited for 193.613853ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-ingress/installplans?limit=500
2025-11-17T09:39:15.299279616Z I1117 09:39:15.299225       1 request.go:628] Waited for 195.471357ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-5lfvc/log?container=oauth-apiserver&limitBytes=87381&previous=true&tailLines=100&timestamps=true
2025-11-17T09:39:15.307980884Z I1117 09:39:15.307947       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for fix-audit-permissions container apiserver-795447c5f-5lfvc pod in namespace openshift-oauth-apiserver (previous: false).
2025-11-17T09:39:15.426175536Z I1117 09:39:15.426116       1 request.go:628] Waited for 196.226023ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-host-network/serviceaccounts?limit=1000
2025-11-17T09:39:15.473498816Z I1117 09:39:15.473436       1 request.go:628] Waited for 196.42415ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-ingress-canary/installplans?limit=500
2025-11-17T09:39:15.498495921Z I1117 09:39:15.498444       1 request.go:628] Waited for 190.411874ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-5lfvc/log?container=fix-audit-permissions&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:15.503878450Z I1117 09:39:15.503833       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:15.503878450Z I1117 09:39:15.503851       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-apiserver container apiserver-795447c5f-5lfvc pod in namespace openshift-oauth-apiserver (previous: false).
2025-11-17T09:39:15.625606377Z I1117 09:39:15.625492       1 request.go:628] Waited for 195.419919ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-image-registry/serviceaccounts?limit=1000
2025-11-17T09:39:15.673769004Z I1117 09:39:15.673727       1 request.go:628] Waited for 192.436891ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-ingress-operator/installplans?limit=500
2025-11-17T09:39:15.699070041Z I1117 09:39:15.698989       1 request.go:628] Waited for 195.033785ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-5lfvc/log?container=oauth-apiserver&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:15.707253500Z I1117 09:39:15.707199       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for fix-audit-permissions container apiserver-795447c5f-7wtt8 pod in namespace openshift-oauth-apiserver (previous: true).
2025-11-17T09:39:15.826115332Z I1117 09:39:15.826061       1 request.go:628] Waited for 138.023723ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts?limit=1000
2025-11-17T09:39:15.873463903Z I1117 09:39:15.873058       1 request.go:628] Waited for 196.042091ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-insights/installplans?limit=500
2025-11-17T09:39:15.900473232Z I1117 09:39:15.900408       1 request.go:628] Waited for 193.084145ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-7wtt8/log?container=fix-audit-permissions&limitBytes=87381&previous=true&tailLines=100&timestamps=true
2025-11-17T09:39:15.911886955Z I1117 09:39:15.911649       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for apiserver-795447c5f-7wtt8 pod in namespace openshift-oauth-apiserver for failing operator fix-audit-permissions (previous: true): "previous terminated container \"fix-audit-permissions\" in pod \"apiserver-795447c5f-7wtt8\" not found"
2025-11-17T09:39:15.911886955Z I1117 09:39:15.911673       1 gather_cluster_operator_pods_and_events.go:276] Error: "previous terminated container \"fix-audit-permissions\" in pod \"apiserver-795447c5f-7wtt8\" not found"
2025-11-17T09:39:15.911886955Z I1117 09:39:15.911685       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-apiserver container apiserver-795447c5f-7wtt8 pod in namespace openshift-oauth-apiserver (previous: true).
2025-11-17T09:39:16.025633272Z I1117 09:39:16.025530       1 request.go:628] Waited for 193.698709ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ingress/serviceaccounts?limit=1000
2025-11-17T09:39:16.073098756Z I1117 09:39:16.073033       1 request.go:628] Waited for 196.759899ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-kni-infra/installplans?limit=500
2025-11-17T09:39:16.099349938Z I1117 09:39:16.099151       1 request.go:628] Waited for 187.372588ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-7wtt8/log?container=oauth-apiserver&limitBytes=87381&previous=true&tailLines=100&timestamps=true
2025-11-17T09:39:16.107778803Z I1117 09:39:16.107702       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for fix-audit-permissions container apiserver-795447c5f-7wtt8 pod in namespace openshift-oauth-apiserver (previous: false).
2025-11-17T09:39:16.226006516Z I1117 09:39:16.225877       1 request.go:628] Waited for 196.965746ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ingress-canary/serviceaccounts?limit=1000
2025-11-17T09:39:16.273582807Z I1117 09:39:16.273525       1 request.go:628] Waited for 196.289004ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-kube-apiserver/installplans?limit=500
2025-11-17T09:39:16.298450354Z I1117 09:39:16.298393       1 request.go:628] Waited for 190.578726ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-7wtt8/log?container=fix-audit-permissions&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:16.305570553Z I1117 09:39:16.305508       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:16.305570553Z I1117 09:39:16.305525       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-apiserver container apiserver-795447c5f-7wtt8 pod in namespace openshift-oauth-apiserver (previous: false).
2025-11-17T09:39:16.426008791Z I1117 09:39:16.425946       1 request.go:628] Waited for 197.404929ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ingress-operator/serviceaccounts?limit=1000
2025-11-17T09:39:16.472924744Z I1117 09:39:16.472848       1 request.go:628] Waited for 196.862884ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-kube-apiserver-operator/installplans?limit=500
2025-11-17T09:39:16.498881972Z I1117 09:39:16.498828       1 request.go:628] Waited for 193.221398ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-7wtt8/log?container=oauth-apiserver&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:16.507632608Z I1117 09:39:16.507572       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for fix-audit-permissions container apiserver-795447c5f-c8cv2 pod in namespace openshift-oauth-apiserver (previous: false).
2025-11-17T09:39:16.626031867Z I1117 09:39:16.625949       1 request.go:628] Waited for 197.038344ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-insights/serviceaccounts?limit=1000
2025-11-17T09:39:16.673171502Z I1117 09:39:16.673120       1 request.go:628] Waited for 197.280331ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-kube-controller-manager/installplans?limit=500
2025-11-17T09:39:16.700530241Z I1117 09:39:16.700465       1 request.go:628] Waited for 192.778362ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-c8cv2/log?container=fix-audit-permissions&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:16.706190651Z I1117 09:39:16.706147       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:16.706190651Z I1117 09:39:16.706172       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-apiserver container apiserver-795447c5f-c8cv2 pod in namespace openshift-oauth-apiserver (previous: false).
2025-11-17T09:39:16.825791764Z I1117 09:39:16.825706       1 request.go:628] Waited for 196.584302ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kni-infra/serviceaccounts?limit=1000
2025-11-17T09:39:16.873084730Z I1117 09:39:16.873029       1 request.go:628] Waited for 196.984812ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-kube-controller-manager-operator/installplans?limit=500
2025-11-17T09:39:16.899227751Z I1117 09:39:16.899136       1 request.go:628] Waited for 192.845232ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/pods/apiserver-795447c5f-c8cv2/log?container=oauth-apiserver&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:16.907983870Z I1117 09:39:16.907888       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for console-operator container console-operator-6bbf6cd699-cp4p2 pod in namespace openshift-console-operator (previous: true).
2025-11-17T09:39:17.026408890Z I1117 09:39:17.026358       1 request.go:628] Waited for 197.385734ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts?limit=1000
2025-11-17T09:39:17.073785118Z I1117 09:39:17.073725       1 request.go:628] Waited for 197.644899ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-kube-scheduler/installplans?limit=500
2025-11-17T09:39:17.102367806Z I1117 09:39:17.098521       1 request.go:628] Waited for 190.506327ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console-operator/pods/console-operator-6bbf6cd699-cp4p2/log?container=console-operator&limitBytes=87381&previous=true&tailLines=100&timestamps=true
2025-11-17T09:39:17.108406845Z I1117 09:39:17.108351       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for conversion-webhook-server container console-operator-6bbf6cd699-cp4p2 pod in namespace openshift-console-operator (previous: true).
2025-11-17T09:39:17.225556964Z I1117 09:39:17.225510       1 request.go:628] Waited for 195.914356ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver-operator/serviceaccounts?limit=1000
2025-11-17T09:39:17.273242156Z I1117 09:39:17.273190       1 request.go:628] Waited for 196.735793ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-kube-scheduler-operator/installplans?limit=500
2025-11-17T09:39:17.298856561Z I1117 09:39:17.298796       1 request.go:628] Waited for 190.336649ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console-operator/pods/console-operator-6bbf6cd699-cp4p2/log?container=conversion-webhook-server&limitBytes=87381&previous=true&tailLines=100&timestamps=true
2025-11-17T09:39:17.304736597Z I1117 09:39:17.304671       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for console-operator-6bbf6cd699-cp4p2 pod in namespace openshift-console-operator for failing operator conversion-webhook-server (previous: true): "previous terminated container \"conversion-webhook-server\" in pod \"console-operator-6bbf6cd699-cp4p2\" not found"
2025-11-17T09:39:17.304736597Z I1117 09:39:17.304699       1 gather_cluster_operator_pods_and_events.go:276] Error: "previous terminated container \"conversion-webhook-server\" in pod \"console-operator-6bbf6cd699-cp4p2\" not found"
2025-11-17T09:39:17.304736597Z I1117 09:39:17.304716       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for console-operator container console-operator-6bbf6cd699-cp4p2 pod in namespace openshift-console-operator (previous: false).
2025-11-17T09:39:17.425901171Z I1117 09:39:17.425843       1 request.go:628] Waited for 196.672608ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts?limit=1000
2025-11-17T09:39:17.473086409Z I1117 09:39:17.473029       1 request.go:628] Waited for 195.40049ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-kube-storage-version-migrator/installplans?limit=500
2025-11-17T09:39:17.498431787Z I1117 09:39:17.498373       1 request.go:628] Waited for 193.552094ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console-operator/pods/console-operator-6bbf6cd699-cp4p2/log?container=console-operator&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:17.505269387Z I1117 09:39:17.505235       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for conversion-webhook-server container console-operator-6bbf6cd699-cp4p2 pod in namespace openshift-console-operator (previous: false).
2025-11-17T09:39:17.625769835Z I1117 09:39:17.625713       1 request.go:628] Waited for 192.720074ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/serviceaccounts?limit=1000
2025-11-17T09:39:17.673857951Z I1117 09:39:17.673805       1 request.go:628] Waited for 197.398966ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-kube-storage-version-migrator-operator/installplans?limit=500
2025-11-17T09:39:17.698405154Z I1117 09:39:17.698364       1 request.go:628] Waited for 192.95329ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console-operator/pods/console-operator-6bbf6cd699-cp4p2/log?container=conversion-webhook-server&limitBytes=87381&tailLines=100&timestamps=true
2025-11-17T09:39:17.707087710Z I1117 09:39:17.706112       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for console container console-595d4b756b-f6fvd pod in namespace openshift-console (previous: false).
2025-11-17T09:39:17.826040704Z I1117 09:39:17.825981       1 request.go:628] Waited for 196.227536ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts?limit=1000
2025-11-17T09:39:17.873409833Z I1117 09:39:17.873367       1 request.go:628] Waited for 196.902704ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-machine-api/installplans?limit=500
2025-11-17T09:39:17.898770976Z I1117 09:39:17.898726       1 request.go:628] Waited for 192.480858ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console/pods/console-595d4b756b-f6fvd/log?container=console&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:17.901654042Z I1117 09:39:17.901622       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:17.901654042Z I1117 09:39:17.901641       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for console container console-595d4b756b-xlhtg pod in namespace openshift-console (previous: false).
2025-11-17T09:39:18.026396102Z I1117 09:39:18.026347       1 request.go:628] Waited for 197.050479ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler-operator/serviceaccounts?limit=1000
2025-11-17T09:39:18.073758474Z I1117 09:39:18.073708       1 request.go:628] Waited for 197.037005ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-machine-config-operator/installplans?limit=500
2025-11-17T09:39:18.098958030Z I1117 09:39:18.098910       1 request.go:628] Waited for 197.169047ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console/pods/console-595d4b756b-xlhtg/log?container=console&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:18.100911738Z I1117 09:39:18.100876       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:18.100911738Z I1117 09:39:18.100895       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for console container console-6876ff4cb4-hqnh8 pod in namespace openshift-console (previous: false).
2025-11-17T09:39:18.225475072Z I1117 09:39:18.225429       1 request.go:628] Waited for 194.723282ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-storage-version-migrator/serviceaccounts?limit=1000
2025-11-17T09:39:18.273399014Z I1117 09:39:18.273282       1 request.go:628] Waited for 196.446645ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-marketplace/installplans?limit=500
2025-11-17T09:39:18.298699641Z I1117 09:39:18.298619       1 request.go:628] Waited for 197.585814ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console/pods/console-6876ff4cb4-hqnh8/log?container=console&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:18.304002065Z I1117 09:39:18.303977       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for console container console-7959f94788-4rchz pod in namespace openshift-console (previous: false).
2025-11-17T09:39:18.426500683Z I1117 09:39:18.426452       1 request.go:628] Waited for 198.288304ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-storage-version-migrator-operator/serviceaccounts?limit=1000
2025-11-17T09:39:18.473312398Z I1117 09:39:18.473260       1 request.go:628] Waited for 196.095469ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-monitoring/installplans?limit=500
2025-11-17T09:39:18.499120643Z I1117 09:39:18.499064       1 request.go:628] Waited for 194.916762ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console/pods/console-7959f94788-4rchz/log?container=console&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:18.505238068Z I1117 09:39:18.505194       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for console container console-7959f94788-8mbqp pod in namespace openshift-console (previous: false).
2025-11-17T09:39:18.625808315Z I1117 09:39:18.625748       1 request.go:628] Waited for 196.683211ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-machine-api/serviceaccounts?limit=1000
2025-11-17T09:39:18.673139534Z I1117 09:39:18.672889       1 request.go:628] Waited for 196.86709ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-multus/installplans?limit=500
2025-11-17T09:39:18.699399056Z I1117 09:39:18.699270       1 request.go:628] Waited for 193.968965ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-console/pods/console-7959f94788-8mbqp/log?container=console&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:19.292108693Z I1117 09:39:19.292046       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for download-server container downloads-7dcf8dd5c4-qchwz pod in namespace openshift-console (previous: false).
2025-11-17T09:39:19.943504726Z I1117 09:39:19.943450       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:19.943504726Z I1117 09:39:19.943479       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for download-server container downloads-7dcf8dd5c4-tcfsl pod in namespace openshift-console (previous: false).
2025-11-17T09:39:20.211460076Z I1117 09:39:20.211416       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:20.211531597Z I1117 09:39:20.211520       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for init-config-reloader container alertmanager-main-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:20.252471658Z I1117 09:39:20.252339       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for alertmanager container alertmanager-main-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:20.259502970Z I1117 09:39:20.259446       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-0 pod in namespace openshift-monitoring for failing operator alertmanager (previous: false): "container \"alertmanager\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.259502970Z I1117 09:39:20.259471       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"alertmanager\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.259502970Z I1117 09:39:20.259481       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for config-reloader container alertmanager-main-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:20.270663919Z I1117 09:39:20.270605       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-0 pod in namespace openshift-monitoring for failing operator config-reloader (previous: false): "container \"config-reloader\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.270663919Z I1117 09:39:20.270637       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"config-reloader\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.270663919Z I1117 09:39:20.270649       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for alertmanager-proxy container alertmanager-main-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:20.277316574Z I1117 09:39:20.277264       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-0 pod in namespace openshift-monitoring for failing operator alertmanager-proxy (previous: false): "container \"alertmanager-proxy\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.277316574Z I1117 09:39:20.277300       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"alertmanager-proxy\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.277316574Z I1117 09:39:20.277309       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container alertmanager-main-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:20.284605304Z I1117 09:39:20.284563       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-0 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy (previous: false): "container \"kube-rbac-proxy\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.284605304Z I1117 09:39:20.284584       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.284605304Z I1117 09:39:20.284593       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-metric container alertmanager-main-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:20.303766554Z I1117 09:39:20.303724       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-0 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy-metric (previous: false): "container \"kube-rbac-proxy-metric\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.303823542Z I1117 09:39:20.303812       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy-metric\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.303845071Z I1117 09:39:20.303837       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prom-label-proxy container alertmanager-main-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:20.425981706Z I1117 09:39:20.425930       1 request.go:628] Waited for 164.097269ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-nutanix-infra/serviceaccounts?limit=1000
2025-11-17T09:39:20.473138045Z I1117 09:39:20.473090       1 request.go:628] Waited for 196.312399ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-operators/installplans?limit=500
2025-11-17T09:39:20.499148941Z I1117 09:39:20.499092       1 request.go:628] Waited for 195.143858ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/alertmanager-main-0/log?container=prom-label-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:20.505690975Z I1117 09:39:20.505632       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-0 pod in namespace openshift-monitoring for failing operator prom-label-proxy (previous: false): "container \"prom-label-proxy\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.505690975Z I1117 09:39:20.505658       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"prom-label-proxy\" in pod \"alertmanager-main-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.505690975Z I1117 09:39:20.505677       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for init-config-reloader container alertmanager-main-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:20.625916547Z I1117 09:39:20.625873       1 request.go:628] Waited for 196.81033ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-oauth-apiserver/serviceaccounts?limit=1000
2025-11-17T09:39:20.673436309Z I1117 09:39:20.673381       1 request.go:628] Waited for 197.273839ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-ovirt-infra/installplans?limit=500
2025-11-17T09:39:20.698971179Z I1117 09:39:20.698922       1 request.go:628] Waited for 193.141551ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/alertmanager-main-1/log?container=init-config-reloader&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:20.707662603Z I1117 09:39:20.707619       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for alertmanager container alertmanager-main-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:20.826493944Z I1117 09:39:20.825496       1 request.go:628] Waited for 197.110068ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-openstack-infra/serviceaccounts?limit=1000
2025-11-17T09:39:20.873845269Z I1117 09:39:20.873798       1 request.go:628] Waited for 195.894272ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-ovn-kubernetes/installplans?limit=500
2025-11-17T09:39:20.898628825Z I1117 09:39:20.898583       1 request.go:628] Waited for 190.751233ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/alertmanager-main-1/log?container=alertmanager&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:20.903989731Z I1117 09:39:20.903939       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-1 pod in namespace openshift-monitoring for failing operator alertmanager (previous: false): "container \"alertmanager\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.904065251Z I1117 09:39:20.904050       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"alertmanager\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:20.904094530Z I1117 09:39:20.904085       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for config-reloader container alertmanager-main-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:21.025532710Z I1117 09:39:21.025465       1 request.go:628] Waited for 196.645178ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-operator-lifecycle-manager/serviceaccounts?limit=1000
2025-11-17T09:39:21.073498190Z I1117 09:39:21.073445       1 request.go:628] Waited for 196.892613ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-route-controller-manager/installplans?limit=500
2025-11-17T09:39:21.098586374Z I1117 09:39:21.098518       1 request.go:628] Waited for 194.309284ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/alertmanager-main-1/log?container=config-reloader&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:21.103963517Z I1117 09:39:21.103913       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-1 pod in namespace openshift-monitoring for failing operator config-reloader (previous: false): "container \"config-reloader\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.103963517Z I1117 09:39:21.103935       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"config-reloader\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.103963517Z I1117 09:39:21.103945       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for alertmanager-proxy container alertmanager-main-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:21.226148587Z I1117 09:39:21.226104       1 request.go:628] Waited for 196.63609ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-operators/serviceaccounts?limit=1000
2025-11-17T09:39:21.272941646Z I1117 09:39:21.272888       1 request.go:628] Waited for 193.380688ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-service-ca/installplans?limit=500
2025-11-17T09:39:21.298612829Z I1117 09:39:21.298561       1 request.go:628] Waited for 194.522458ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/alertmanager-main-1/log?container=alertmanager-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:21.308990516Z I1117 09:39:21.308943       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-1 pod in namespace openshift-monitoring for failing operator alertmanager-proxy (previous: false): "container \"alertmanager-proxy\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.309055942Z I1117 09:39:21.309041       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"alertmanager-proxy\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.309077973Z I1117 09:39:21.309070       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container alertmanager-main-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:21.426388852Z I1117 09:39:21.426280       1 request.go:628] Waited for 192.285736ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ovirt-infra/serviceaccounts?limit=1000
2025-11-17T09:39:21.473838058Z I1117 09:39:21.473792       1 request.go:628] Waited for 196.557703ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-service-ca-operator/installplans?limit=500
2025-11-17T09:39:21.499364916Z I1117 09:39:21.499197       1 request.go:628] Waited for 190.006714ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/alertmanager-main-1/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:21.508332596Z I1117 09:39:21.506740       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-1 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy (previous: false): "container \"kube-rbac-proxy\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.508332596Z I1117 09:39:21.506765       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.508332596Z I1117 09:39:21.506774       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-metric container alertmanager-main-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:21.626146897Z I1117 09:39:21.626059       1 request.go:628] Waited for 196.431156ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-ovn-kubernetes/serviceaccounts?limit=1000
2025-11-17T09:39:21.673688614Z I1117 09:39:21.673543       1 request.go:628] Waited for 197.335817ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-user-workload-monitoring/installplans?limit=500
2025-11-17T09:39:21.699363209Z I1117 09:39:21.699265       1 request.go:628] Waited for 192.40548ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/alertmanager-main-1/log?container=kube-rbac-proxy-metric&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:21.704826152Z I1117 09:39:21.704780       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-1 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy-metric (previous: false): "container \"kube-rbac-proxy-metric\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.704826152Z I1117 09:39:21.704800       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy-metric\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.704826152Z I1117 09:39:21.704810       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prom-label-proxy container alertmanager-main-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:21.826249400Z I1117 09:39:21.826177       1 request.go:628] Waited for 196.610512ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-route-controller-manager/serviceaccounts?limit=1000
2025-11-17T09:39:21.872906599Z I1117 09:39:21.872842       1 request.go:628] Waited for 196.599826ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1/namespaces/openshift-vsphere-infra/installplans?limit=500
2025-11-17T09:39:21.875691115Z I1117 09:39:21.875645       1 tasks_processing.go:74] XXXXXX 28 stopped.
2025-11-17T09:39:21.875727923Z I1117 09:39:21.875690       1 recorder.go:70] Recording config/installplans with fingerprint=7b887df561a3a9e6ef0dc672845aa5d56e348505006b7496d3a2f83892b0c95b
2025-11-17T09:39:21.875727923Z I1117 09:39:21.875702       1 gather.go:183] gatherer "clusterconfig" function "install_plans" took 10.853527787s to process 1 records
2025-11-17T09:39:21.898737661Z I1117 09:39:21.898699       1 request.go:628] Waited for 193.818321ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/alertmanager-main-1/log?container=prom-label-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:21.904054759Z I1117 09:39:21.903998       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for alertmanager-main-1 pod in namespace openshift-monitoring for failing operator prom-label-proxy (previous: false): "container \"prom-label-proxy\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.904054759Z I1117 09:39:21.904028       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"prom-label-proxy\" in pod \"alertmanager-main-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:21.904054759Z I1117 09:39:21.904044       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for cluster-monitoring-operator container cluster-monitoring-operator-694f894778-95kgk pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:22.025627780Z I1117 09:39:22.025512       1 request.go:628] Waited for 196.678819ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-service-ca/serviceaccounts?limit=1000
2025-11-17T09:39:22.099408589Z I1117 09:39:22.098923       1 request.go:628] Waited for 194.80144ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/cluster-monitoring-operator-694f894778-95kgk/log?container=cluster-monitoring-operator&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:22.109474187Z I1117 09:39:22.109173       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-state-metrics container kube-state-metrics-688f695dcc-5xxjr pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:22.226411875Z I1117 09:39:22.226368       1 request.go:628] Waited for 197.585836ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-service-ca-operator/serviceaccounts?limit=1000
2025-11-17T09:39:22.298942915Z I1117 09:39:22.298890       1 request.go:628] Waited for 189.588197ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/kube-state-metrics-688f695dcc-5xxjr/log?container=kube-state-metrics&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:22.304847633Z I1117 09:39:22.304804       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-main container kube-state-metrics-688f695dcc-5xxjr pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:22.426091840Z I1117 09:39:22.426042       1 request.go:628] Waited for 196.466365ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-user-workload-monitoring/serviceaccounts?limit=1000
2025-11-17T09:39:22.499386822Z I1117 09:39:22.499330       1 request.go:628] Waited for 194.327173ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/kube-state-metrics-688f695dcc-5xxjr/log?container=kube-rbac-proxy-main&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:22.506055077Z I1117 09:39:22.505984       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-self container kube-state-metrics-688f695dcc-5xxjr pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:22.625479268Z I1117 09:39:22.625422       1 request.go:628] Waited for 196.15358ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-vsphere-infra/serviceaccounts?limit=1000
2025-11-17T09:39:22.628559393Z I1117 09:39:22.628529       1 tasks_processing.go:74] XXXXXX 40 stopped.
2025-11-17T09:39:22.628962120Z I1117 09:39:22.628932       1 recorder.go:70] Recording config/serviceaccounts with fingerprint=7bf5713ecb900443171aaab4ca99c2858961e7208ea2b097a383ad70c379fe61
2025-11-17T09:39:22.628997194Z I1117 09:39:22.628987       1 gather.go:183] gatherer "clusterconfig" function "service_accounts" took 11.604153143s to process 1 records
2025-11-17T09:39:22.698855551Z I1117 09:39:22.698799       1 request.go:628] Waited for 192.610348ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/kube-state-metrics-688f695dcc-5xxjr/log?container=kube-rbac-proxy-self&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:22.704586526Z I1117 09:39:22.704563       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for monitoring-plugin container monitoring-plugin-7b5d6d66c-k6f6m pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:22.898462006Z I1117 09:39:22.898412       1 request.go:628] Waited for 193.706336ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/monitoring-plugin-7b5d6d66c-k6f6m/log?container=monitoring-plugin&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:22.908135130Z I1117 09:39:22.908084       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for monitoring-plugin container monitoring-plugin-7b5d6d66c-mxrkd pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:23.098876197Z I1117 09:39:23.098826       1 request.go:628] Waited for 190.553515ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/monitoring-plugin-7b5d6d66c-mxrkd/log?container=monitoring-plugin&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:23.106908218Z I1117 09:39:23.106863       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for init-textfile container node-exporter-2xsjl pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:23.299466630Z I1117 09:39:23.299409       1 request.go:628] Waited for 192.423194ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-2xsjl/log?container=init-textfile&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:23.304731048Z I1117 09:39:23.304690       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:23.304731048Z I1117 09:39:23.304709       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for node-exporter container node-exporter-2xsjl pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:23.499025398Z I1117 09:39:23.498946       1 request.go:628] Waited for 194.134429ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-2xsjl/log?container=node-exporter&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:23.505663046Z I1117 09:39:23.505616       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container node-exporter-2xsjl pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:23.698809007Z I1117 09:39:23.698755       1 request.go:628] Waited for 193.032776ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-2xsjl/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:23.704705825Z I1117 09:39:23.704670       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for init-textfile container node-exporter-7sftb pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:23.899345741Z I1117 09:39:23.899273       1 request.go:628] Waited for 194.487503ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-7sftb/log?container=init-textfile&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:23.905848492Z I1117 09:39:23.905780       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:23.905848492Z I1117 09:39:23.905802       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for node-exporter container node-exporter-7sftb pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:24.098870583Z I1117 09:39:24.098822       1 request.go:628] Waited for 192.931996ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-7sftb/log?container=node-exporter&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:24.105814650Z I1117 09:39:24.105777       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container node-exporter-7sftb pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:24.298752681Z I1117 09:39:24.298704       1 request.go:628] Waited for 192.749273ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-7sftb/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:24.304981751Z I1117 09:39:24.304930       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for init-textfile container node-exporter-kwg8s pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:24.498476918Z I1117 09:39:24.498419       1 request.go:628] Waited for 193.349878ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-kwg8s/log?container=init-textfile&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:24.503869634Z I1117 09:39:24.503843       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:24.503910303Z I1117 09:39:24.503901       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for node-exporter container node-exporter-kwg8s pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:24.699329305Z I1117 09:39:24.699244       1 request.go:628] Waited for 195.199276ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-kwg8s/log?container=node-exporter&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:24.706181986Z I1117 09:39:24.706137       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container node-exporter-kwg8s pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:24.898720667Z I1117 09:39:24.898652       1 request.go:628] Waited for 192.322256ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-kwg8s/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:24.907194550Z I1117 09:39:24.907157       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for init-textfile container node-exporter-mbvln pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:25.098913947Z I1117 09:39:25.098856       1 request.go:628] Waited for 191.50516ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-mbvln/log?container=init-textfile&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:25.104787458Z I1117 09:39:25.104750       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:25.104840385Z I1117 09:39:25.104830       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for node-exporter container node-exporter-mbvln pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:25.299112477Z I1117 09:39:25.299064       1 request.go:628] Waited for 194.101653ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-mbvln/log?container=node-exporter&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:25.305729707Z I1117 09:39:25.305697       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container node-exporter-mbvln pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:25.499439954Z I1117 09:39:25.499353       1 request.go:628] Waited for 193.487918ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/node-exporter-mbvln/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:26.020118160Z I1117 09:39:26.020073       1 insightsuploader.go:129] Nothing to report since 0001-01-01T00:00:00Z
2025-11-17T09:39:26.025914203Z I1117 09:39:26.025864       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for init-textfile container node-exporter-x4xx4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:26.073693598Z I1117 09:39:26.073606       1 gather_cluster_operator_pods_and_events.go:276] Error: "log buffer is empty"
2025-11-17T09:39:26.073693598Z I1117 09:39:26.073628       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for node-exporter container node-exporter-x4xx4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:26.081357898Z I1117 09:39:26.081264       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container node-exporter-x4xx4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:26.105236523Z I1117 09:39:26.105175       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-main container openshift-state-metrics-fb74d4bc4-q2ssp pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:26.298729375Z I1117 09:39:26.298677       1 request.go:628] Waited for 193.364116ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/openshift-state-metrics-fb74d4bc4-q2ssp/log?container=kube-rbac-proxy-main&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:26.303896389Z I1117 09:39:26.303858       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-self container openshift-state-metrics-fb74d4bc4-q2ssp pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:26.499372529Z I1117 09:39:26.499297       1 request.go:628] Waited for 195.330775ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/openshift-state-metrics-fb74d4bc4-q2ssp/log?container=kube-rbac-proxy-self&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:26.505864610Z I1117 09:39:26.505830       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for openshift-state-metrics container openshift-state-metrics-fb74d4bc4-q2ssp pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:26.698513491Z I1117 09:39:26.698459       1 request.go:628] Waited for 192.529025ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/openshift-state-metrics-fb74d4bc4-q2ssp/log?container=openshift-state-metrics&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:26.704193872Z I1117 09:39:26.704155       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prometheus-adapter container prometheus-adapter-5d757bbd4b-hr9ql pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:26.899145174Z I1117 09:39:26.899081       1 request.go:628] Waited for 194.777722ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-adapter-5d757bbd4b-hr9ql/log?container=prometheus-adapter&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:26.905200889Z I1117 09:39:26.905138       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prometheus-adapter container prometheus-adapter-5d757bbd4b-rm9nc pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:27.098939683Z I1117 09:39:27.098883       1 request.go:628] Waited for 193.630085ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-adapter-5d757bbd4b-rm9nc/log?container=prometheus-adapter&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:27.104135441Z I1117 09:39:27.104103       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for init-config-reloader container prometheus-k8s-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:27.298671986Z I1117 09:39:27.298621       1 request.go:628] Waited for 194.403099ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-0/log?container=init-config-reloader&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:27.304412566Z I1117 09:39:27.304363       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prometheus container prometheus-k8s-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:27.498909615Z I1117 09:39:27.498852       1 request.go:628] Waited for 194.362307ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-0/log?container=prometheus&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:27.504372540Z I1117 09:39:27.504242       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-0 pod in namespace openshift-monitoring for failing operator prometheus (previous: false): "container \"prometheus\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:27.504372540Z I1117 09:39:27.504265       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"prometheus\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:27.504372540Z I1117 09:39:27.504273       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for config-reloader container prometheus-k8s-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:27.698889340Z I1117 09:39:27.698840       1 request.go:628] Waited for 194.470743ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-0/log?container=config-reloader&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:27.704428539Z I1117 09:39:27.704354       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-0 pod in namespace openshift-monitoring for failing operator config-reloader (previous: false): "container \"config-reloader\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:27.704428539Z I1117 09:39:27.704374       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"config-reloader\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:27.704428539Z I1117 09:39:27.704382       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for thanos-sidecar container prometheus-k8s-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:27.898614275Z I1117 09:39:27.898557       1 request.go:628] Waited for 194.085983ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-0/log?container=thanos-sidecar&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:28.056971160Z I1117 09:39:28.056906       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-0 pod in namespace openshift-monitoring for failing operator thanos-sidecar (previous: false): "container \"thanos-sidecar\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.056971160Z I1117 09:39:28.056932       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"thanos-sidecar\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.056971160Z I1117 09:39:28.056940       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prometheus-proxy container prometheus-k8s-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:28.104747737Z I1117 09:39:28.104604       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-0 pod in namespace openshift-monitoring for failing operator prometheus-proxy (previous: false): "container \"prometheus-proxy\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.104747737Z I1117 09:39:28.104628       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"prometheus-proxy\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.104747737Z I1117 09:39:28.104636       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container prometheus-k8s-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:28.299121223Z I1117 09:39:28.299056       1 request.go:628] Waited for 194.318701ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-0/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:28.304524394Z I1117 09:39:28.304467       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-0 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy (previous: false): "container \"kube-rbac-proxy\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.304524394Z I1117 09:39:28.304497       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.304524394Z I1117 09:39:28.304505       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-thanos container prometheus-k8s-0 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:28.498991488Z I1117 09:39:28.498925       1 request.go:628] Waited for 194.326279ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-0/log?container=kube-rbac-proxy-thanos&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:28.504084717Z I1117 09:39:28.504027       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-0 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy-thanos (previous: false): "container \"kube-rbac-proxy-thanos\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.504084717Z I1117 09:39:28.504049       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy-thanos\" in pod \"prometheus-k8s-0\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.504084717Z I1117 09:39:28.504063       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for init-config-reloader container prometheus-k8s-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:28.699029680Z I1117 09:39:28.698946       1 request.go:628] Waited for 194.7888ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-1/log?container=init-config-reloader&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:28.704154874Z I1117 09:39:28.704087       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prometheus container prometheus-k8s-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:28.898748435Z I1117 09:39:28.898696       1 request.go:628] Waited for 194.492574ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-1/log?container=prometheus&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:28.904070962Z I1117 09:39:28.904022       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-1 pod in namespace openshift-monitoring for failing operator prometheus (previous: false): "container \"prometheus\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.904070962Z I1117 09:39:28.904044       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"prometheus\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:28.904070962Z I1117 09:39:28.904052       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for config-reloader container prometheus-k8s-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:29.098838385Z I1117 09:39:29.098767       1 request.go:628] Waited for 194.610071ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-1/log?container=config-reloader&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:29.104698342Z I1117 09:39:29.104647       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-1 pod in namespace openshift-monitoring for failing operator config-reloader (previous: false): "container \"config-reloader\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:29.104698342Z I1117 09:39:29.104672       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"config-reloader\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:29.104698342Z I1117 09:39:29.104682       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for thanos-sidecar container prometheus-k8s-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:29.298753660Z I1117 09:39:29.298673       1 request.go:628] Waited for 193.901323ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-1/log?container=thanos-sidecar&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:29.306609200Z I1117 09:39:29.306515       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-1 pod in namespace openshift-monitoring for failing operator thanos-sidecar (previous: false): "container \"thanos-sidecar\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:29.306609200Z I1117 09:39:29.306543       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"thanos-sidecar\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:29.306609200Z I1117 09:39:29.306558       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prometheus-proxy container prometheus-k8s-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:29.499155719Z I1117 09:39:29.499092       1 request.go:628] Waited for 192.435298ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-1/log?container=prometheus-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:29.504471350Z I1117 09:39:29.504329       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-1 pod in namespace openshift-monitoring for failing operator prometheus-proxy (previous: false): "container \"prometheus-proxy\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:29.504471350Z I1117 09:39:29.504351       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"prometheus-proxy\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:29.504471350Z I1117 09:39:29.504361       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container prometheus-k8s-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:29.698985597Z I1117 09:39:29.698922       1 request.go:628] Waited for 194.483775ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-1/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:29.704085749Z I1117 09:39:29.704044       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for prometheus-k8s-1 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy (previous: false): "container \"kube-rbac-proxy\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:29.704085749Z I1117 09:39:29.704066       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy\" in pod \"prometheus-k8s-1\" is waiting to start: PodInitializing"
2025-11-17T09:39:29.704085749Z I1117 09:39:29.704075       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-thanos container prometheus-k8s-1 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:29.898855036Z I1117 09:39:29.898802       1 request.go:628] Waited for 194.636334ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-k8s-1/log?container=kube-rbac-proxy-thanos&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:29.904986003Z I1117 09:39:29.904949       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prometheus-operator-admission-webhook container prometheus-operator-admission-webhook-686c664ffb-qdprf pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:30.098871874Z I1117 09:39:30.098828       1 request.go:628] Waited for 193.706532ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-operator-admission-webhook-686c664ffb-qdprf/log?container=prometheus-operator-admission-webhook&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:30.104820904Z I1117 09:39:30.104786       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prometheus-operator-admission-webhook container prometheus-operator-admission-webhook-686c664ffb-xlz8g pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:30.298397153Z I1117 09:39:30.298358       1 request.go:628] Waited for 193.414386ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-operator-admission-webhook-686c664ffb-xlz8g/log?container=prometheus-operator-admission-webhook&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:30.303682394Z I1117 09:39:30.303660       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prometheus-operator container prometheus-operator-f9ccfd6c9-ht5ln pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:30.499267884Z I1117 09:39:30.499231       1 request.go:628] Waited for 195.435555ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-operator-f9ccfd6c9-ht5ln/log?container=prometheus-operator&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:30.504970068Z I1117 09:39:30.504945       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container prometheus-operator-f9ccfd6c9-ht5ln pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:30.698627777Z I1117 09:39:30.698587       1 request.go:628] Waited for 193.497173ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/prometheus-operator-f9ccfd6c9-ht5ln/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:30.704136974Z I1117 09:39:30.704106       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for telemeter-client container telemeter-client-7bb98fb6bc-ptrbd pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:30.899229661Z I1117 09:39:30.899178       1 request.go:628] Waited for 194.913223ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/telemeter-client-7bb98fb6bc-ptrbd/log?container=telemeter-client&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:30.908356453Z I1117 09:39:30.908254       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for reload container telemeter-client-7bb98fb6bc-ptrbd pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:31.099521265Z I1117 09:39:31.099473       1 request.go:628] Waited for 190.953201ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/telemeter-client-7bb98fb6bc-ptrbd/log?container=reload&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:31.110443151Z I1117 09:39:31.110404       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container telemeter-client-7bb98fb6bc-ptrbd pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:31.299044366Z I1117 09:39:31.298999       1 request.go:628] Waited for 188.418766ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/telemeter-client-7bb98fb6bc-ptrbd/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:31.304780011Z I1117 09:39:31.304756       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for thanos-query container thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:31.499347737Z I1117 09:39:31.499306       1 request.go:628] Waited for 194.417507ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-7dzg4/log?container=thanos-query&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:31.505808442Z I1117 09:39:31.505775       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring for failing operator thanos-query (previous: false): "container \"thanos-query\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:31.505853364Z I1117 09:39:31.505842       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"thanos-query\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:31.505878334Z I1117 09:39:31.505871       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-proxy container thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:31.699361538Z I1117 09:39:31.699314       1 request.go:628] Waited for 193.325429ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-7dzg4/log?container=oauth-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:31.703781347Z I1117 09:39:31.703742       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring for failing operator oauth-proxy (previous: false): "container \"oauth-proxy\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:31.703781347Z I1117 09:39:31.703764       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"oauth-proxy\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:31.703781347Z I1117 09:39:31.703772       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:31.899421831Z I1117 09:39:31.899368       1 request.go:628] Waited for 195.499575ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-7dzg4/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:31.903900916Z I1117 09:39:31.903869       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy (previous: false): "container \"kube-rbac-proxy\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:31.903942319Z I1117 09:39:31.903932       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:31.903965598Z I1117 09:39:31.903958       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prom-label-proxy container thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:32.098698232Z I1117 09:39:32.098647       1 request.go:628] Waited for 194.584014ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-7dzg4/log?container=prom-label-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:32.103537211Z I1117 09:39:32.103507       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring for failing operator prom-label-proxy (previous: false): "container \"prom-label-proxy\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.103537211Z I1117 09:39:32.103531       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"prom-label-proxy\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.103554485Z I1117 09:39:32.103540       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-rules container thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:32.299073797Z I1117 09:39:32.299027       1 request.go:628] Waited for 195.365949ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-7dzg4/log?container=kube-rbac-proxy-rules&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:32.304278311Z I1117 09:39:32.304184       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy-rules (previous: false): "container \"kube-rbac-proxy-rules\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.304278311Z I1117 09:39:32.304206       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy-rules\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.304278311Z I1117 09:39:32.304215       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-metrics container thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:32.498700568Z I1117 09:39:32.498631       1 request.go:628] Waited for 194.332229ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-7dzg4/log?container=kube-rbac-proxy-metrics&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:32.503315414Z I1117 09:39:32.503264       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-7dzg4 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy-metrics (previous: false): "container \"kube-rbac-proxy-metrics\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.503315414Z I1117 09:39:32.503300       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy-metrics\" in pod \"thanos-querier-95bdbdd77-7dzg4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.503343390Z I1117 09:39:32.503318       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for thanos-query container thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:32.698627507Z I1117 09:39:32.698576       1 request.go:628] Waited for 195.144587ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-wnqt4/log?container=thanos-query&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:32.703688557Z I1117 09:39:32.703634       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring for failing operator thanos-query (previous: false): "container \"thanos-query\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.703688557Z I1117 09:39:32.703653       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"thanos-query\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.703688557Z I1117 09:39:32.703664       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for oauth-proxy container thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:32.899549779Z I1117 09:39:32.899451       1 request.go:628] Waited for 195.624711ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-wnqt4/log?container=oauth-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:32.904240898Z I1117 09:39:32.904200       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring for failing operator oauth-proxy (previous: false): "container \"oauth-proxy\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.904240898Z I1117 09:39:32.904220       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"oauth-proxy\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:32.904240898Z I1117 09:39:32.904228       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy container thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:33.098965680Z I1117 09:39:33.098903       1 request.go:628] Waited for 194.576344ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-wnqt4/log?container=kube-rbac-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:33.104172306Z I1117 09:39:33.104135       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy (previous: false): "container \"kube-rbac-proxy\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:33.104172306Z I1117 09:39:33.104153       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:33.104172306Z I1117 09:39:33.104161       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for prom-label-proxy container thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:33.298513462Z I1117 09:39:33.298458       1 request.go:628] Waited for 194.201029ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-wnqt4/log?container=prom-label-proxy&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:33.303781445Z I1117 09:39:33.303733       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring for failing operator prom-label-proxy (previous: false): "container \"prom-label-proxy\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:33.303781445Z I1117 09:39:33.303757       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"prom-label-proxy\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:33.303781445Z I1117 09:39:33.303766       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-rules container thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:33.499292840Z I1117 09:39:33.499231       1 request.go:628] Waited for 195.350411ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-wnqt4/log?container=kube-rbac-proxy-rules&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:33.504812134Z I1117 09:39:33.504769       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy-rules (previous: false): "container \"kube-rbac-proxy-rules\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:33.504812134Z I1117 09:39:33.504790       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy-rules\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:33.504812134Z I1117 09:39:33.504799       1 gather_cluster_operator_pods_and_events.go:361] Fetching logs for kube-rbac-proxy-metrics container thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring (previous: false).
2025-11-17T09:39:33.699738469Z I1117 09:39:33.699672       1 request.go:628] Waited for 194.778123ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-95bdbdd77-wnqt4/log?container=kube-rbac-proxy-metrics&limitBytes=180224&tailLines=100&timestamps=true
2025-11-17T09:39:33.706900190Z I1117 09:39:33.706678       1 gather_cluster_operator_pods_and_events.go:404] Failed to fetch log for thanos-querier-95bdbdd77-wnqt4 pod in namespace openshift-monitoring for failing operator kube-rbac-proxy-metrics (previous: false): "container \"kube-rbac-proxy-metrics\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:33.706900190Z I1117 09:39:33.706715       1 gather_cluster_operator_pods_and_events.go:276] Error: "container \"kube-rbac-proxy-metrics\" in pod \"thanos-querier-95bdbdd77-wnqt4\" is waiting to start: ContainerCreating"
2025-11-17T09:39:33.706900190Z I1117 09:39:33.706739       1 tasks_processing.go:74] XXXXXX 41 stopped.
2025-11-17T09:39:33.706900190Z I1117 09:39:33.706799       1 recorder.go:70] Recording events/openshift-config-managed with fingerprint=501c9a82fe201fe7a9c869cb7be4a69a9bcefee927bb34b53960e83dd77a2f99
2025-11-17T09:39:33.706953789Z I1117 09:39:33.706916       1 recorder.go:70] Recording events/openshift-authentication with fingerprint=164a123ba54220f023b39fab37266951f4e20e43f4445e565b0e30d90dacefe0
2025-11-17T09:39:33.709465438Z I1117 09:39:33.709327       1 recorder.go:70] Recording events/openshift-authentication-operator with fingerprint=ed7f3acdfbb6d6c561b309359488628998b817d54f88817ee4ce5e3b357ebcfc
2025-11-17T09:39:33.709465438Z I1117 09:39:33.709377       1 recorder.go:70] Recording events/openshift-ingress with fingerprint=c19a4a40f00100a787692afaa40588073c8085bfb83b419026ce48790f34ecc6
2025-11-17T09:39:33.709542280Z I1117 09:39:33.709524       1 recorder.go:70] Recording events/openshift-oauth-apiserver with fingerprint=83346029fbae98e79021f93be5d16950265374aa8f4425607c35f5b7c9b2d7fb
2025-11-17T09:39:33.709822260Z I1117 09:39:33.709786       1 recorder.go:70] Recording events/openshift-console-operator with fingerprint=c72c576e5c30c1c7d9456a2c52c771119f09b6efc6f8d90a44e110c936a72451
2025-11-17T09:39:33.709950796Z I1117 09:39:33.709924       1 recorder.go:70] Recording events/openshift-console with fingerprint=18ffe56a923ce184a3044ea6cf61e73f05099d2192e9b855af3beb7aa179cd3c
2025-11-17T09:39:33.710310625Z I1117 09:39:33.710268       1 recorder.go:70] Recording events/openshift-monitoring with fingerprint=6c8e49c3a18b2f07d058b2fa2e63854e8bd6823848adc3426f0494bb457ea4ae
2025-11-17T09:39:33.710351457Z I1117 09:39:33.710332       1 recorder.go:70] Recording config/pod/openshift-authentication/logs/oauth-openshift-76f996bb9d-9lf69/oauth-openshift_current.log with fingerprint=9435b7795cd2d35d41f7cb15de0a4b5b61f7539ae62e69f9d059abe1251d1981
2025-11-17T09:39:33.710401519Z I1117 09:39:33.710379       1 recorder.go:70] Recording config/pod/openshift-authentication/logs/oauth-openshift-76f996bb9d-cdbwh/oauth-openshift_current.log with fingerprint=cc27ee8230f932e758d7f8966f6493bdac0e9ea850c6ae459aba3871b6841bc7
2025-11-17T09:39:33.710447052Z I1117 09:39:33.710431       1 recorder.go:70] Recording config/pod/openshift-authentication/logs/oauth-openshift-76f996bb9d-wbblj/oauth-openshift_current.log with fingerprint=b6f49e84d8aa460cb57a14493bd4e8f1f163a8249206fa7b4e74f3509c2ddc1f
2025-11-17T09:39:33.710635784Z I1117 09:39:33.710608       1 recorder.go:70] Recording config/pod/openshift-authentication-operator/authentication-operator-f4cdf7bdc-t2h4s with fingerprint=ca5ea859046fcdc17520ea3f23aba0b2396e116379eb4dc1d8202fbb199edd1b
2025-11-17T09:39:33.710751805Z I1117 09:39:33.710727       1 recorder.go:70] Recording config/pod/openshift-authentication-operator/logs/authentication-operator-f4cdf7bdc-t2h4s/authentication-operator_previous.log with fingerprint=cd7432a700e5f9eed2ef42dfb9da0c2b9e1fb95421234a6dd385dcd91dafdc6b
2025-11-17T09:39:33.711152519Z I1117 09:39:33.711111       1 recorder.go:70] Recording config/pod/openshift-authentication-operator/logs/authentication-operator-f4cdf7bdc-t2h4s/authentication-operator_current.log with fingerprint=37bb6b553b000080ed821af9aa79714eff67e9a5356c3be9e22b189e3e882471
2025-11-17T09:39:33.711152519Z I1117 09:39:33.711136       1 recorder.go:70] Recording config/pod/openshift-ingress/logs/router-default-668949fb46-52qxp/router_current.log with fingerprint=2bc68bb3da9e29fb1eb1c12be5a99cd3b686b9eb87dc864beb5bb4f73d10b845
2025-11-17T09:39:33.711152519Z I1117 09:39:33.711148       1 recorder.go:70] Recording config/pod/openshift-ingress/logs/router-default-668949fb46-d6qkr/router_current.log with fingerprint=5d053d6fa8843c8d4cd217b2f670240a23b8e7ca98619ca14ad0f4a51d6c7899
2025-11-17T09:39:33.711412896Z I1117 09:39:33.711348       1 recorder.go:70] Recording config/pod/openshift-oauth-apiserver/apiserver-795447c5f-5lfvc with fingerprint=b38b549366dc26e5f9df7a75afb2f646a5985be71c5e062e19af0e9eec1fec74
2025-11-17T09:39:33.711437495Z I1117 09:39:33.711417       1 recorder.go:70] Recording config/pod/openshift-oauth-apiserver/logs/apiserver-795447c5f-5lfvc/oauth-apiserver_previous.log with fingerprint=1a51694019a8e20f4cb1a7afe59a8a25537fd505559986ffe9c251deb9aea8bd
2025-11-17T09:39:33.711524537Z I1117 09:39:33.711503       1 recorder.go:70] Recording config/pod/openshift-oauth-apiserver/logs/apiserver-795447c5f-5lfvc/oauth-apiserver_current.log with fingerprint=6488cbc211ab2bb617d69e6e5f2515165b71e918fd83bae6ff97b3d6f3e0e1d3
2025-11-17T09:39:33.711635442Z I1117 09:39:33.711614       1 recorder.go:70] Recording config/pod/openshift-oauth-apiserver/apiserver-795447c5f-7wtt8 with fingerprint=2d0bfff171c4019bef0ef51062a775276faa9c3af9de5bc896886e776b9fa5dc
2025-11-17T09:39:33.711700222Z I1117 09:39:33.711679       1 recorder.go:70] Recording config/pod/openshift-oauth-apiserver/logs/apiserver-795447c5f-7wtt8/oauth-apiserver_previous.log with fingerprint=6324c86154010d8aeaeedc541bf36a68cac2f33471fcdb588838e7d2d8cd335d
2025-11-17T09:39:33.711799009Z I1117 09:39:33.711779       1 recorder.go:70] Recording config/pod/openshift-oauth-apiserver/logs/apiserver-795447c5f-7wtt8/oauth-apiserver_current.log with fingerprint=9103348d8c5cb131fe631425c1a7363bc91ded8394dac40a0e4af3f7ddd37954
2025-11-17T09:39:33.711895657Z I1117 09:39:33.711879       1 recorder.go:70] Recording config/pod/openshift-oauth-apiserver/logs/apiserver-795447c5f-c8cv2/oauth-apiserver_current.log with fingerprint=239d81e779f14a99724896bbf7f050a643e1eb160e1916249464d4c60b8b85f7
2025-11-17T09:39:33.712050275Z I1117 09:39:33.712028       1 recorder.go:70] Recording config/pod/openshift-console-operator/console-operator-6bbf6cd699-cp4p2 with fingerprint=5edf7645b2efc887a86b0c4382a93939e820c5752ec153401a7782816ef27fc4
2025-11-17T09:39:33.712216348Z I1117 09:39:33.712195       1 recorder.go:70] Recording config/pod/openshift-console-operator/logs/console-operator-6bbf6cd699-cp4p2/console-operator_previous.log with fingerprint=e22e1d8455dff59700d21dfaff6142387e1174ac8fa49721a652b8d8fd853011
2025-11-17T09:39:33.712422441Z I1117 09:39:33.712397       1 recorder.go:70] Recording config/pod/openshift-console-operator/logs/console-operator-6bbf6cd699-cp4p2/console-operator_current.log with fingerprint=f1b2e5addc96c885d43cee02403dea91fbd66b1d05cadf1a57b9aec96009cce0
2025-11-17T09:39:33.712926228Z I1117 09:39:33.712884       1 recorder.go:70] Recording config/pod/openshift-console-operator/logs/console-operator-6bbf6cd699-cp4p2/conversion-webhook-server_current.log with fingerprint=1b110b66ad0c144326d65907b3ca5cd0d7488463988252f9462d02654f334678
2025-11-17T09:39:33.712926228Z I1117 09:39:33.712905       1 recorder.go:70] Recording config/pod/openshift-console/logs/console-6876ff4cb4-hqnh8/console_current.log with fingerprint=2b276f36ed6473ca4404313419101db22d33ca1083a7165946ff68f0ea48a448
2025-11-17T09:39:33.712926228Z I1117 09:39:33.712919       1 recorder.go:70] Recording config/pod/openshift-console/logs/console-7959f94788-4rchz/console_current.log with fingerprint=7fce2122ce963ba5ee59a8b25c294a01dba1f384aea14b573d5eda08dfd9f749
2025-11-17T09:39:33.712960541Z I1117 09:39:33.712944       1 recorder.go:70] Recording config/pod/openshift-console/logs/console-7959f94788-8mbqp/console_current.log with fingerprint=108f875195dcdd75f4b511a50881fecaa265751b9d38102ce1b23a078ef9a478
2025-11-17T09:39:33.712969675Z I1117 09:39:33.712960       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/alertmanager-main-0/init-config-reloader_current.log with fingerprint=3a74ce7b088421b6ba3595e6dff25b38b82a5d97acbb9cf605c4ba87e6ca1de8
2025-11-17T09:39:33.712977751Z I1117 09:39:33.712972       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/alertmanager-main-1/init-config-reloader_current.log with fingerprint=a2eea40d7f64fc2172fc452025b5b3c07c09b02cfd535719163e5a2dee3a6537
2025-11-17T09:39:33.713120987Z I1117 09:39:33.713090       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/cluster-monitoring-operator-694f894778-95kgk/cluster-monitoring-operator_current.log with fingerprint=1ecc61a5695a00ab1355b2353bedefc1d5542e26d46fc043ab4d005e68eca956
2025-11-17T09:39:33.713181830Z I1117 09:39:33.713154       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/kube-state-metrics-688f695dcc-5xxjr/kube-state-metrics_current.log with fingerprint=3f9fce0c447365da14ec9653423c1bf3fcbc01461f5167563576c5e70b5648ea
2025-11-17T09:39:33.713191761Z I1117 09:39:33.713181       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/kube-state-metrics-688f695dcc-5xxjr/kube-rbac-proxy-main_current.log with fingerprint=89fe21180423def399fd346f72ebe5b8f57f185478fd17c861cb52d0e0ca8418
2025-11-17T09:39:33.713221830Z I1117 09:39:33.713203       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/kube-state-metrics-688f695dcc-5xxjr/kube-rbac-proxy-self_current.log with fingerprint=c58e79c86cd67b154b5d272f313d33ab9afda8584f287affc2546814e768d9ec
2025-11-17T09:39:33.713231128Z I1117 09:39:33.713223       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/monitoring-plugin-7b5d6d66c-k6f6m/monitoring-plugin_current.log with fingerprint=91e968d6c0984c4a8c8d3e74824fedcf2a02f0f37e1c58aa075dadabf7be6b66
2025-11-17T09:39:33.713254000Z I1117 09:39:33.713239       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/monitoring-plugin-7b5d6d66c-mxrkd/monitoring-plugin_current.log with fingerprint=69e4a33c7c574ad97c94c28d1b25c3a83730050494c8fa6f0f134ec8cfe4ac39
2025-11-17T09:39:33.713332404Z I1117 09:39:33.713305       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-2xsjl/node-exporter_current.log with fingerprint=b0ed8d6d627ef5759a7c06275315ce82f1112ba2a5ed80e6d80bd24ca49be169
2025-11-17T09:39:33.713349220Z I1117 09:39:33.713334       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-2xsjl/kube-rbac-proxy_current.log with fingerprint=2a1fa494b16b90d0cddee44835f1f160fb5d3371870537d2ddcc17069e079c13
2025-11-17T09:39:33.713407404Z I1117 09:39:33.713390       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-7sftb/node-exporter_current.log with fingerprint=a8798879a779d879cdfafcb4095f047fe422c7a13d5e6d715fac6685184c9020
2025-11-17T09:39:33.713421639Z I1117 09:39:33.713413       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-7sftb/kube-rbac-proxy_current.log with fingerprint=60877b2c4151fa7f2dc45012765a86daebde8d8f954f705473519b1c85b7000f
2025-11-17T09:39:33.713490485Z I1117 09:39:33.713472       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-kwg8s/node-exporter_current.log with fingerprint=6200f6030771ea338351b9a5a1294aad1b9161f29ca50d30735863945a19959b
2025-11-17T09:39:33.713514154Z I1117 09:39:33.713495       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-kwg8s/kube-rbac-proxy_current.log with fingerprint=02634d5d15e6382c7c6d84ce7f4745c408d9ee13b5e26902bc67a84e9270bfdf
2025-11-17T09:39:33.713574549Z I1117 09:39:33.713547       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-mbvln/node-exporter_current.log with fingerprint=d9a076171047018bb606a83aab9ec5e789f7df9f2477a7f75e9391a0ab3a4907
2025-11-17T09:39:33.713586538Z I1117 09:39:33.713581       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-mbvln/kube-rbac-proxy_current.log with fingerprint=d8e813edb0c395bddf60e1adb78f444f3f09ff17bad32839cd08124846552ad0
2025-11-17T09:39:33.713649187Z I1117 09:39:33.713631       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-x4xx4/node-exporter_current.log with fingerprint=3bde2cf98b43c2ca72e1d3bde0bc343ceef3d0cc853b3d872e797dc12eed09f6
2025-11-17T09:39:33.713660741Z I1117 09:39:33.713654       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/node-exporter-x4xx4/kube-rbac-proxy_current.log with fingerprint=eaaa502f6edde4483e57339e40a21852134d3c9f295704742ef266deefaa2a80
2025-11-17T09:39:33.713690907Z I1117 09:39:33.713673       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/openshift-state-metrics-fb74d4bc4-q2ssp/kube-rbac-proxy-main_current.log with fingerprint=fab9ddf5dfa8ce61c8ae0cac72e978693a7f76493f9da7179f90589a959c17ce
2025-11-17T09:39:33.713702078Z I1117 09:39:33.713695       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/openshift-state-metrics-fb74d4bc4-q2ssp/kube-rbac-proxy-self_current.log with fingerprint=7e3036f85c88f1daa890611e3d637821407d4897487726ff24f3b0ff2cfd3bb4
2025-11-17T09:39:33.713736461Z I1117 09:39:33.713718       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/openshift-state-metrics-fb74d4bc4-q2ssp/openshift-state-metrics_current.log with fingerprint=ce7e7033999fa6f59172b9a0a25c6c7b8ac797a7ef031cfaa636c9e182d802ec
2025-11-17T09:39:33.713745270Z I1117 09:39:33.713738       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/prometheus-adapter-5d757bbd4b-hr9ql/prometheus-adapter_current.log with fingerprint=ddd56d3db15bed67ee5b32150ac22c2a532be56b8f4ef0ac7e69c984a1f8c6da
2025-11-17T09:39:33.713770783Z I1117 09:39:33.713751       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/prometheus-adapter-5d757bbd4b-rm9nc/prometheus-adapter_current.log with fingerprint=6013674c10a263e9b3d1e1ff735f9164f54784deb11b9d05fe5c9a806a0a02ac
2025-11-17T09:39:33.713770783Z I1117 09:39:33.713765       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/prometheus-k8s-0/init-config-reloader_current.log with fingerprint=36189f9a99804ae76cc9bcfa8bea89a1a1677b05737bb465c7e23c6e383105a6
2025-11-17T09:39:33.713782360Z I1117 09:39:33.713775       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/prometheus-k8s-1/init-config-reloader_current.log with fingerprint=550102768461f11e44ee58918dc97647b3ece48d5def08dd8e4c7a900af16417
2025-11-17T09:39:33.713805390Z I1117 09:39:33.713790       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/prometheus-k8s-1/kube-rbac-proxy-thanos_current.log with fingerprint=5b7d557c833721fca4c61b85aefb95591b960aa0a8e34bacd70e8acf6846bc10
2025-11-17T09:39:33.713805390Z I1117 09:39:33.713800       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/prometheus-operator-admission-webhook-686c664ffb-qdprf/prometheus-operator-admission-webhook_current.log with fingerprint=313417babd011d9bc30ca3da4a23fd30cc51ee0ddfabcec3897f661e9e762cfc
2025-11-17T09:39:33.713819952Z I1117 09:39:33.713810       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/prometheus-operator-admission-webhook-686c664ffb-xlz8g/prometheus-operator-admission-webhook_current.log with fingerprint=90fbecbcb56d02cc1e99273a8b3edee0e6b7064f5b73aba159788df42931f958
2025-11-17T09:39:33.713888570Z I1117 09:39:33.713869       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/prometheus-operator-f9ccfd6c9-ht5ln/prometheus-operator_current.log with fingerprint=6403430e538e21dbbdfb2f970248f5c8e93b1095c95d4428c746cf0c7db7571e
2025-11-17T09:39:33.713897420Z I1117 09:39:33.713891       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/prometheus-operator-f9ccfd6c9-ht5ln/kube-rbac-proxy_current.log with fingerprint=3f598addf123cc5bd56a48cdc38d9de713e473bacb9f5daea047b327dd0faaa1
2025-11-17T09:39:33.714018352Z I1117 09:39:33.713985       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/telemeter-client-7bb98fb6bc-ptrbd/telemeter-client_current.log with fingerprint=a9348001cf9ec67d1dbaf63769e4ee714d065f0eec140a3e4f9e3e56a8fc2ec0
2025-11-17T09:39:33.714018352Z I1117 09:39:33.714009       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/telemeter-client-7bb98fb6bc-ptrbd/reload_current.log with fingerprint=7874ec8c04413425c13b6000571705fadc93db6e1ca93873d88da4a2cdd83fb0
2025-11-17T09:39:33.714028982Z I1117 09:39:33.714023       1 recorder.go:70] Recording config/pod/openshift-monitoring/logs/telemeter-client-7bb98fb6bc-ptrbd/kube-rbac-proxy_current.log with fingerprint=d9c3396d6753b7729140fc4b02796655135735f68e7e05d80677f1510ccd33b1
2025-11-17T09:39:33.714059491Z I1117 09:39:33.714034       1 gather.go:183] gatherer "clusterconfig" function "operators_pods_and_events" took 22.685501003s to process 63 records
2025-11-17T09:39:33.714153646Z E1117 09:39:33.714134       1 periodic.go:215] clusterconfig failed after 22.694s with: function "active_alerts" failed with an error, function "config_maps" failed with an error, function "metrics" failed with an error, function "pod_network_connectivity_checks" failed with an error, function "silenced_alerts" failed with an error, function "tsdb_status" failed with an error
2025-11-17T09:39:33.714185695Z I1117 09:39:33.714160       1 controllerstatus.go:86] name=periodic-clusterconfig healthy=false reason=PeriodicGatherFailed message=Source clusterconfig could not be retrieved: function "active_alerts" failed with an error, function "config_maps" failed with an error, function "metrics" failed with an error, function "pod_network_connectivity_checks" failed with an error, function "silenced_alerts" failed with an error, function "tsdb_status" failed with an error
2025-11-17T09:39:33.714185695Z I1117 09:39:33.714178       1 periodic.go:204] Running workloads gatherer
2025-11-17T09:39:33.714231367Z I1117 09:39:33.714213       1 tasks_processing.go:45] number of XXXXXXs: 2
2025-11-17T09:39:33.714240720Z I1117 09:39:33.714229       1 tasks_processing.go:69] XXXXXX 1 listening for tasks.
2025-11-17T09:39:33.714240720Z I1117 09:39:33.714235       1 tasks_processing.go:71] XXXXXX 1 working on helmchart_info task.
2025-11-17T09:39:33.714338042Z I1117 09:39:33.714308       1 tasks_processing.go:69] XXXXXX 0 listening for tasks.
2025-11-17T09:39:33.714389580Z I1117 09:39:33.714376       1 tasks_processing.go:71] XXXXXX 0 working on workload_info task.
2025-11-17T09:39:33.746094824Z I1117 09:39:33.746041       1 tasks_processing.go:74] XXXXXX 1 stopped.
2025-11-17T09:39:33.746094824Z I1117 09:39:33.746066       1 gather.go:183] gatherer "workloads" function "helmchart_info" took 31.785424ms to process 0 records
2025-11-17T09:39:33.781133407Z I1117 09:39:33.781044       1 gather_workloads_info.go:254] Loaded pods in 0s, will wait 39s for image data
2025-11-17T09:39:33.783122088Z I1117 09:39:33.783069       1 gather_workloads_info.go:363] No image sha256:228978b7b9758ace8adbc774f560770866ae90adcb0ccb79c0886efb4837d12c (7ms)
2025-11-17T09:39:33.793281144Z I1117 09:39:33.793220       1 gather_workloads_info.go:363] No image sha256:4a3e7a9a5f1cbfd06efde645c7702d9452f55d4b46e5441c60b62ee1c701c093 (10ms)
2025-11-17T09:39:33.801561631Z I1117 09:39:33.801518       1 gather_workloads_info.go:363] No image sha256:f0009b75e2c7f64b549d75603042f1d3275b2c7c13bed393fef500d65a2090d5 (8ms)
2025-11-17T09:39:33.809023250Z I1117 09:39:33.808965       1 gather_workloads_info.go:363] No image sha256:6d649893ab29722f7a441c869b32bb69a31beab6c3062d87a63ac31686ab1c0b (7ms)
2025-11-17T09:39:33.822051128Z I1117 09:39:33.821982       1 gather_workloads_info.go:363] No image sha256:5c2b2d529c2d86cf1c4d92c4093ddae2d449bb16ba2b561fe1ebdf7bc651eb4f (13ms)
2025-11-17T09:39:33.835947992Z I1117 09:39:33.835883       1 gather_workloads_info.go:363] No image sha256:d408b5a24e328873cb0faefbc8173996e14d5dbf561bf3851cc2c827f137bbd0 (14ms)
2025-11-17T09:39:33.842536895Z I1117 09:39:33.842478       1 gather_workloads_info.go:363] No image sha256:babc973b7e2f5f2b215a5dfada59cecf1b3829601e999f8e81f89b3f2f4fcb53 (7ms)
2025-11-17T09:39:33.847885946Z I1117 09:39:33.847845       1 gather_workloads_info.go:363] No image sha256:b0f2614c757998d6955b1675db231cb020da21f23bae6c6e87b4850ee30ad06d (5ms)
2025-11-17T09:39:33.858888118Z I1117 09:39:33.858829       1 gather_workloads_info.go:374] Found image sha256:5811836d81c0944f596c5a1f5dfbbda914379db450d5ac2c6db505d6f2ef36e7 (11ms)
2025-11-17T09:39:33.867589369Z I1117 09:39:33.867531       1 gather_workloads_info.go:363] No image sha256:ba9ff4c933739f1774bc8277d636053c5306863221a8c7b7b9ddc4470eb7feff (8ms)
2025-11-17T09:39:33.883793828Z I1117 09:39:33.883642       1 gather_workloads_info.go:363] No image sha256:00c8cabcf0c942d8341c84da768f5414331b92420a1dd395f6ce16c39082c40d (16ms)
2025-11-17T09:39:33.976989159Z I1117 09:39:33.976931       1 request.go:628] Waited for 93.204018ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:bf8c959d1fa2bcfeb6646938ecfc49628a5d750b2bad3b7a1931078b84e82f01
2025-11-17T09:39:33.997701002Z I1117 09:39:33.997638       1 gather_workloads_info.go:363] No image sha256:bf8c959d1fa2bcfeb6646938ecfc49628a5d750b2bad3b7a1931078b84e82f01 (114ms)
2025-11-17T09:39:34.076701347Z I1117 09:39:34.076631       1 request.go:628] Waited for 78.907954ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:647e2dd79a4e5c1e8b1d12a83b23d53dc3447a944d6fab1dbb95ac6482d5635c
2025-11-17T09:39:34.083966613Z I1117 09:39:34.083845       1 gather_workloads_info.go:363] No image sha256:647e2dd79a4e5c1e8b1d12a83b23d53dc3447a944d6fab1dbb95ac6482d5635c (86ms)
2025-11-17T09:39:34.177370778Z I1117 09:39:34.177308       1 request.go:628] Waited for 93.298019ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:cce51eb023d7031304b2e5f51d25a884c523314c6c88842c83bfb7b9f6d42e52
2025-11-17T09:39:34.183886628Z I1117 09:39:34.183836       1 gather_workloads_info.go:363] No image sha256:cce51eb023d7031304b2e5f51d25a884c523314c6c88842c83bfb7b9f6d42e52 (100ms)
2025-11-17T09:39:34.277548838Z I1117 09:39:34.277487       1 request.go:628] Waited for 93.555938ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:430408e1d338dd3e13407042f1ab225a128944fd1f5131f205a7afd1157b6dad
2025-11-17T09:39:34.285224008Z I1117 09:39:34.285172       1 gather_workloads_info.go:363] No image sha256:430408e1d338dd3e13407042f1ab225a128944fd1f5131f205a7afd1157b6dad (101ms)
2025-11-17T09:39:34.377678085Z I1117 09:39:34.377615       1 request.go:628] Waited for 92.35994ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:888b7c3512ea108675bb2a72eaa16adab519cfca3a2b4fa0610767f0d4972d1b
2025-11-17T09:39:34.383505287Z I1117 09:39:34.383472       1 gather_workloads_info.go:363] No image sha256:888b7c3512ea108675bb2a72eaa16adab519cfca3a2b4fa0610767f0d4972d1b (98ms)
2025-11-17T09:39:34.477014132Z I1117 09:39:34.476952       1 request.go:628] Waited for 93.331708ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:e8da3ec3d996ecf44f62a5d8bf98299d3847dc1da633c023117e3cbd6c5ca5ed
2025-11-17T09:39:34.484763484Z I1117 09:39:34.484719       1 gather_workloads_info.go:363] No image sha256:e8da3ec3d996ecf44f62a5d8bf98299d3847dc1da633c023117e3cbd6c5ca5ed (101ms)
2025-11-17T09:39:34.577485972Z I1117 09:39:34.577439       1 request.go:628] Waited for 92.627939ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:b2e8eda7a2741b5e441c7932b4622daa152327f86066086694a481981164f4c9
2025-11-17T09:39:34.585130130Z I1117 09:39:34.585097       1 gather_workloads_info.go:363] No image sha256:b2e8eda7a2741b5e441c7932b4622daa152327f86066086694a481981164f4c9 (100ms)
2025-11-17T09:39:34.677560747Z I1117 09:39:34.677518       1 request.go:628] Waited for 92.335163ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:5be33ea0cd705c99a2f9e3ecb72610b29795277a45279cab759f1e14f8ec520f
2025-11-17T09:39:34.684375204Z I1117 09:39:34.684303       1 gather_workloads_info.go:363] No image sha256:5be33ea0cd705c99a2f9e3ecb72610b29795277a45279cab759f1e14f8ec520f (99ms)
2025-11-17T09:39:34.777676225Z I1117 09:39:34.777624       1 request.go:628] Waited for 93.225539ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:abf0c37adc28a1eb3846a8d2d14fb3d0b8cfe878d12f824d96ceac7556ae0953
2025-11-17T09:39:34.783580973Z I1117 09:39:34.783543       1 gather_workloads_info.go:363] No image sha256:abf0c37adc28a1eb3846a8d2d14fb3d0b8cfe878d12f824d96ceac7556ae0953 (99ms)
2025-11-17T09:39:34.877650496Z I1117 09:39:34.877593       1 request.go:628] Waited for 93.950561ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:ed77bf76d333d8a2304bdf32c4896d4879542d3c417e784333acf1de7e871b7e
2025-11-17T09:39:34.885129784Z I1117 09:39:34.885087       1 gather_workloads_info.go:374] Found image sha256:ed77bf76d333d8a2304bdf32c4896d4879542d3c417e784333acf1de7e871b7e (102ms)
2025-11-17T09:39:34.977650898Z I1117 09:39:34.977608       1 request.go:628] Waited for 92.252455ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:ec0a4d8782359c203d67e658be93894a1451c9cd0e0683262502f113f33ae580
2025-11-17T09:39:34.984350696Z I1117 09:39:34.984324       1 gather_workloads_info.go:363] No image sha256:ec0a4d8782359c203d67e658be93894a1451c9cd0e0683262502f113f33ae580 (99ms)
2025-11-17T09:39:35.077129102Z I1117 09:39:35.077070       1 request.go:628] Waited for 92.6627ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:283f03198e6f6ea880a010d79ce7fb023a246460a782e7bdb8a0e39e7c3dea23
2025-11-17T09:39:35.083777457Z I1117 09:39:35.083737       1 gather_workloads_info.go:363] No image sha256:283f03198e6f6ea880a010d79ce7fb023a246460a782e7bdb8a0e39e7c3dea23 (99ms)
2025-11-17T09:39:35.177092285Z I1117 09:39:35.177045       1 request.go:628] Waited for 93.193944ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:1a02fd2ae00143baf6432510ede55f9add8779973b07c1e5fe6c9036f008f2da
2025-11-17T09:39:35.183622477Z I1117 09:39:35.183590       1 gather_workloads_info.go:363] No image sha256:1a02fd2ae00143baf6432510ede55f9add8779973b07c1e5fe6c9036f008f2da (100ms)
2025-11-17T09:39:35.277080499Z I1117 09:39:35.277013       1 request.go:628] Waited for 93.27611ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:a3d55e835ff9d72acf206a419ca1e24c9b32d6535b34b7e79160577fd46c74e0
2025-11-17T09:39:35.283520466Z I1117 09:39:35.283478       1 gather_workloads_info.go:363] No image sha256:a3d55e835ff9d72acf206a419ca1e24c9b32d6535b34b7e79160577fd46c74e0 (100ms)
2025-11-17T09:39:35.376733600Z I1117 09:39:35.376680       1 request.go:628] Waited for 93.027283ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:e163e1357b0251bc76ce6b4f0e5b63103b52862acc66511b2a55caa1951fae7a
2025-11-17T09:39:35.383702430Z I1117 09:39:35.383659       1 gather_workloads_info.go:363] No image sha256:e163e1357b0251bc76ce6b4f0e5b63103b52862acc66511b2a55caa1951fae7a (100ms)
2025-11-17T09:39:35.477024791Z I1117 09:39:35.476964       1 request.go:628] Waited for 93.19773ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:d32a7fa0d3d4532baa7af2e6e98760ae6a09b130163e465df8adcb2db2cb0c63
2025-11-17T09:39:35.483723607Z I1117 09:39:35.483691       1 gather_workloads_info.go:363] No image sha256:d32a7fa0d3d4532baa7af2e6e98760ae6a09b130163e465df8adcb2db2cb0c63 (100ms)
2025-11-17T09:39:35.577240434Z I1117 09:39:35.577183       1 request.go:628] Waited for 93.34934ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:3ba453dc2bd460133b5666aec4b82b6cb331cfaea611cb9fa244729f9519d9d8
2025-11-17T09:39:35.584054288Z I1117 09:39:35.583993       1 gather_workloads_info.go:363] No image sha256:3ba453dc2bd460133b5666aec4b82b6cb331cfaea611cb9fa244729f9519d9d8 (100ms)
2025-11-17T09:39:35.677417006Z I1117 09:39:35.677374       1 request.go:628] Waited for 93.268727ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:33a95d0860da1ff59cfa2506ab860cc8ce6f354eec9e899612f7e89d6dd915ee
2025-11-17T09:39:35.683496814Z I1117 09:39:35.683255       1 gather_workloads_info.go:363] No image sha256:33a95d0860da1ff59cfa2506ab860cc8ce6f354eec9e899612f7e89d6dd915ee (99ms)
2025-11-17T09:39:35.777645563Z I1117 09:39:35.777585       1 request.go:628] Waited for 94.223088ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:e5eee83ff9f2de2e9decf8819c29cf6b822cef208048a1216305666716dbe3bc
2025-11-17T09:39:35.783165716Z I1117 09:39:35.783103       1 gather_workloads_info.go:363] No image sha256:e5eee83ff9f2de2e9decf8819c29cf6b822cef208048a1216305666716dbe3bc (100ms)
2025-11-17T09:39:35.877187952Z I1117 09:39:35.877076       1 request.go:628] Waited for 93.890166ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:66585fe343cc244cde73568084891b573a61066e20b3404b646f1905e762f472
2025-11-17T09:39:35.884469362Z I1117 09:39:35.884420       1 gather_workloads_info.go:363] No image sha256:66585fe343cc244cde73568084891b573a61066e20b3404b646f1905e762f472 (101ms)
2025-11-17T09:39:35.977152026Z I1117 09:39:35.976937       1 request.go:628] Waited for 92.428758ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:685b764835a135da12c73f4fea43b72423b39897f2c65d726293760ab28a9df0
2025-11-17T09:39:35.983305211Z I1117 09:39:35.983260       1 gather_workloads_info.go:363] No image sha256:685b764835a135da12c73f4fea43b72423b39897f2c65d726293760ab28a9df0 (99ms)
2025-11-17T09:39:36.077063111Z I1117 09:39:36.076959       1 request.go:628] Waited for 93.601285ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:3d61fb86528ad9443c933105b91c987345930e3215e563e3587e082b3b0d7f6b
2025-11-17T09:39:36.083404320Z I1117 09:39:36.083371       1 gather_workloads_info.go:363] No image sha256:3d61fb86528ad9443c933105b91c987345930e3215e563e3587e082b3b0d7f6b (100ms)
2025-11-17T09:39:36.177755075Z I1117 09:39:36.177690       1 request.go:628] Waited for 94.231142ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:7154a84c4f7317dc26e545f010f0a00afc2cac546ca50e85b5ceccc021707957
2025-11-17T09:39:36.183916057Z I1117 09:39:36.183794       1 gather_workloads_info.go:363] No image sha256:7154a84c4f7317dc26e545f010f0a00afc2cac546ca50e85b5ceccc021707957 (100ms)
2025-11-17T09:39:36.277333447Z I1117 09:39:36.277154       1 request.go:628] Waited for 93.283391ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:af8d2b3007c5efb7a02d0ef94fa542ee98ebfc6da30364f7bdb6489bfa7b96ac
2025-11-17T09:39:36.286406836Z I1117 09:39:36.286321       1 gather_workloads_info.go:363] No image sha256:af8d2b3007c5efb7a02d0ef94fa542ee98ebfc6da30364f7bdb6489bfa7b96ac (102ms)
2025-11-17T09:39:36.377674312Z I1117 09:39:36.377615       1 request.go:628] Waited for 91.213458ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:8e9eb588e207480ad5f5a5f817253451df5fd90749ee5bffe3addf652c1a84af
2025-11-17T09:39:36.383551104Z I1117 09:39:36.383522       1 gather_workloads_info.go:363] No image sha256:8e9eb588e207480ad5f5a5f817253451df5fd90749ee5bffe3addf652c1a84af (97ms)
2025-11-17T09:39:36.477060473Z I1117 09:39:36.476829       1 request.go:628] Waited for 93.223388ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:3de3b6d3f91f80c901c6bb073e939240a5a239a8333e10d4ba24a7d7e5b5990d
2025-11-17T09:39:36.483756621Z I1117 09:39:36.483646       1 gather_workloads_info.go:363] No image sha256:3de3b6d3f91f80c901c6bb073e939240a5a239a8333e10d4ba24a7d7e5b5990d (100ms)
2025-11-17T09:39:36.577461923Z I1117 09:39:36.577337       1 request.go:628] Waited for 93.595422ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:a7b207552d68e89034828d63a08ccb5cccbedc4d28cba0c58bbcda63f726e448
2025-11-17T09:39:36.583946411Z I1117 09:39:36.583887       1 gather_workloads_info.go:363] No image sha256:a7b207552d68e89034828d63a08ccb5cccbedc4d28cba0c58bbcda63f726e448 (100ms)
2025-11-17T09:39:36.677296540Z I1117 09:39:36.677220       1 request.go:628] Waited for 93.22616ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:1bed9ed6078f08a2bf1169d65d3185e08124231b209fdf8d1f107b5a914fb776
2025-11-17T09:39:36.683612163Z I1117 09:39:36.683562       1 gather_workloads_info.go:363] No image sha256:1bed9ed6078f08a2bf1169d65d3185e08124231b209fdf8d1f107b5a914fb776 (100ms)
2025-11-17T09:39:36.781662540Z I1117 09:39:36.779364       1 request.go:628] Waited for 95.720096ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:a6c98395f56f5d5f33da09e7c825696d2dd66847a68a9c0c1b95c5f95ea89626
2025-11-17T09:39:36.788517581Z I1117 09:39:36.787630       1 gather_workloads_info.go:363] No image sha256:a6c98395f56f5d5f33da09e7c825696d2dd66847a68a9c0c1b95c5f95ea89626 (104ms)
2025-11-17T09:39:36.877079350Z I1117 09:39:36.877029       1 request.go:628] Waited for 89.274123ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:416f639ead0c4435fbfbf252176af291a5b443e85e64cabe9d6e2312a3eb4ea9
2025-11-17T09:39:36.884500557Z I1117 09:39:36.884473       1 gather_workloads_info.go:363] No image sha256:416f639ead0c4435fbfbf252176af291a5b443e85e64cabe9d6e2312a3eb4ea9 (97ms)
2025-11-17T09:39:36.976777650Z I1117 09:39:36.976719       1 request.go:628] Waited for 92.153412ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:f5a9e4e7ae83db4da1a39746e2c4482ec2a1f966c80a20c2fdc9324048985d3b
2025-11-17T09:39:36.984666663Z I1117 09:39:36.984621       1 gather_workloads_info.go:363] No image sha256:f5a9e4e7ae83db4da1a39746e2c4482ec2a1f966c80a20c2fdc9324048985d3b (100ms)
2025-11-17T09:39:37.077748047Z I1117 09:39:37.077679       1 request.go:628] Waited for 92.979011ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:c86cbe82a7e5053ef108de61846e78a9b6c8511fef6dbd54c871632c329ae365
2025-11-17T09:39:37.083654309Z I1117 09:39:37.083613       1 gather_workloads_info.go:363] No image sha256:c86cbe82a7e5053ef108de61846e78a9b6c8511fef6dbd54c871632c329ae365 (99ms)
2025-11-17T09:39:37.176973339Z I1117 09:39:37.176916       1 request.go:628] Waited for 93.226329ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:414f00ab948017c5c4177359116b335ee790107434b897315de7ae2bf8c9f365
2025-11-17T09:39:37.182806184Z I1117 09:39:37.182710       1 gather_workloads_info.go:363] No image sha256:414f00ab948017c5c4177359116b335ee790107434b897315de7ae2bf8c9f365 (99ms)
2025-11-17T09:39:37.277111184Z I1117 09:39:37.277038       1 request.go:628] Waited for 94.231133ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:8887fd04eb9e75cd24d508f73d43efaf23b1948240c4947643bb70982b1465e3
2025-11-17T09:39:37.288105718Z I1117 09:39:37.288040       1 gather_workloads_info.go:363] No image sha256:8887fd04eb9e75cd24d508f73d43efaf23b1948240c4947643bb70982b1465e3 (105ms)
2025-11-17T09:39:37.377463975Z I1117 09:39:37.377306       1 request.go:628] Waited for 89.165751ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:c7f25712d12448b913416e4c2b0b67196666247e43505a61c25797570a9a208a
2025-11-17T09:39:37.383142885Z I1117 09:39:37.383107       1 gather_workloads_info.go:363] No image sha256:c7f25712d12448b913416e4c2b0b67196666247e43505a61c25797570a9a208a (95ms)
2025-11-17T09:39:37.477500104Z I1117 09:39:37.477435       1 request.go:628] Waited for 94.247687ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:78b436e1afcb03ba063babeaefc80714793775d7547de52ff2af05a53b6bc831
2025-11-17T09:39:37.485832257Z I1117 09:39:37.485780       1 gather_workloads_info.go:363] No image sha256:78b436e1afcb03ba063babeaefc80714793775d7547de52ff2af05a53b6bc831 (103ms)
2025-11-17T09:39:37.577574687Z I1117 09:39:37.577521       1 request.go:628] Waited for 91.661517ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:4a54ff5000da3fdab0bbb4a27e3fee77e926ea650d25488beed190484f7a7cf7
2025-11-17T09:39:37.583855188Z I1117 09:39:37.583804       1 gather_workloads_info.go:363] No image sha256:4a54ff5000da3fdab0bbb4a27e3fee77e926ea650d25488beed190484f7a7cf7 (98ms)
2025-11-17T09:39:37.677512240Z I1117 09:39:37.677458       1 request.go:628] Waited for 93.566513ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:a574da8dfc49d111bad70b1e86321aa586a0d043c9e07b41b7790e95d8c4a115
2025-11-17T09:39:37.684444676Z I1117 09:39:37.684403       1 gather_workloads_info.go:363] No image sha256:a574da8dfc49d111bad70b1e86321aa586a0d043c9e07b41b7790e95d8c4a115 (101ms)
2025-11-17T09:39:37.776878712Z I1117 09:39:37.776824       1 request.go:628] Waited for 92.340632ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:0edb715030d631711ac59ca73dc7f9083080f937f5675f5e45503e40b1e8aa5d
2025-11-17T09:39:37.784659168Z I1117 09:39:37.784631       1 gather_workloads_info.go:363] No image sha256:0edb715030d631711ac59ca73dc7f9083080f937f5675f5e45503e40b1e8aa5d (100ms)
2025-11-17T09:39:37.877006937Z I1117 09:39:37.876944       1 request.go:628] Waited for 92.235416ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:0dc628de8b8f6899659a0da11b21ba81d4cb9f73684f3f502bee47e06c5dbe36
2025-11-17T09:39:37.883954136Z I1117 09:39:37.883911       1 gather_workloads_info.go:363] No image sha256:0dc628de8b8f6899659a0da11b21ba81d4cb9f73684f3f502bee47e06c5dbe36 (99ms)
2025-11-17T09:39:37.977316084Z I1117 09:39:37.977249       1 request.go:628] Waited for 93.226214ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:1e159dbb6d697025ac85dcd28660c1c607924a3dc51b740979e6e8d03a3ceca8
2025-11-17T09:39:37.984595310Z I1117 09:39:37.984546       1 gather_workloads_info.go:363] No image sha256:1e159dbb6d697025ac85dcd28660c1c607924a3dc51b740979e6e8d03a3ceca8 (101ms)
2025-11-17T09:39:38.077125125Z I1117 09:39:38.077058       1 request.go:628] Waited for 92.406241ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:787a5613f0ceb353e229b86bb6e54a612313662da30da0c5bc6463ecfb247e90
2025-11-17T09:39:38.084865835Z I1117 09:39:38.084805       1 gather_workloads_info.go:363] No image sha256:787a5613f0ceb353e229b86bb6e54a612313662da30da0c5bc6463ecfb247e90 (100ms)
2025-11-17T09:39:38.177371909Z I1117 09:39:38.177312       1 request.go:628] Waited for 92.332283ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:a8e92e3d80cba91d101ae4133ece6c98185165da5c1fec6ae9fcb91320ed5ef4
2025-11-17T09:39:38.184684592Z I1117 09:39:38.184639       1 gather_workloads_info.go:363] No image sha256:a8e92e3d80cba91d101ae4133ece6c98185165da5c1fec6ae9fcb91320ed5ef4 (100ms)
2025-11-17T09:39:38.277065070Z I1117 09:39:38.277008       1 request.go:628] Waited for 92.249142ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:c1fa49664557842e1d10be112b6e3a97a13edbc612b3fbc0bfa53eb8f058d142
2025-11-17T09:39:38.283831294Z I1117 09:39:38.283797       1 gather_workloads_info.go:363] No image sha256:c1fa49664557842e1d10be112b6e3a97a13edbc612b3fbc0bfa53eb8f058d142 (99ms)
2025-11-17T09:39:38.377369276Z I1117 09:39:38.377315       1 request.go:628] Waited for 93.348717ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:6c1af3099f51f104eb3c0754d92a3dec69f6e946c5fcff0d8f35e36f45d1c716
2025-11-17T09:39:38.383787354Z I1117 09:39:38.383761       1 gather_workloads_info.go:363] No image sha256:6c1af3099f51f104eb3c0754d92a3dec69f6e946c5fcff0d8f35e36f45d1c716 (100ms)
2025-11-17T09:39:38.477172527Z I1117 09:39:38.477106       1 request.go:628] Waited for 93.254276ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:54ef1f5f4734820c8e741771aab19943c82be1287b9654a89260885e0abfadd2
2025-11-17T09:39:38.483934569Z I1117 09:39:38.483902       1 gather_workloads_info.go:363] No image sha256:54ef1f5f4734820c8e741771aab19943c82be1287b9654a89260885e0abfadd2 (100ms)
2025-11-17T09:39:38.577655814Z I1117 09:39:38.577601       1 request.go:628] Waited for 93.615843ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:86702c0acd55053ff2e5c2072d86f82109142b1d0d7c8701919f5c06489d1ffe
2025-11-17T09:39:38.584747812Z I1117 09:39:38.584716       1 gather_workloads_info.go:363] No image sha256:86702c0acd55053ff2e5c2072d86f82109142b1d0d7c8701919f5c06489d1ffe (101ms)
2025-11-17T09:39:38.677178066Z I1117 09:39:38.677118       1 request.go:628] Waited for 92.28288ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:e5d6cb0faaed138203b65e4c4033a18a855db82f2a05f95289904d1d5601b4a6
2025-11-17T09:39:38.683887982Z I1117 09:39:38.683853       1 gather_workloads_info.go:363] No image sha256:e5d6cb0faaed138203b65e4c4033a18a855db82f2a05f95289904d1d5601b4a6 (99ms)
2025-11-17T09:39:38.777294037Z I1117 09:39:38.777230       1 request.go:628] Waited for 93.274678ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:52985af8af2dc23fc2a79016936f33f41ae5c9ccad4b65f5f704f37d7cb59cce
2025-11-17T09:39:38.783182179Z I1117 09:39:38.783142       1 gather_workloads_info.go:363] No image sha256:52985af8af2dc23fc2a79016936f33f41ae5c9ccad4b65f5f704f37d7cb59cce (99ms)
2025-11-17T09:39:38.876728684Z I1117 09:39:38.876675       1 request.go:628] Waited for 93.410561ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:6913f393088542a2ab8382b1f12df53c6bbb6ccc8bde7ee8736053b7875be849
2025-11-17T09:39:38.885239091Z I1117 09:39:38.885207       1 gather_workloads_info.go:363] No image sha256:6913f393088542a2ab8382b1f12df53c6bbb6ccc8bde7ee8736053b7875be849 (102ms)
2025-11-17T09:39:38.977578601Z I1117 09:39:38.977526       1 request.go:628] Waited for 92.23567ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:3cd509e6bc6df891aaf6057a0c9d6228e5fd198fe3c00bfd66a30cb3fc21f60f
2025-11-17T09:39:38.984302218Z I1117 09:39:38.984246       1 gather_workloads_info.go:363] No image sha256:3cd509e6bc6df891aaf6057a0c9d6228e5fd198fe3c00bfd66a30cb3fc21f60f (99ms)
2025-11-17T09:39:39.076979759Z I1117 09:39:39.076915       1 request.go:628] Waited for 92.551022ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:41a6c259299f7019a4b737c86e0c785ae99119710064c2513778df37be2a4894
2025-11-17T09:39:39.085708847Z I1117 09:39:39.085681       1 gather_workloads_info.go:363] No image sha256:41a6c259299f7019a4b737c86e0c785ae99119710064c2513778df37be2a4894 (101ms)
2025-11-17T09:39:39.177053276Z I1117 09:39:39.177006       1 request.go:628] Waited for 91.239476ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:e1abb867440543193e616d744184d03b841f6870e278cc6f3d9f50450e222111
2025-11-17T09:39:39.184985772Z I1117 09:39:39.184947       1 gather_workloads_info.go:363] No image sha256:e1abb867440543193e616d744184d03b841f6870e278cc6f3d9f50450e222111 (99ms)
2025-11-17T09:39:39.277424800Z I1117 09:39:39.277385       1 request.go:628] Waited for 92.366763ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:0dcaafeb58ed3f4be926bbcfeb33fe260e22de5c9effa79fe3fb60a19cedeb8b
2025-11-17T09:39:39.284070835Z I1117 09:39:39.284031       1 gather_workloads_info.go:363] No image sha256:0dcaafeb58ed3f4be926bbcfeb33fe260e22de5c9effa79fe3fb60a19cedeb8b (99ms)
2025-11-17T09:39:39.377486652Z I1117 09:39:39.377434       1 request.go:628] Waited for 93.315004ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:7b09cc7018210fdc6a5f0c0b713591bc37530f25c3892be00020a132f0e46140
2025-11-17T09:39:39.383331409Z I1117 09:39:39.383307       1 gather_workloads_info.go:363] No image sha256:7b09cc7018210fdc6a5f0c0b713591bc37530f25c3892be00020a132f0e46140 (99ms)
2025-11-17T09:39:39.477582645Z I1117 09:39:39.477531       1 request.go:628] Waited for 94.145542ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:ee6c0e81c886fc7ef52f2a34d19fd1db76f82b83ebfe28d3f93cbe63485ecaf3
2025-11-17T09:39:39.482879381Z I1117 09:39:39.482841       1 gather_workloads_info.go:363] No image sha256:ee6c0e81c886fc7ef52f2a34d19fd1db76f82b83ebfe28d3f93cbe63485ecaf3 (100ms)
2025-11-17T09:39:39.577669102Z I1117 09:39:39.577620       1 request.go:628] Waited for 94.691651ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:e9c3ddef625be929e17000e39d72e71f3cf42babd9c0aebaad259bc3dfd29d43
2025-11-17T09:39:39.584182331Z I1117 09:39:39.584133       1 gather_workloads_info.go:363] No image sha256:e9c3ddef625be929e17000e39d72e71f3cf42babd9c0aebaad259bc3dfd29d43 (101ms)
2025-11-17T09:39:39.677496265Z I1117 09:39:39.677447       1 request.go:628] Waited for 93.238659ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:08246544079b9d97090ecfe34b29b2770b9c31ce7b3eb2ff330f37c3428cbd2a
2025-11-17T09:39:39.687505223Z I1117 09:39:39.687469       1 gather_workloads_info.go:363] No image sha256:08246544079b9d97090ecfe34b29b2770b9c31ce7b3eb2ff330f37c3428cbd2a (103ms)
2025-11-17T09:39:39.776902872Z I1117 09:39:39.776856       1 request.go:628] Waited for 89.310914ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:e8f65cf7ea2c96ecce9ce614528673e1e384b08b84c979f12d5e210e9d169859
2025-11-17T09:39:39.782869965Z I1117 09:39:39.782825       1 gather_workloads_info.go:363] No image sha256:e8f65cf7ea2c96ecce9ce614528673e1e384b08b84c979f12d5e210e9d169859 (95ms)
2025-11-17T09:39:39.877348244Z I1117 09:39:39.877291       1 request.go:628] Waited for 94.382439ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:40d15ebff8ccab7d330f5b995d59e01999bac4cffc57d702e14a9f407dd210f4
2025-11-17T09:39:39.885768918Z I1117 09:39:39.885726       1 gather_workloads_info.go:363] No image sha256:40d15ebff8ccab7d330f5b995d59e01999bac4cffc57d702e14a9f407dd210f4 (103ms)
2025-11-17T09:39:39.977182040Z I1117 09:39:39.977052       1 request.go:628] Waited for 91.233915ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:08237dea0e1cdc10f6b15edf4b3ef084c91d3ee9ac445268abed28a2207fae0c
2025-11-17T09:39:39.986250085Z I1117 09:39:39.986219       1 gather_workloads_info.go:363] No image sha256:08237dea0e1cdc10f6b15edf4b3ef084c91d3ee9ac445268abed28a2207fae0c (100ms)
2025-11-17T09:39:40.077033756Z I1117 09:39:40.076912       1 request.go:628] Waited for 90.555526ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:635d55b81c965996bf4c9b5fc8ac3b894062789a609279db6b185a6d0a42093b
2025-11-17T09:39:40.084345911Z I1117 09:39:40.083259       1 gather_workloads_info.go:363] No image sha256:635d55b81c965996bf4c9b5fc8ac3b894062789a609279db6b185a6d0a42093b (97ms)
2025-11-17T09:39:40.185264097Z I1117 09:39:40.183371       1 request.go:628] Waited for 100.005903ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:35e01911b75e094419e463401828596ba93ab641cb11521d2a54c523d8917303
2025-11-17T09:39:40.190920855Z I1117 09:39:40.190885       1 gather_workloads_info.go:374] Found image sha256:35e01911b75e094419e463401828596ba93ab641cb11521d2a54c523d8917303 (108ms)
2025-11-17T09:39:40.277427299Z I1117 09:39:40.277364       1 request.go:628] Waited for 86.274942ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:0c03c0edd26a33051b7899f7e6c54fe138d0e8c4e96d1e927280a1165643db32
2025-11-17T09:39:40.285719305Z I1117 09:39:40.285669       1 gather_workloads_info.go:363] No image sha256:0c03c0edd26a33051b7899f7e6c54fe138d0e8c4e96d1e927280a1165643db32 (95ms)
2025-11-17T09:39:40.377045806Z I1117 09:39:40.376983       1 request.go:628] Waited for 91.226917ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:d52837dc67ae6fc0870a06c4f990b84d26f616c563711f7a14dc72b3dac00da0
2025-11-17T09:39:40.383677907Z I1117 09:39:40.383633       1 gather_workloads_info.go:363] No image sha256:d52837dc67ae6fc0870a06c4f990b84d26f616c563711f7a14dc72b3dac00da0 (98ms)
2025-11-17T09:39:40.477133886Z I1117 09:39:40.476985       1 request.go:628] Waited for 93.273699ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:325d61e0c4a2babc342b41545258feea102c7e1ba9840c14c6dde0e5d04a8b66
2025-11-17T09:39:40.485810774Z I1117 09:39:40.485751       1 gather_workloads_info.go:363] No image sha256:325d61e0c4a2babc342b41545258feea102c7e1ba9840c14c6dde0e5d04a8b66 (102ms)
2025-11-17T09:39:40.577333927Z I1117 09:39:40.577259       1 request.go:628] Waited for 91.413349ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:27625051ea60d657eb87cb49ac9fb8dc86377a83048125c63e9dcf8d233c45cd
2025-11-17T09:39:40.582912017Z I1117 09:39:40.582871       1 gather_workloads_info.go:363] No image sha256:27625051ea60d657eb87cb49ac9fb8dc86377a83048125c63e9dcf8d233c45cd (97ms)
2025-11-17T09:39:40.677257533Z I1117 09:39:40.677191       1 request.go:628] Waited for 94.239731ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:51de826f984f59a3eeaaa65f0097c5c3ef2bd06af0f13f4f21a94cf0788942f2
2025-11-17T09:39:40.683538947Z I1117 09:39:40.683492       1 gather_workloads_info.go:363] No image sha256:51de826f984f59a3eeaaa65f0097c5c3ef2bd06af0f13f4f21a94cf0788942f2 (101ms)
2025-11-17T09:39:40.777611911Z I1117 09:39:40.777543       1 request.go:628] Waited for 93.937096ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:3f0fa3977d0a00292dc382c8492a38f2f6619e7b9b1b2d691e9f8c4116c43bff
2025-11-17T09:39:40.785561039Z I1117 09:39:40.785500       1 gather_workloads_info.go:363] No image sha256:3f0fa3977d0a00292dc382c8492a38f2f6619e7b9b1b2d691e9f8c4116c43bff (102ms)
2025-11-17T09:39:40.876843706Z I1117 09:39:40.876700       1 request.go:628] Waited for 91.111674ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:47f0484734ad2f554dbca4d9b5b2e40ec796513bcb8a4569ab3e7b30a4c3dd14
2025-11-17T09:39:40.884954975Z I1117 09:39:40.884853       1 gather_workloads_info.go:363] No image sha256:47f0484734ad2f554dbca4d9b5b2e40ec796513bcb8a4569ab3e7b30a4c3dd14 (99ms)
2025-11-17T09:39:40.977391078Z I1117 09:39:40.977329       1 request.go:628] Waited for 92.352529ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:cd03e8036b1bc970ad4dce41f12961a7ca5a833361dd02aaf8951d0e4736f2c2
2025-11-17T09:39:40.983515863Z I1117 09:39:40.983482       1 gather_workloads_info.go:363] No image sha256:cd03e8036b1bc970ad4dce41f12961a7ca5a833361dd02aaf8951d0e4736f2c2 (99ms)
2025-11-17T09:39:41.021423973Z I1117 09:39:41.021307       1 insightsuploader.go:129] Nothing to report since 0001-01-01T00:00:00Z
2025-11-17T09:39:41.077548249Z I1117 09:39:41.077453       1 request.go:628] Waited for 93.870906ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:e88df5c8f365decc59f848649d23f02e678451d1a698af928b4a80340b998d73
2025-11-17T09:39:41.083860788Z I1117 09:39:41.083809       1 gather_workloads_info.go:363] No image sha256:e88df5c8f365decc59f848649d23f02e678451d1a698af928b4a80340b998d73 (100ms)
2025-11-17T09:39:41.177296112Z I1117 09:39:41.177194       1 request.go:628] Waited for 93.291167ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:4a667bb9783fa4cdbc32e147ace044bdc78c958b120038934cd66e15b6e96035
2025-11-17T09:39:41.185518646Z I1117 09:39:41.185481       1 gather_workloads_info.go:363] No image sha256:4a667bb9783fa4cdbc32e147ace044bdc78c958b120038934cd66e15b6e96035 (102ms)
2025-11-17T09:39:41.276972487Z I1117 09:39:41.276815       1 request.go:628] Waited for 91.194413ms due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/image.openshift.io/v1/images/sha256:e59aa00c7f9dafd9df046b93f62073d6fc36e5f788975a74a2d043f33192d2a3
2025-11-17T09:39:41.283909082Z I1117 09:39:41.283867       1 gather_workloads_info.go:363] No image sha256:e59aa00c7f9dafd9df046b93f62073d6fc36e5f788975a74a2d043f33192d2a3 (98ms)
2025-11-17T09:39:41.283940969Z I1117 09:39:41.283923       1 tasks_processing.go:74] XXXXXX 0 stopped.
2025-11-17T09:39:41.284526920Z I1117 09:39:41.284506       1 recorder.go:70] Recording config/workload_info with fingerprint=7c0a6e02623f4b9bda6ee0c11c53bf5e2b816de2e5264753ce9e75d6519a8519
2025-11-17T09:39:41.284534440Z I1117 09:39:41.284524       1 gather.go:183] gatherer "workloads" function "workload_info" took 7.56951092s to process 1 records
2025-11-17T09:39:41.284541052Z I1117 09:39:41.284536       1 periodic.go:210] Periodic gather workloads completed in 7.57s
2025-11-17T09:39:41.284547546Z I1117 09:39:41.284543       1 controllerstatus.go:77] name=periodic-workloads healthy=true reason= message=
2025-11-17T09:39:41.284560512Z I1117 09:39:41.284552       1 periodic.go:204] Running conditional gatherer
2025-11-17T09:39:41.284577879Z I1117 09:39:41.284563       1 requests.go:225] Preparing a request to Insights Operator Gathering Conditions Service at the endpoint "https://console.redhat.com/api/gathering/gathering_rules"
2025-11-17T09:39:41.291688344Z I1117 09:39:41.291563       1 requests.go:244] Performing a request to Insights Operator Gathering Conditions Service
2025-11-17T09:39:41.444440702Z I1117 09:39:41.444387       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.504454ms" userAgent="Prometheus/2.46.0" audit-ID="93bbe975-83e7-4edb-84d6-a73d348bba4a" srcIP="10.131.0.17:45670" resp=200
2025-11-17T09:39:42.046268807Z I1117 09:39:42.046189       1 conditional_gatherer.go:94] got 9 gathering rules for conditional gatherer with version 1.1.3
2025-11-17T09:39:42.049364309Z I1117 09:39:42.049329       1 conditional_gatherer.go:242] updating alerts cache for conditional gatherer
2025-11-17T09:39:43.082982489Z I1117 09:39:43.082917       1 conditional_gatherer.go:288] updating version cache for conditional gatherer
2025-11-17T09:39:43.087752581Z I1117 09:39:43.087703       1 conditional_gatherer.go:296] cluster version is '4.14.59'
2025-11-17T09:39:43.087776137Z I1117 09:39:43.087756       1 tasks_processing.go:45] number of XXXXXXs: 1
2025-11-17T09:39:43.087776137Z I1117 09:39:43.087763       1 tasks_processing.go:69] XXXXXX 0 listening for tasks.
2025-11-17T09:39:43.087782742Z I1117 09:39:43.087774       1 tasks_processing.go:71] XXXXXX 0 working on conditional_gatherer_rules task.
2025-11-17T09:39:43.087821706Z I1117 09:39:43.087800       1 tasks_processing.go:74] XXXXXX 0 stopped.
2025-11-17T09:39:43.087912754Z I1117 09:39:43.087892       1 recorder.go:70] Recording insights-operator/conditional-gatherer-rules with fingerprint=abcc8638512299ce9f07a1b8c04e92dcc52f0bc51686b9b4c9d920c7771e8226
2025-11-17T09:39:43.087919292Z I1117 09:39:43.087909       1 gather.go:183] gatherer "conditional" function "conditional_gatherer_rules" took 1.159s to process 1 records
2025-11-17T09:39:43.087926457Z I1117 09:39:43.087922       1 periodic.go:210] Periodic gather conditional completed in 1.803s
2025-11-17T09:39:43.087941543Z I1117 09:39:43.087931       1 controllerstatus.go:77] name=periodic-conditional healthy=true reason= message=
2025-11-17T09:39:43.104798310Z W1117 09:39:43.104744       1 gather.go:215] can't read cgroups memory usage data: open /sys/fs/cgroup/memory/memory.usage_in_bytes: no such file or directory
2025-11-17T09:39:43.104950731Z I1117 09:39:43.104930       1 recorder.go:70] Recording insights-operator/gathers with fingerprint=78d5e0e2e9521166a56b81cddca6ad1753683d3b09d906f1aa4c7370d6f8a9ff
2025-11-17T09:39:43.105189694Z I1117 09:39:43.105170       1 diskrecorder.go:70] Writing 246 records to /var/lib/insights-operator/insights-2025-11-17-093943.tar.gz
2025-11-17T09:39:43.131437130Z I1117 09:39:43.131348       1 diskrecorder.go:51] Wrote 246 records to disk in 26ms
2025-11-17T09:39:43.131470146Z I1117 09:39:43.131442       1 periodic.go:240] Gathering cluster info every 2h0m0s
2025-11-17T09:39:47.528641962Z I1117 09:39:47.528570       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.30956ms" userAgent="Prometheus/2.46.0" audit-ID="7b90892f-451f-49a2-b1c1-9a50a27132dd" srcIP="10.128.2.12:50340" resp=200
2025-11-17T09:39:56.022323692Z I1117 09:39:56.022260       1 diskrecorder.go:170] Found files to send: insights-2025-11-17-093943.tar.gz
2025-11-17T09:39:56.022323692Z I1117 09:39:56.022310       1 insightsuploader.go:137] Uploading XXXXXX report since 0001-01-01T00:00:00Z
2025-11-17T09:39:56.032000887Z I1117 09:39:56.031942       1 requests.go:47] Uploading application/vnd.redhat.openshift.periodic to https://console.redhat.com/api/ingress/v1/upload
2025-11-17T09:39:57.466390968Z I1117 09:39:57.466323       1 requests.go:88] Successfully reported id=2025-11-17T09:39:56Z x-rh-insights-request-id=791a29b2da504e94be1e91dea1ba56c0, wrote=181076
2025-11-17T09:39:57.466428992Z I1117 09:39:57.466406       1 insightsuploader.go:158] Uploaded report successfully in 1.444086482s
2025-11-17T09:39:57.466436688Z I1117 09:39:57.466431       1 controller.go:116] Initializing last reported time to 2025-11-17T09:39:56Z
2025-11-17T09:39:57.466478716Z I1117 09:39:57.466464       1 insightsreport.go:303] Archive uploaded, starting pulling report...
2025-11-17T09:39:57.466478716Z I1117 09:39:57.466475       1 insightsreport.go:209] Starting retrieving report from Smart Proxy
2025-11-17T09:39:57.466497149Z I1117 09:39:57.466482       1 insightsreport.go:220] Initial delay for pulling: 1m0s
2025-11-17T09:39:57.469785775Z I1117 09:39:57.469747       1 controller.go:444] The operator is healthy
2025-11-17T09:40:17.521964992Z I1117 09:40:17.521851       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.224242ms" userAgent="Prometheus/2.46.0" audit-ID="8e5ab53f-9a29-417f-baae-d20b98533efa" srcIP="10.128.2.12:50340" resp=200
2025-11-17T09:40:41.443620364Z I1117 09:40:41.443554       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.517001ms" userAgent="Prometheus/2.46.0" audit-ID="459b2aac-a339-462d-aae5-499c6d0dae2d" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:40:47.528233005Z I1117 09:40:47.528177       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.033004ms" userAgent="Prometheus/2.46.0" audit-ID="8cb4cd35-ea28-4fa2-a4b0-83c4bf3d9e01" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:41:01.410463174Z I1117 09:41:01.410390       1 insightsreport.go:131] Pulling report from smart-proxy
2025-11-17T09:41:01.410463174Z I1117 09:41:01.410434       1 insightsreport.go:143] Retrieving report
2025-11-17T09:41:01.416426071Z I1117 09:41:01.416354       1 requests.go:111] Retrieving report for cluster: 0cde7b02-99d6-493b-9816-ee792ccc0666
2025-11-17T09:41:01.416426071Z I1117 09:41:01.416366       1 requests.go:112] Endpoint: https://console.redhat.com/api/insights-results-aggregator/v2/cluster/0cde7b02-99d6-493b-9816-ee792ccc0666/reports
2025-11-17T09:41:01.420102027Z I1117 09:41:01.420068       1 requests.go:122] Retrieving report from https://console.redhat.com/api/insights-results-aggregator/v2/cluster/0cde7b02-99d6-493b-9816-ee792ccc0666/reports
2025-11-17T09:41:02.245225476Z I1117 09:41:02.245161       1 insightsreport.go:178] Report retrieved
2025-11-17T09:41:02.245576740Z I1117 09:41:02.245554       1 insightsreport.go:187] Smart Proxy report correctly parsed
2025-11-17T09:41:02.273743396Z I1117 09:41:02.273592       1 insightsreport.go:238] Report retrieved correctly
2025-11-17T09:41:11.439085929Z I1117 09:41:11.439003       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.815935ms" userAgent="Prometheus/2.46.0" audit-ID="d9a2c491-f7ea-4419-8b9e-90107a5c5604" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:41:13.119047909Z I1117 09:41:13.118978       1 controller.go:444] The operator is healthy
2025-11-17T09:41:13.119047909Z I1117 09:41:13.119040       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:41:17.522929308Z I1117 09:41:17.522863       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.425128ms" userAgent="Prometheus/2.46.0" audit-ID="eed14f60-18dd-46bb-b5cb-a73122b720f1" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:41:41.439951598Z I1117 09:41:41.439149       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.073538ms" userAgent="Prometheus/2.46.0" audit-ID="651cd074-5d86-445e-852f-85d09caddeb4" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:41:47.523396413Z I1117 09:41:47.523253       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.018965ms" userAgent="Prometheus/2.46.0" audit-ID="7773bd69-3636-4d1f-a134-b1e13ae36cb6" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:42:11.440221743Z I1117 09:42:11.439373       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.248382ms" userAgent="Prometheus/2.46.0" audit-ID="d799a876-bd62-4ac2-9ce7-5434ee8d79dd" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:42:17.524045611Z I1117 09:42:17.523239       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.530338ms" userAgent="Prometheus/2.46.0" audit-ID="92038ebe-f4b3-49e6-a9c0-c326d93f9397" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:42:41.440176579Z I1117 09:42:41.440118       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="5.462398ms" userAgent="Prometheus/2.46.0" audit-ID="e612b278-56d9-4d60-95cb-fabd7a861f8e" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:42:43.930257426Z I1117 09:42:43.930193       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.FeatureGate total 7 items received
2025-11-17T09:42:47.521782539Z I1117 09:42:47.521725       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.687578ms" userAgent="Prometheus/2.46.0" audit-ID="48da0f4b-62a9-436d-96ec-8ac8f8a9361b" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:42:55.758812044Z I1117 09:42:55.758710       1 secretconfigobserver.go:136] Refreshing configuration from cluster pull secret
2025-11-17T09:42:55.765296103Z I1117 09:42:55.765246       1 secretconfigobserver.go:248] Found cloud.openshift.com token
2025-11-17T09:42:55.765325136Z I1117 09:42:55.765306       1 secretconfigobserver.go:162] Refreshing configuration from cluster secret
2025-11-17T09:42:55.768097621Z I1117 09:42:55.768065       1 secretconfigobserver.go:119] support secret does not exist
2025-11-17T09:43:09.554933690Z I1117 09:43:09.554849       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 8 items received
2025-11-17T09:43:11.438518315Z I1117 09:43:11.438039       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.057995ms" userAgent="Prometheus/2.46.0" audit-ID="5cddc647-2286-4f95-b73c-e2698c9cbdfe" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:43:13.117338290Z I1117 09:43:13.117159       1 controller.go:444] The operator is healthy
2025-11-17T09:43:13.117338290Z I1117 09:43:13.117267       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:43:17.521623184Z I1117 09:43:17.521568       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.525007ms" userAgent="Prometheus/2.46.0" audit-ID="861c7f9d-511a-43d9-be02-d71276f217ff" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:43:28.873767849Z I1117 09:43:28.873670       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.ClusterVersion total 21 items received
2025-11-17T09:43:41.437601697Z I1117 09:43:41.437463       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.268408ms" userAgent="Prometheus/2.46.0" audit-ID="c6891d2e-e6cd-426d-9f7f-740582908f9e" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:43:47.522168434Z I1117 09:43:47.522092       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.830817ms" userAgent="Prometheus/2.46.0" audit-ID="4e75ae6b-e18a-45de-9071-40bf1b160c1a" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:44:11.437932587Z I1117 09:44:11.437870       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.752424ms" userAgent="Prometheus/2.46.0" audit-ID="70c8d91d-2825-4865-bcd0-f644dfcd6f18" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:44:17.521809946Z I1117 09:44:17.521670       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.608228ms" userAgent="Prometheus/2.46.0" audit-ID="1ef676e0-52fa-4cc5-addd-dfaac93bd277" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:44:41.437134509Z I1117 09:44:41.436920       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.307619ms" userAgent="Prometheus/2.46.0" audit-ID="b2daaf20-53ce-47e7-8dbb-5d6e1c640dbe" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:44:47.521851063Z I1117 09:44:47.521802       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.187243ms" userAgent="Prometheus/2.46.0" audit-ID="0c7c92bc-19ac-4c6b-ab28-72f5389829a1" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:45:05.797876194Z I1117 09:45:05.797808       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 9 items received
2025-11-17T09:45:11.439945048Z I1117 09:45:11.439871       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.244291ms" userAgent="Prometheus/2.46.0" audit-ID="3601c791-e494-4cc3-9679-13db76623eec" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:45:13.117501405Z I1117 09:45:13.117440       1 controller.go:444] The operator is healthy
2025-11-17T09:45:13.117501405Z I1117 09:45:13.117492       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:45:17.522981248Z I1117 09:45:17.522893       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.615605ms" userAgent="Prometheus/2.46.0" audit-ID="8b59c965-8092-4821-978a-7488dd79a9d8" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:45:41.437623591Z I1117 09:45:41.437456       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.884631ms" userAgent="Prometheus/2.46.0" audit-ID="8ded8cec-8c6a-4aed-bbd4-5794ba8bbbe8" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:45:47.521799562Z I1117 09:45:47.521733       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.239143ms" userAgent="Prometheus/2.46.0" audit-ID="45c001ee-e8d1-4d0a-acc4-a221b65623e3" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:46:04.607685282Z I1117 09:46:04.607509       1 reflector.go:788] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: Watch close - *v1.ConfigMap total 11 items received
2025-11-17T09:46:11.437533279Z I1117 09:46:11.437491       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.674195ms" userAgent="Prometheus/2.46.0" audit-ID="a0dfabda-1d20-4b00-8912-7711632be57b" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:46:17.521315268Z I1117 09:46:17.521250       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.024468ms" userAgent="Prometheus/2.46.0" audit-ID="1158edc3-35c5-4300-b97e-358fa9c08442" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:46:41.438433678Z I1117 09:46:41.438272       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.020084ms" userAgent="Prometheus/2.46.0" audit-ID="69864718-a872-4406-8d59-5399370a76f6" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:46:45.873805931Z I1117 09:46:45.873731       1 reflector.go:376] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: forcing resync
2025-11-17T09:46:45.929935955Z I1117 09:46:45.929874       1 reflector.go:376] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: forcing resync
2025-11-17T09:46:47.523899418Z I1117 09:46:47.523848       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="6.442903ms" userAgent="Prometheus/2.46.0" audit-ID="b6d497a0-ae2c-4858-9faa-0f9dd9d02eac" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:46:52.763463069Z I1117 09:46:52.762328       1 reflector.go:788] k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader_controller.go:172: Watch close - *v1.ConfigMap total 0 items received
2025-11-17T09:46:52.763463069Z I1117 09:46:52.763051       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 1 items received
2025-11-17T09:46:52.763463069Z I1117 09:46:52.763409       1 reflector.go:788] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Watch close - *v1.ConfigMap total 3 items received
2025-11-17T09:46:52.798369239Z I1117 09:46:52.798209       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.FeatureGate total 4 items received
2025-11-17T09:46:52.809063515Z I1117 09:46:52.805809       1 reflector.go:788] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Watch close - *v1.ClusterVersion total 3 items received
2025-11-17T09:47:11.437947649Z I1117 09:47:11.437889       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.911187ms" userAgent="Prometheus/2.46.0" audit-ID="95a72429-5b67-4631-802b-9e9599b1e6c2" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:47:13.119475156Z I1117 09:47:13.119415       1 controller.go:444] The operator is healthy
2025-11-17T09:47:13.119524642Z I1117 09:47:13.119499       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:47:17.521416497Z I1117 09:47:17.521359       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.739605ms" userAgent="Prometheus/2.46.0" audit-ID="0eb26971-23c4-4024-942f-2f1c0c562e14" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:47:41.439017382Z I1117 09:47:41.438953       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="5.179573ms" userAgent="Prometheus/2.46.0" audit-ID="259b28b7-08d6-467d-a78f-ab6b52af445a" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:47:47.521346568Z I1117 09:47:47.521303       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.799093ms" userAgent="Prometheus/2.46.0" audit-ID="c2bb695a-45cd-4867-b782-5b8130a2e5c8" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:47:55.768587425Z I1117 09:47:55.768528       1 secretconfigobserver.go:136] Refreshing configuration from cluster pull secret
2025-11-17T09:47:55.774565989Z I1117 09:47:55.774380       1 secretconfigobserver.go:248] Found cloud.openshift.com token
2025-11-17T09:47:55.774565989Z I1117 09:47:55.774436       1 secretconfigobserver.go:162] Refreshing configuration from cluster secret
2025-11-17T09:47:55.776879386Z I1117 09:47:55.776830       1 secretconfigobserver.go:119] support secret does not exist
2025-11-17T09:48:11.437139523Z I1117 09:48:11.436963       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.100306ms" userAgent="Prometheus/2.46.0" audit-ID="f5f46895-cb7b-4d2a-ab52-18a3cafdfe0c" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:48:17.522254798Z I1117 09:48:17.522112       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.853475ms" userAgent="Prometheus/2.46.0" audit-ID="b0d56637-92b0-4748-b67a-f41ce2937c54" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:48:41.437112582Z I1117 09:48:41.437029       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.24057ms" userAgent="Prometheus/2.46.0" audit-ID="07ed30dd-c912-401d-86a7-40cfe220a5e4" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:48:47.521055419Z I1117 09:48:47.520981       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.223713ms" userAgent="Prometheus/2.46.0" audit-ID="5a3e6026-4143-4eb2-abd2-0d1209740ea9" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:49:11.436865463Z I1117 09:49:11.436782       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.483611ms" userAgent="Prometheus/2.46.0" audit-ID="a7933ef7-b609-4da0-9d46-807587d58b99" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:49:13.118941169Z I1117 09:49:13.118794       1 controller.go:444] The operator is healthy
2025-11-17T09:49:13.118941169Z I1117 09:49:13.118871       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:49:17.521199649Z I1117 09:49:17.521130       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.558466ms" userAgent="Prometheus/2.46.0" audit-ID="e48e31f0-2993-4f2a-a480-306aff96aedd" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:49:41.437795215Z I1117 09:49:41.437602       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.835002ms" userAgent="Prometheus/2.46.0" audit-ID="d5d2eb29-1272-4e39-aa95-df95f2e587e5" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:49:47.522153428Z I1117 09:49:47.522091       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.698664ms" userAgent="Prometheus/2.46.0" audit-ID="ced13d23-f299-4bf1-85ad-254841ba9421" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:50:11.438680745Z I1117 09:50:11.438508       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.514062ms" userAgent="Prometheus/2.46.0" audit-ID="5ce0d66b-b280-4ea5-a830-8dffdfe658f7" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:50:17.522557948Z I1117 09:50:17.522480       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.236217ms" userAgent="Prometheus/2.46.0" audit-ID="1333636c-bdff-4f82-a91a-ebd71da9ad3a" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:50:41.439134099Z I1117 09:50:41.439063       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="5.754363ms" userAgent="Prometheus/2.46.0" audit-ID="e6d506b1-2b3e-416d-8949-9c1feb70b2f9" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:50:47.521304464Z I1117 09:50:47.521234       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="3.553373ms" userAgent="Prometheus/2.46.0" audit-ID="7e89ebed-6709-4e39-a1a4-1c8db89a100a" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:51:11.437477970Z I1117 09:51:11.437410       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.521645ms" userAgent="Prometheus/2.46.0" audit-ID="f90bdf9f-8899-4b4e-9230-87f54a3648ce" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:51:13.119255124Z I1117 09:51:13.119193       1 controller.go:444] The operator is healthy
2025-11-17T09:51:13.119312202Z I1117 09:51:13.119260       1 controller.go:325] No status update necessary, objects are identical
2025-11-17T09:51:17.522250921Z I1117 09:51:17.522187       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.043995ms" userAgent="Prometheus/2.46.0" audit-ID="c0c08a49-cf2f-433e-a83d-772b898181ff" srcIP="10.128.2.14:51622" resp=200
2025-11-17T09:51:41.438572851Z I1117 09:51:41.438276       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.287416ms" userAgent="Prometheus/2.46.0" audit-ID="4c8c2c3b-b71c-4b38-8f51-3ed741c41093" srcIP="10.131.0.18:53400" resp=200
2025-11-17T09:51:47.522534556Z I1117 09:51:47.522471       1 httplog.go:132] "HTTP" verb="GET" URI="/metrics" latency="4.490071ms" userAgent="Prometheus/2.46.0" audit-ID="b2681b16-717a-4cdb-af97-9c2b14e39d1e" srcIP="10.128.2.14:51622" resp=200
