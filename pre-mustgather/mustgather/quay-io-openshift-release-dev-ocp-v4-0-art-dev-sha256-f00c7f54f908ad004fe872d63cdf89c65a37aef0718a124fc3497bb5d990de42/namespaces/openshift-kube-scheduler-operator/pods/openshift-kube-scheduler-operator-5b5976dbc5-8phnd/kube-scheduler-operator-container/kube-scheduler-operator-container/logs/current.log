2025-11-17T09:33:36.992790088Z I1117 09:33:36.992659       1 cmd.go:233] Using service-serving-cert provided certificates
2025-11-17T09:33:36.992790088Z I1117 09:33:36.992766       1 leaderelection.go:122] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2025-11-17T09:33:36.993156755Z I1117 09:33:36.993140       1 observer_polling.go:159] Starting file observer
2025-11-17T09:33:37.010527790Z I1117 09:33:37.010496       1 builder.go:271] openshift-cluster-kube-scheduler-operator version 4.14.0-202511060117.p2.g33f630d.assembly.stream.el8-33f630d-33f630dc1f890ca59c5e57fb5b6cc24a3f22a1d4
2025-11-17T09:33:37.392265165Z I1117 09:33:37.392225       1 secure_serving.go:57] Forcing use of http/1.1 only
2025-11-17T09:33:37.392331976Z W1117 09:33:37.392322       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2025-11-17T09:33:37.392354482Z W1117 09:33:37.392348       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2025-11-17T09:33:37.395409585Z I1117 09:33:37.395384       1 leaderelection.go:245] attempting to acquire leader lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock...
2025-11-17T09:33:37.395591565Z I1117 09:33:37.395576       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2025-11-17T09:33:37.395607577Z I1117 09:33:37.395599       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
2025-11-17T09:33:37.395655795Z I1117 09:33:37.395645       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2025-11-17T09:33:37.395660584Z I1117 09:33:37.395655       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-11-17T09:33:37.395760512Z I1117 09:33:37.395720       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2025-11-17T09:33:37.395777490Z I1117 09:33:37.395766       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-11-17T09:33:37.395976138Z I1117 09:33:37.395965       1 secure_serving.go:213] Serving securely on [::]:8443
2025-11-17T09:33:37.396021399Z I1117 09:33:37.396013       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2025-11-17T09:33:37.396075436Z I1117 09:33:37.396017       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2025-11-17T09:33:37.401862284Z I1117 09:33:37.401833       1 leaderelection.go:255] successfully acquired lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock
2025-11-17T09:33:37.403031834Z I1117 09:33:37.403006       1 event.go:298] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-cluster-kube-scheduler-operator-lock", UID:"69cb67e8-e60a-47cc-9554-63ec3c014b72", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"20134", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' openshift-kube-scheduler-operator-5b5976dbc5-8phnd_86b6d3ee-3b36-453b-8d66-cae63a45dd89 became leader
2025-11-17T09:33:37.403055359Z I1117 09:33:37.403044       1 simple_featuregate_reader.go:171] Starting feature-gate-detector
2025-11-17T09:33:37.404589186Z I1117 09:33:37.404563       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'FeatureGatesInitialized' FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AlibabaPlatform", "AzureWorkloadIdentity", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "PrivateHostedZoneAWS"}, Disabled:[]v1.FeatureGateName{"AdminNetworkPolicy", "AdmissionWebhookMatchConditions", "AutomatedEtcdBackup", "CSIDriverSharedResource", "DynamicResourceAllocation", "EventedPLEG", "ExternalCloudProvider", "ExternalCloudProviderGCP", "GCPLabelsTags", "GatewayAPI", "InsightsConfigAPI", "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack", "MaxUnavailableStatefulSet", "NetworkLiveMigration", "NodeSwap", "OpenShiftPodSecurityAdmission", "RetroactiveDefaultStorageClass", "RouteExternalCertificate", "SigstoreImageVerification", "VSphereStaticIPs", "ValidatingAdmissionPolicy"}}
2025-11-17T09:33:37.405243399Z I1117 09:33:37.404573       1 starter.go:77] FeatureGates initialized: knownFeatureGates=[AdminNetworkPolicy AdmissionWebhookMatchConditions AlibabaPlatform AutomatedEtcdBackup AzureWorkloadIdentity BuildCSIVolumes CSIDriverSharedResource CloudDualStackNodeIPs DynamicResourceAllocation EventedPLEG ExternalCloudProvider ExternalCloudProviderAzure ExternalCloudProviderExternal ExternalCloudProviderGCP GCPLabelsTags GatewayAPI InsightsConfigAPI MachineAPIOperatorDisableMachineHealthCheckController MachineAPIProviderOpenStack MaxUnavailableStatefulSet NetworkLiveMigration NodeSwap OpenShiftPodSecurityAdmission PrivateHostedZoneAWS RetroactiveDefaultStorageClass RouteExternalCertificate SigstoreImageVerification VSphereStaticIPs ValidatingAdmissionPolicy]
2025-11-17T09:33:37.413644881Z I1117 09:33:37.413609       1 base_controller.go:67] Waiting for caches to sync for RemoveStaleConditionsController
2025-11-17T09:33:37.417043605Z I1117 09:33:37.416938       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2025-11-17T09:33:37.417129615Z I1117 09:33:37.417110       1 base_controller.go:67] Waiting for caches to sync for KubeControllerManagerStaticResources
2025-11-17T09:33:37.417136292Z I1117 09:33:37.417130       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2025-11-17T09:33:37.417152319Z I1117 09:33:37.417140       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2025-11-17T09:33:37.417157241Z I1117 09:33:37.417152       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2025-11-17T09:33:37.417248223Z I1117 09:33:37.417233       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_kube-scheduler
2025-11-17T09:33:37.417427257Z I1117 09:33:37.417410       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2025-11-17T09:33:37.417493549Z I1117 09:33:37.417479       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2025-11-17T09:33:37.417499045Z I1117 09:33:37.417494       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2025-11-17T09:33:37.417512132Z I1117 09:33:37.417503       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2025-11-17T09:33:37.417516763Z I1117 09:33:37.417512       1 base_controller.go:67] Waiting for caches to sync for PruneController
2025-11-17T09:33:37.417529039Z I1117 09:33:37.417522       1 base_controller.go:67] Waiting for caches to sync for NodeController
2025-11-17T09:33:37.417599674Z I1117 09:33:37.417589       1 base_controller.go:67] Waiting for caches to sync for BackingResourceController
2025-11-17T09:33:37.417605561Z I1117 09:33:37.417602       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2025-11-17T09:33:37.417619299Z I1117 09:33:37.417611       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2025-11-17T09:33:37.417624790Z I1117 09:33:37.417621       1 base_controller.go:67] Waiting for caches to sync for GuardController
2025-11-17T09:33:37.495847190Z I1117 09:33:37.495796       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-11-17T09:33:37.495877122Z I1117 09:33:37.495867       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
2025-11-17T09:33:37.496360943Z I1117 09:33:37.496341       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-11-17T09:33:37.514610595Z I1117 09:33:37.514547       1 base_controller.go:73] Caches are synced for RemoveStaleConditionsController 
2025-11-17T09:33:37.514610595Z I1117 09:33:37.514566       1 base_controller.go:110] Starting #1 XXXXXX of RemoveStaleConditionsController controller ...
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517827       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517838       1 base_controller.go:73] Caches are synced for InstallerController 
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517843       1 base_controller.go:110] Starting #1 XXXXXX of MissingStaticPodController controller ...
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517865       1 base_controller.go:73] Caches are synced for KubeControllerManagerStaticResources 
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517868       1 base_controller.go:73] Caches are synced for InstallerStateController 
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517871       1 base_controller.go:110] Starting #1 XXXXXX of KubeControllerManagerStaticResources controller ...
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517880       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517884       1 base_controller.go:110] Starting #1 XXXXXX of StaticPodStateController controller ...
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517897       1 base_controller.go:73] Caches are synced for StatusSyncer_kube-scheduler 
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517898       1 base_controller.go:73] Caches are synced for PruneController 
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517902       1 base_controller.go:110] Starting #1 XXXXXX of PruneController controller ...
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517903       1 base_controller.go:110] Starting #1 XXXXXX of StatusSyncer_kube-scheduler controller ...
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517916       1 base_controller.go:73] Caches are synced for RevisionController 
2025-11-17T09:33:37.517962403Z I1117 09:33:37.517919       1 base_controller.go:110] Starting #1 XXXXXX of RevisionController controller ...
2025-11-17T09:33:37.518095099Z I1117 09:33:37.517872       1 base_controller.go:110] Starting #1 XXXXXX of InstallerStateController controller ...
2025-11-17T09:33:37.518864283Z I1117 09:33:37.518845       1 base_controller.go:73] Caches are synced for BackingResourceController 
2025-11-17T09:33:37.518864283Z I1117 09:33:37.518856       1 base_controller.go:110] Starting #1 XXXXXX of BackingResourceController controller ...
2025-11-17T09:33:37.518873782Z I1117 09:33:37.518866       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2025-11-17T09:33:37.518873782Z I1117 09:33:37.518869       1 base_controller.go:110] Starting #1 XXXXXX of UnsupportedConfigOverridesController controller ...
2025-11-17T09:33:37.519074089Z I1117 09:33:37.519058       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2025-11-17T09:33:37.519074089Z I1117 09:33:37.519067       1 base_controller.go:110] Starting #1 XXXXXX of LoggingSyncer controller ...
2025-11-17T09:33:37.519363419Z I1117 09:33:37.517848       1 base_controller.go:110] Starting #1 XXXXXX of InstallerController controller ...
2025-11-17T09:33:37.627004519Z I1117 09:33:37.618140       1 base_controller.go:73] Caches are synced for GuardController 
2025-11-17T09:33:37.627004519Z I1117 09:33:37.618155       1 base_controller.go:110] Starting #1 XXXXXX of GuardController controller ...
2025-11-17T09:33:37.627004519Z I1117 09:33:37.618648       1 base_controller.go:73] Caches are synced for NodeController 
2025-11-17T09:33:37.627004519Z I1117 09:33:37.618656       1 base_controller.go:110] Starting #1 XXXXXX of NodeController controller ...
2025-11-17T09:33:37.627004519Z I1117 09:33:37.619577       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MasterNodeObserved' Observed new XXXXXX node XXXXXX0
2025-11-17T09:33:37.633967034Z I1117 09:33:37.633942       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:33:37.634306131Z I1117 09:33:37.634276       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodDisruptionBudgetUpdated' Updated PodDisruptionBudget.policy/openshift-kube-scheduler-guard-pdb -n openshift-kube-scheduler because it changed
2025-11-17T09:33:37.636413974Z I1117 09:33:37.636391       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:30:35Z","message":"TargetConfigControllerDegraded: \"configmap\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config\": dial tcp 172.30.0.1:443: connect: connection refused\nTargetConfigControllerDegraded: \"configmap/serviceaccount-ca\": configmaps \"serviceaccount-ca\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\": serviceaccounts \"localhost-recovery-client\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"configmap/scheduler-kubeconfig\": configmaps \"scheduler-kubeconfig\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nNodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady ([container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes \"XXXXXX0\" not found])","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 2 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:33:37.647655740Z I1117 09:33:37.647601       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "TargetConfigControllerDegraded: \"configmap\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config\": dial tcp 172.30.0.1:443: connect: connection refused\nTargetConfigControllerDegraded: \"configmap/serviceaccount-ca\": configmaps \"serviceaccount-ca\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\": serviceaccounts \"localhost-recovery-client\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"configmap/scheduler-kubeconfig\": configmaps \"scheduler-kubeconfig\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nNodeControllerDegraded: All XXXXXX nodes are ready" to "TargetConfigControllerDegraded: \"configmap\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config\": dial tcp 172.30.0.1:443: connect: connection refused\nTargetConfigControllerDegraded: \"configmap/serviceaccount-ca\": configmaps \"serviceaccount-ca\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\": serviceaccounts \"localhost-recovery-client\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"configmap/scheduler-kubeconfig\": configmaps \"scheduler-kubeconfig\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nNodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady ([container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes \"XXXXXX0\" not found])"
2025-11-17T09:33:38.317404122Z I1117 09:33:38.317335       1 base_controller.go:73] Caches are synced for ConfigObserver 
2025-11-17T09:33:38.317404122Z I1117 09:33:38.317363       1 base_controller.go:110] Starting #1 XXXXXX of ConfigObserver controller ...
2025-11-17T09:33:38.517770559Z I1117 09:33:38.517633       1 base_controller.go:73] Caches are synced for TargetConfigController 
2025-11-17T09:33:38.517770559Z I1117 09:33:38.517662       1 base_controller.go:110] Starting #1 XXXXXX of TargetConfigController controller ...
2025-11-17T09:33:38.517770559Z I1117 09:33:38.517689       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2025-11-17T09:33:38.517770559Z I1117 09:33:38.517693       1 base_controller.go:110] Starting #1 XXXXXX of ResourceSyncController controller ...
2025-11-17T09:33:38.614441256Z I1117 09:33:38.614360       1 request.go:696] Waited for 1.095907339s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2025-11-17T09:33:39.614763400Z I1117 09:33:39.614711       1 request.go:696] Waited for 2.09361505s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX1
2025-11-17T09:33:40.814650731Z I1117 09:33:40.814453       1 request.go:696] Waited for 1.593163226s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2025-11-17T09:33:41.814665546Z I1117 09:33:41.814467       1 request.go:696] Waited for 1.5976061s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa
2025-11-17T09:33:42.416278879Z I1117 09:33:42.416150       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2025-11-17T09:33:42.425182355Z I1117 09:33:42.425145       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:30:35Z","message":"TargetConfigControllerDegraded: \"configmap\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config\": dial tcp 172.30.0.1:443: connect: connection refused\nTargetConfigControllerDegraded: \"configmap/serviceaccount-ca\": configmaps \"serviceaccount-ca\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\": serviceaccounts \"localhost-recovery-client\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"configmap/scheduler-kubeconfig\": configmaps \"scheduler-kubeconfig\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nNodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady ([container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes \"XXXXXX0\" not found])","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:33:42.433959315Z I1117 09:33:42.431720       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 7; 0 nodes have achieved new revision 8" to "NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8",Available message changed from "StaticPodsAvailable: 2 nodes are active; 2 nodes are at revision 7; 0 nodes have achieved new revision 8" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8"
2025-11-17T09:33:42.814772401Z I1117 09:33:42.814672       1 request.go:696] Waited for 1.396510983s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-XXXXXX2
2025-11-17T09:33:44.015328240Z I1117 09:33:44.014936       1 request.go:696] Waited for 1.397378517s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX2
2025-11-17T09:33:44.220048257Z I1117 09:33:44.220000       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:33:45.213939992Z I1117 09:33:45.213872       1 request.go:696] Waited for 1.195810495s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX0
2025-11-17T09:33:45.556252833Z I1117 09:33:45.555397       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:30:35Z","message":"TargetConfigControllerDegraded: \"configmap\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config\": dial tcp 172.30.0.1:443: connect: connection refused\nTargetConfigControllerDegraded: \"configmap/serviceaccount-ca\": configmaps \"serviceaccount-ca\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\": serviceaccounts \"localhost-recovery-client\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"configmap/scheduler-kubeconfig\": configmaps \"scheduler-kubeconfig\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nNodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:33:45.569022991Z I1117 09:33:45.568484       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "TargetConfigControllerDegraded: \"configmap\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config\": dial tcp 172.30.0.1:443: connect: connection refused\nTargetConfigControllerDegraded: \"configmap/serviceaccount-ca\": configmaps \"serviceaccount-ca\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\": serviceaccounts \"localhost-recovery-client\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"configmap/scheduler-kubeconfig\": configmaps \"scheduler-kubeconfig\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nNodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady ([container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?, failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes \"XXXXXX0\" not found])" to "TargetConfigControllerDegraded: \"configmap\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config\": dial tcp 172.30.0.1:443: connect: connection refused\nTargetConfigControllerDegraded: \"configmap/serviceaccount-ca\": configmaps \"serviceaccount-ca\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\": serviceaccounts \"localhost-recovery-client\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"configmap/scheduler-kubeconfig\": configmaps \"scheduler-kubeconfig\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nNodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)"
2025-11-17T09:33:46.025292567Z I1117 09:33:46.025190       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-XXXXXX0 -n openshift-kube-scheduler because it was missing
2025-11-17T09:33:46.214893154Z I1117 09:33:46.214538       1 request.go:696] Waited for 1.196824208s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX1
2025-11-17T09:33:46.217421578Z I1117 09:33:46.217382       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2025-11-17T09:33:47.414187053Z I1117 09:33:47.414124       1 request.go:696] Waited for 1.389038733s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-11-17T09:33:48.414952258Z I1117 09:33:48.414730       1 request.go:696] Waited for 1.395177273s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod
2025-11-17T09:33:48.427566365Z I1117 09:33:48.427512       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:30:35Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:33:48.435496049Z I1117 09:33:48.435090       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "TargetConfigControllerDegraded: \"configmap\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config\": dial tcp 172.30.0.1:443: connect: connection refused\nTargetConfigControllerDegraded: \"configmap/serviceaccount-ca\": configmaps \"serviceaccount-ca\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\": serviceaccounts \"localhost-recovery-client\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nTargetConfigControllerDegraded: \"configmap/scheduler-kubeconfig\": configmaps \"scheduler-kubeconfig\" is forbidden: User \"system:serviceaccount:openshift-kube-scheduler-operator:openshift-kube-scheduler-operator\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"openshift-kube-scheduler\"\nNodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)" to "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)"
2025-11-17T09:33:49.614209670Z I1117 09:33:49.614138       1 request.go:696] Waited for 1.397007647s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-XXXXXX2
2025-11-17T09:33:50.614693561Z I1117 09:33:50.614650       1 request.go:696] Waited for 1.597299639s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX0
2025-11-17T09:33:50.817856348Z I1117 09:33:50.817797       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2025-11-17T09:33:51.814009712Z I1117 09:33:51.813954       1 request.go:696] Waited for 1.19623863s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX1
2025-11-17T09:33:53.014840450Z I1117 09:33:53.014782       1 request.go:696] Waited for 1.197349293s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX2
2025-11-17T09:33:54.214130934Z I1117 09:33:54.213947       1 request.go:696] Waited for 1.393729614s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2025-11-17T09:33:54.819967249Z I1117 09:33:54.819913       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:33:55.214907244Z I1117 09:33:55.214810       1 request.go:696] Waited for 1.398570561s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-XXXXXX2
2025-11-17T09:33:55.218196031Z I1117 09:33:55.218096       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:33:55.221687996Z I1117 09:33:55.221502       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:33:56.414313596Z I1117 09:33:56.414264       1 request.go:696] Waited for 1.192132014s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-XXXXXX1
2025-11-17T09:33:58.016980778Z I1117 09:33:58.016921       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:33:59.014148905Z I1117 09:33:59.014093       1 request.go:696] Waited for 1.008183409s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-11-17T09:33:59.620078509Z I1117 09:33:59.619948       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:00.817321769Z I1117 09:34:00.817172       1 installer_controller.go:512] "XXXXXX1" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:34:41.988521273Z I1117 09:34:41.988478       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:41.996698382Z I1117 09:34:41.996658       1 installer_controller.go:500] "XXXXXX1" moving to (v1.NodeStatus) {
2025-11-17T09:34:41.996698382Z  NodeName: (string) (len=7) "XXXXXX1",
2025-11-17T09:34:41.996698382Z  CurrentRevision: (int32) 8,
2025-11-17T09:34:41.996698382Z  TargetRevision: (int32) 0,
2025-11-17T09:34:41.996698382Z  LastFailedRevision: (int32) 0,
2025-11-17T09:34:41.996698382Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:34:41.996698382Z  LastFailedReason: (string) "",
2025-11-17T09:34:41.996698382Z  LastFailedCount: (int) 0,
2025-11-17T09:34:41.996698382Z  LastFallbackCount: (int) 0,
2025-11-17T09:34:41.996698382Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:34:41.996698382Z }
2025-11-17T09:34:41.996698382Z  because static pod is ready
2025-11-17T09:34:42.007564131Z I1117 09:34:42.007258       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:30:35Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:34:42.007564131Z I1117 09:34:42.007314       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX1" from revision 7 to 8 because static pod is ready
2025-11-17T09:34:42.014639045Z I1117 09:34:42.014597       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8" to "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 7; 0 nodes have achieved new revision 8" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8"
2025-11-17T09:34:43.185115478Z I1117 09:34:43.185035       1 request.go:696] Waited for 1.177527615s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2025-11-17T09:34:44.789525400Z I1117 09:34:44.789465       1 installer_controller.go:524] node XXXXXX0 static pod not found and needs new revision 8
2025-11-17T09:34:44.789556539Z I1117 09:34:44.789528       1 installer_controller.go:532] "XXXXXX0" moving to (v1.NodeStatus) {
2025-11-17T09:34:44.789556539Z  NodeName: (string) (len=7) "XXXXXX0",
2025-11-17T09:34:44.789556539Z  CurrentRevision: (int32) 0,
2025-11-17T09:34:44.789556539Z  TargetRevision: (int32) 8,
2025-11-17T09:34:44.789556539Z  LastFailedRevision: (int32) 0,
2025-11-17T09:34:44.789556539Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:34:44.789556539Z  LastFailedReason: (string) "",
2025-11-17T09:34:44.789556539Z  LastFailedCount: (int) 0,
2025-11-17T09:34:44.789556539Z  LastFallbackCount: (int) 0,
2025-11-17T09:34:44.789556539Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:34:44.789556539Z }
2025-11-17T09:34:44.798439330Z I1117 09:34:44.798357       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "XXXXXX0" from revision 0 to 8 because node XXXXXX0 static pod not found
2025-11-17T09:34:45.985656577Z I1117 09:34:45.985603       1 request.go:696] Waited for 1.180062847s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX0
2025-11-17T09:34:46.993513368Z I1117 09:34:46.993447       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-XXXXXX0 -n openshift-kube-scheduler because it was missing
2025-11-17T09:34:48.185234915Z I1117 09:34:48.185115       1 request.go:696] Waited for 1.191350815s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX0
2025-11-17T09:34:48.188267558Z I1117 09:34:48.188236       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:34:49.186046197Z I1117 09:34:49.185897       1 request.go:696] Waited for 1.397647109s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod
2025-11-17T09:34:49.788061745Z I1117 09:34:49.788008       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:34:50.386047322Z I1117 09:34:50.385910       1 request.go:696] Waited for 1.196096716s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2025-11-17T09:34:50.589054055Z I1117 09:34:50.589009       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:34:51.592824897Z I1117 09:34:51.592777       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:35:37.519373247Z I1117 09:35:37.519234       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:35:37Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:35:44.526935802Z E1117 09:35:44.526816       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded
2025-11-17T09:35:44.533501398Z I1117 09:35:44.533468       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:35:44Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:35:51.539302832Z E1117 09:35:51.539158       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded
2025-11-17T09:35:51.550003165Z I1117 09:35:51.549964       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:35:51Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:35:58.557194624Z E1117 09:35:58.557074       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: rpc error: code = DeadlineExceeded desc = context deadline exceeded
2025-11-17T09:35:58.578082503Z I1117 09:35:58.577976       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:35:58Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:01.591324392Z I1117 09:36:01.589958       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded changed from False to True ("NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)")
2025-11-17T09:36:01.723074549Z I1117 09:36:01.722766       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:36:01Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:01.744919896Z E1117 09:36:01.742616       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:36:01.744919896Z I1117 09:36:01.743092       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:36:01Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:01.754428172Z E1117 09:36:01.754123       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:36:01.756181563Z I1117 09:36:01.754651       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:36:01Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:01.769321926Z E1117 09:36:01.766658       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:36:01.769321926Z I1117 09:36:01.767306       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:36:01Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:01.778367085Z E1117 09:36:01.774759       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:36:01.789458626Z I1117 09:36:01.789264       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:36:01Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:01.794503679Z E1117 09:36:01.793817       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:36:01.875328358Z I1117 09:36:01.874847       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:36:01Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:01.891392251Z E1117 09:36:01.886622       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:36:02.047500451Z I1117 09:36:02.047454       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:36:02Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:02.051156774Z E1117 09:36:02.051118       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:36:02.372681705Z I1117 09:36:02.372567       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:36:02Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:02.377242889Z E1117 09:36:02.377213       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:36:02.610055985Z I1117 09:36:02.610004       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:02.807336720Z I1117 09:36:02.801712       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:36:02Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:36:02.812849886Z E1117 09:36:02.812390       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:36:03.836511861Z I1117 09:36:03.834482       1 request.go:696] Waited for 1.040994208s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces?resourceVersion=20861
2025-11-17T09:36:05.035576439Z I1117 09:36:05.035504       1 request.go:696] Waited for 2.169353672s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/secrets?resourceVersion=20866
2025-11-17T09:36:06.233334495Z I1117 09:36:06.233257       1 request.go:696] Waited for 3.005398343s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps?resourceVersion=20857
2025-11-17T09:36:07.434149331Z I1117 09:36:07.432990       1 request.go:696] Waited for 3.386291392s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX0
2025-11-17T09:36:07.439186002Z I1117 09:36:07.438831       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:36:08.434875109Z I1117 09:36:08.434504       1 request.go:696] Waited for 3.198473256s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-11-17T09:36:09.633276892Z I1117 09:36:09.633218       1 request.go:696] Waited for 1.395669236s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-XXXXXX2
2025-11-17T09:36:10.440646060Z I1117 09:36:10.440073       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:36:10.635172024Z I1117 09:36:10.634076       1 request.go:696] Waited for 1.398232556s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX1
2025-11-17T09:36:13.035244427Z I1117 09:36:13.035192       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:37.251495108Z I1117 09:36:37.237042       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:37.266333618Z I1117 09:36:37.266219       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:37.331008599Z I1117 09:36:37.330959       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:46.249014950Z I1117 09:36:46.248945       1 request.go:696] Waited for 1.054663477s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps?resourceVersion=21182
2025-11-17T09:36:47.253094900Z I1117 09:36:47.252172       1 request.go:696] Waited for 1.898393702s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX1
2025-11-17T09:36:48.450614567Z I1117 09:36:48.449720       1 request.go:696] Waited for 2.761091042s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config/secrets?resourceVersion=21068
2025-11-17T09:36:48.855436771Z I1117 09:36:48.855388       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:49.456114909Z I1117 09:36:49.456057       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:36:49.649573963Z I1117 09:36:49.649431       1 request.go:696] Waited for 2.38754423s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX2
2025-11-17T09:36:50.849236110Z I1117 09:36:50.849179       1 request.go:696] Waited for 1.592249525s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX0
2025-11-17T09:36:51.849771327Z I1117 09:36:51.849710       1 request.go:696] Waited for 1.395337536s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-11-17T09:36:52.458323572Z I1117 09:36:52.457508       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:36:52.849801418Z I1117 09:36:52.849741       1 request.go:696] Waited for 1.197999441s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/serviceaccount-ca
2025-11-17T09:36:53.856029101Z I1117 09:36:53.855975       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:55.853270810Z I1117 09:36:55.853217       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:58.876911171Z I1117 09:36:58.876825       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:36:58.879086748Z I1117 09:36:58.879035       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:14.574331325Z I1117 09:37:14.570855       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:15.891633651Z I1117 09:37:15.891052       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:19.098024630Z I1117 09:37:19.097965       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:19.585030877Z I1117 09:37:19.584975       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:20.904279757Z I1117 09:37:20.903919       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:22.290038263Z I1117 09:37:22.289973       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:24.113453939Z I1117 09:37:24.111657       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:24.591010791Z I1117 09:37:24.590955       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:25.909686457Z I1117 09:37:25.909630       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:29.116914019Z I1117 09:37:29.116846       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:39.830150982Z I1117 09:37:39.830086       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:50.830302946Z I1117 09:37:50.830230       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:51.750767102Z I1117 09:37:51.750708       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:52.087819081Z I1117 09:37:52.087771       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:54.744738810Z I1117 09:37:54.744681       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:54.759094782Z I1117 09:37:54.759038       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:37:54.773813245Z I1117 09:37:54.773757       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:35:58Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nStaticPodsDegraded: pod/openshift-kube-scheduler-XXXXXX2 container \"kube-scheduler-cert-syncer\" is terminated: Error: 7.7/tools/cache/reflector.go:231: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E1117 09:36:48.075357       1 reflector.go:148] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W1117 09:37:15.325356       1 reflector.go:533] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E1117 09:37:15.325561       1 reflector.go:148] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W1117 09:37:18.406417       1 reflector.go:533] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E1117 09:37:18.406455       1 reflector.go:148] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: F1117 09:37:54.347403       1 base_controller.go:96] unable to sync caches for CertSyncController\nStaticPodsDegraded: ","reason":"NodeController_MasterNodesReady::StaticPods_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:37:54.783573804Z I1117 09:37:54.783504       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)" to "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nStaticPodsDegraded: pod/openshift-kube-scheduler-XXXXXX2 container \"kube-scheduler-cert-syncer\" is terminated: Error: 7.7/tools/cache/reflector.go:231: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E1117 09:36:48.075357       1 reflector.go:148] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W1117 09:37:15.325356       1 reflector.go:533] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E1117 09:37:15.325561       1 reflector.go:148] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W1117 09:37:18.406417       1 reflector.go:533] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E1117 09:37:18.406455       1 reflector.go:148] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: F1117 09:37:54.347403       1 base_controller.go:96] unable to sync caches for CertSyncController\nStaticPodsDegraded: "
2025-11-17T09:37:55.720902160Z I1117 09:37:55.720840       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:37:55.939726708Z I1117 09:37:55.939564       1 request.go:696] Waited for 1.165058802s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2025-11-17T09:37:56.542028581Z I1117 09:37:56.541978       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:37:57.139214597Z I1117 09:37:57.139163       1 request.go:696] Waited for 1.387905088s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2025-11-17T09:37:57.751103549Z I1117 09:37:57.751048       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:35:58Z","message":"NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)","reason":"NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:37:57.758043399Z I1117 09:37:57.757985       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)\nStaticPodsDegraded: pod/openshift-kube-scheduler-XXXXXX2 container \"kube-scheduler-cert-syncer\" is terminated: Error: 7.7/tools/cache/reflector.go:231: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E1117 09:36:48.075357       1 reflector.go:148] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W1117 09:37:15.325356       1 reflector.go:533] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E1117 09:37:15.325561       1 reflector.go:148] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W1117 09:37:18.406417       1 reflector.go:533] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E1117 09:37:18.406455       1 reflector.go:148] k8s.io/client-go@v0.27.7/tools/cache/reflector.go:231: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: F1117 09:37:54.347403       1 base_controller.go:96] unable to sync caches for CertSyncController\nStaticPodsDegraded: " to "NodeControllerDegraded: The XXXXXX nodes not ready: node \"XXXXXX0\" not ready since 2025-11-17 09:33:35 +0000 UTC because KubeletNotReady (container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your network provider started?)"
2025-11-17T09:37:58.139730804Z I1117 09:37:58.139678       1 request.go:696] Waited for 1.198168415s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-XXXXXX1
2025-11-17T09:37:59.339658307Z I1117 09:37:59.339596       1 request.go:696] Waited for 1.39752192s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX0
2025-11-17T09:37:59.417021565Z I1117 09:37:59.416964       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:00.539937025Z I1117 09:38:00.539891       1 request.go:696] Waited for 1.121944208s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX0
2025-11-17T09:38:00.744116387Z I1117 09:38:00.744064       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:01.739013113Z I1117 09:38:01.738963       1 request.go:696] Waited for 1.194138649s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX0
2025-11-17T09:38:01.741495321Z I1117 09:38:01.741472       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:06.142510172Z I1117 09:38:06.142465       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:06.145177496Z I1117 09:38:06.145149       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:07.543242960Z I1117 09:38:07.543188       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:08.544205648Z I1117 09:38:08.544152       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:08.942412791Z I1117 09:38:08.942362       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:10.342951456Z I1117 09:38:10.342884       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:10.944543487Z I1117 09:38:10.944497       1 guard_controller.go:267] Node XXXXXX0 not ready, skipping reconciling the guard pod
2025-11-17T09:38:21.705854750Z I1117 09:38:21.705783       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:38:21Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:38:21.913376666Z I1117 09:38:21.913304       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded changed from True to False ("NodeControllerDegraded: All XXXXXX nodes are ready")
2025-11-17T09:38:21.913758679Z I1117 09:38:21.913733       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:38:21Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:38:22.204775358Z E1117 09:38:22.204710       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:38:22.205807388Z I1117 09:38:22.205741       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:38:22Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:38:22.429167647Z I1117 09:38:22.429108       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:22.437821315Z E1117 09:38:22.437776       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:22.438259292Z E1117 09:38:22.438229       1 base_controller.go:268] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the XXXXXX version and try again
2025-11-17T09:38:23.769838960Z E1117 09:38:23.769777       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:23.789103418Z I1117 09:38:23.789037       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:38:21Z","message":"GuardControllerDegraded: Missing operand on node XXXXXX0\nNodeControllerDegraded: All XXXXXX nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:38:23.905944621Z E1117 09:38:23.905880       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:23.909421901Z I1117 09:38:23.909366       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All XXXXXX nodes are ready" to "GuardControllerDegraded: Missing operand on node XXXXXX0\nNodeControllerDegraded: All XXXXXX nodes are ready"
2025-11-17T09:38:24.906675199Z I1117 09:38:24.906607       1 request.go:696] Waited for 1.097007678s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2025-11-17T09:38:25.908754934Z I1117 09:38:25.908697       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:26.106718200Z I1117 09:38:26.106650       1 request.go:696] Waited for 1.19729757s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod
2025-11-17T09:38:27.909971996Z E1117 09:38:27.909910       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:27.913427222Z E1117 09:38:27.913391       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:29.909526902Z E1117 09:38:29.909471       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:29.912027263Z E1117 09:38:29.911998       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:30.909018100Z E1117 09:38:30.908965       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:30.912005688Z E1117 09:38:30.911962       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:31.712623499Z E1117 09:38:31.712559       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:32.109466592Z E1117 09:38:32.109409       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:32.909504391Z E1117 09:38:32.909437       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:32.911492018Z E1117 09:38:32.911452       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:34.509409563Z E1117 09:38:34.509352       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:34.511370944Z E1117 09:38:34.511344       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:35.509937406Z E1117 09:38:35.509735       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:35.514760244Z E1117 09:38:35.514710       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:36.309404705Z E1117 09:38:36.309352       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:36.311441695Z E1117 09:38:36.311422       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:37.108844001Z E1117 09:38:37.108795       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:37.593786288Z E1117 09:38:37.593719       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:40.309406181Z E1117 09:38:40.309336       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:41.999710343Z E1117 09:38:41.999656       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:42.000008238Z E1117 09:38:41.999822       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:44.002245968Z E1117 09:38:44.002190       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:44.109621657Z E1117 09:38:44.109553       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:44.527836480Z E1117 09:38:44.527010       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:44.909096938Z E1117 09:38:44.909049       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:45.434560348Z E1117 09:38:45.434373       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:45.709377668Z E1117 09:38:45.709304       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:49.003389333Z E1117 09:38:49.002542       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:49.014678446Z E1117 09:38:49.014632       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:49.539181397Z E1117 09:38:49.538179       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:49.554021952Z E1117 09:38:49.553968       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:51.977264310Z E1117 09:38:51.977019       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:52.544088948Z E1117 09:38:52.544027       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:53.608802907Z E1117 09:38:53.608668       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:54.943229547Z E1117 09:38:54.943124       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:54.945534145Z E1117 09:38:54.945505       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:56.357400610Z E1117 09:38:56.356594       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:57.315767951Z E1117 09:38:57.315590       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:57.356125880Z E1117 09:38:57.356033       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:59.023337536Z E1117 09:38:59.023266       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:59.038468478Z E1117 09:38:59.038414       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:38:59.069115598Z I1117 09:38:59.069037       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:38:59.546512572Z E1117 09:38:59.546448       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:38:59.944745896Z E1117 09:38:59.944599       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:04.641334317Z E1117 09:39:04.641264       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:04.652802320Z E1117 09:39:04.652743       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:05.060865077Z E1117 09:39:05.060704       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:05.073406918Z E1117 09:39:05.073352       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:05.657721484Z E1117 09:39:05.657665       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:05.660524209Z I1117 09:39:05.660414       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:39:06.245903914Z E1117 09:39:06.245791       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:10.074911870Z E1117 09:39:10.074855       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:10.087954385Z E1117 09:39:10.087895       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:11.714853331Z E1117 09:39:11.714802       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:11.715018793Z E1117 09:39:11.715001       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:15.080639470Z E1117 09:39:15.080428       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:15.090780824Z E1117 09:39:15.090745       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:22.610662241Z E1117 09:39:22.610615       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:22.622311506Z E1117 09:39:22.622206       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:24.128261898Z E1117 09:39:24.128070       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:24.148575553Z E1117 09:39:24.148489       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:24.265364848Z E1117 09:39:24.265074       1 guard_controller.go:271] Missing operand on node XXXXXX0
2025-11-17T09:39:24.265364848Z E1117 09:39:24.265259       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node XXXXXX0
2025-11-17T09:39:35.827603356Z E1117 09:39:35.827525       1 guard_controller.go:277] Missing PodIP in operand openshift-kube-scheduler-XXXXXX0 on node XXXXXX0
2025-11-17T09:39:35.832389386Z I1117 09:39:35.832324       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:39:35.850519880Z E1117 09:39:35.850494       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand openshift-kube-scheduler-XXXXXX0 on node XXXXXX0
2025-11-17T09:39:35.851256398Z I1117 09:39:35.851225       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:38:21Z","message":"GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-XXXXXX0 on node XXXXXX0\nNodeControllerDegraded: All XXXXXX nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:39:35.858741897Z I1117 09:39:35.858709       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node XXXXXX0\nNodeControllerDegraded: All XXXXXX nodes are ready" to "GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-XXXXXX0 on node XXXXXX0\nNodeControllerDegraded: All XXXXXX nodes are ready"
2025-11-17T09:39:37.025323026Z I1117 09:39:37.025192       1 request.go:696] Waited for 1.173531056s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2025-11-17T09:39:38.025777041Z I1117 09:39:38.025719       1 request.go:696] Waited for 1.215809842s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-XXXXXX0
2025-11-17T09:39:39.224953273Z I1117 09:39:39.224904       1 request.go:696] Waited for 1.595609548s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX0
2025-11-17T09:39:39.828811234Z I1117 09:39:39.828749       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:40.225684056Z I1117 09:39:40.225586       1 request.go:696] Waited for 1.597070837s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX1
2025-11-17T09:39:40.831146177Z I1117 09:39:40.831081       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-XXXXXX0 -n openshift-kube-scheduler because it was missing
2025-11-17T09:39:41.225662720Z I1117 09:39:41.225583       1 request.go:696] Waited for 1.197338332s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2025-11-17T09:39:42.425619672Z I1117 09:39:42.425556       1 request.go:696] Waited for 1.197142987s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2025-11-17T09:39:43.428857862Z I1117 09:39:43.428786       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:43.625231476Z I1117 09:39:43.625179       1 request.go:696] Waited for 1.194873337s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2025-11-17T09:39:44.625354601Z I1117 09:39:44.625293       1 request.go:696] Waited for 1.195480403s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX0
2025-11-17T09:39:45.625432289Z I1117 09:39:45.625334       1 request.go:696] Waited for 1.196697482s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-XXXXXX2
2025-11-17T09:39:45.837176244Z I1117 09:39:45.837136       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:38:21Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:39:45.848711621Z I1117 09:39:45.848667       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-XXXXXX0 on node XXXXXX0\nNodeControllerDegraded: All XXXXXX nodes are ready" to "NodeControllerDegraded: All XXXXXX nodes are ready"
2025-11-17T09:39:46.428021255Z I1117 09:39:46.427964       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:47.025430345Z I1117 09:39:47.025244       1 request.go:696] Waited for 1.187017192s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX1
2025-11-17T09:39:48.224882041Z I1117 09:39:48.224816       1 request.go:696] Waited for 1.195944575s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX2
2025-11-17T09:39:49.225501588Z I1117 09:39:49.225433       1 request.go:696] Waited for 1.197441087s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa
2025-11-17T09:39:49.633105075Z I1117 09:39:49.632886       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodUpdated' Updated Pod/openshift-kube-scheduler-guard-XXXXXX0 -n openshift-kube-scheduler because it changed
2025-11-17T09:39:50.228050720Z I1117 09:39:50.227905       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:39:50.425257134Z I1117 09:39:50.425082       1 request.go:696] Waited for 1.194397911s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2025-11-17T09:39:51.425567382Z I1117 09:39:51.425421       1 request.go:696] Waited for 1.1964651s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX0
2025-11-17T09:39:53.028558117Z I1117 09:39:53.028500       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:40:00.227867288Z I1117 09:40:00.227804       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:40:03.030666815Z I1117 09:40:03.030611       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:40:05.227675380Z I1117 09:40:05.227603       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:40:07.028507063Z I1117 09:40:07.028068       1 installer_controller.go:512] "XXXXXX0" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:40:26.166758920Z I1117 09:40:26.166687       1 installer_controller.go:500] "XXXXXX0" moving to (v1.NodeStatus) {
2025-11-17T09:40:26.166758920Z  NodeName: (string) (len=7) "XXXXXX0",
2025-11-17T09:40:26.166758920Z  CurrentRevision: (int32) 8,
2025-11-17T09:40:26.166758920Z  TargetRevision: (int32) 0,
2025-11-17T09:40:26.166758920Z  LastFailedRevision: (int32) 0,
2025-11-17T09:40:26.166758920Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:40:26.166758920Z  LastFailedReason: (string) "",
2025-11-17T09:40:26.166758920Z  LastFailedCount: (int) 0,
2025-11-17T09:40:26.166758920Z  LastFallbackCount: (int) 0,
2025-11-17T09:40:26.166758920Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:40:26.166758920Z }
2025-11-17T09:40:26.166758920Z  because static pod is ready
2025-11-17T09:40:26.176342405Z I1117 09:40:26.176259       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX0" from revision 0 to 8 because static pod is ready
2025-11-17T09:40:26.177914616Z I1117 09:40:26.177872       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:38:21Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:30:43Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:40:26.185631365Z I1117 09:40:26.185551       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8" to "NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 7; 1 nodes are at revision 8" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8"
2025-11-17T09:40:27.351880961Z I1117 09:40:27.351646       1 request.go:696] Waited for 1.173296251s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX1
2025-11-17T09:40:28.351741152Z I1117 09:40:28.351694       1 request.go:696] Waited for 1.397980266s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX2
2025-11-17T09:40:29.551075083Z I1117 09:40:29.551021       1 request.go:696] Waited for 1.196496586s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX0
2025-11-17T09:40:31.353965548Z I1117 09:40:31.353901       1 installer_controller.go:524] node XXXXXX2 with revision 7 is the oldest and needs new revision 8
2025-11-17T09:40:31.353965548Z I1117 09:40:31.353953       1 installer_controller.go:532] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:40:31.353965548Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:40:31.353965548Z  CurrentRevision: (int32) 7,
2025-11-17T09:40:31.353965548Z  TargetRevision: (int32) 8,
2025-11-17T09:40:31.353965548Z  LastFailedRevision: (int32) 0,
2025-11-17T09:40:31.353965548Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:40:31.353965548Z  LastFailedReason: (string) "",
2025-11-17T09:40:31.353965548Z  LastFailedCount: (int) 0,
2025-11-17T09:40:31.353965548Z  LastFallbackCount: (int) 0,
2025-11-17T09:40:31.353965548Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:40:31.353965548Z }
2025-11-17T09:40:31.362391212Z I1117 09:40:31.362320       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "XXXXXX2" from revision 7 to 8 because node XXXXXX2 with revision 7 is the oldest
2025-11-17T09:40:32.551779493Z I1117 09:40:32.551734       1 request.go:696] Waited for 1.186445422s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX2
2025-11-17T09:40:33.566513952Z I1117 09:40:33.566448       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-XXXXXX2 -n openshift-kube-scheduler because it was missing
2025-11-17T09:40:34.554933302Z I1117 09:40:34.554863       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-11-17T09:40:34.751111696Z I1117 09:40:34.750976       1 request.go:696] Waited for 1.184455593s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-11-17T09:40:35.751805170Z I1117 09:40:35.751754       1 request.go:696] Waited for 1.195716817s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-8-XXXXXX2
2025-11-17T09:40:36.754656392Z I1117 09:40:36.754596       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:40:38.551962368Z I1117 09:40:38.551838       1 request.go:696] Waited for 1.029640826s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2025-11-17T09:40:39.155323254Z I1117 09:40:39.155233       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:40:39.750975213Z I1117 09:40:39.750861       1 request.go:696] Waited for 1.396271132s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod
2025-11-17T09:41:07.872009367Z I1117 09:41:07.871935       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-11-17T09:41:10.068946399Z I1117 09:41:10.068876       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2025-11-17T09:41:12.067680540Z I1117 09:41:12.067612       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2025-11-17T09:41:20.474669161Z I1117 09:41:20.474578       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2025-11-17T09:41:23.266121832Z I1117 09:41:23.266056       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:41:25.064422542Z I1117 09:41:25.064346       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:41:52.183318265Z I1117 09:41:52.183225       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:41:54.575936348Z I1117 09:41:54.575888       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:42:08.289805766Z I1117 09:42:08.289732       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:42:10.285884898Z I1117 09:42:10.285831       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:42:12.286046898Z I1117 09:42:12.285965       1 installer_controller.go:512] "XXXXXX2" is in transition to 8, but has not made progress because static pod is pending
2025-11-17T09:43:01.185851514Z I1117 09:43:01.185791       1 installer_controller.go:500] "XXXXXX2" moving to (v1.NodeStatus) {
2025-11-17T09:43:01.185851514Z  NodeName: (string) (len=7) "XXXXXX2",
2025-11-17T09:43:01.185851514Z  CurrentRevision: (int32) 8,
2025-11-17T09:43:01.185851514Z  TargetRevision: (int32) 0,
2025-11-17T09:43:01.185851514Z  LastFailedRevision: (int32) 0,
2025-11-17T09:43:01.185851514Z  LastFailedTime: (*v1.Time)(<nil>),
2025-11-17T09:43:01.185851514Z  LastFailedReason: (string) "",
2025-11-17T09:43:01.185851514Z  LastFailedCount: (int) 0,
2025-11-17T09:43:01.185851514Z  LastFallbackCount: (int) 0,
2025-11-17T09:43:01.185851514Z  LastFailedRevisionErrors: ([]string) <nil>
2025-11-17T09:43:01.185851514Z }
2025-11-17T09:43:01.185851514Z  because static pod is ready
2025-11-17T09:43:01.211023142Z I1117 09:43:01.210954       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "XXXXXX2" from revision 7 to 8 because static pod is ready
2025-11-17T09:43:01.212156824Z I1117 09:43:01.212127       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-11-17T09:38:21Z","message":"NodeControllerDegraded: All XXXXXX nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-11-17T09:43:01Z","message":"NodeInstallerProgressing: 3 nodes are at revision 8","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2025-11-17T09:24:45Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-11-17T09:20:21Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-11-17T09:20:21Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-11-17T09:43:01.240997268Z I1117 09:43:01.240926       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"b27def28-8790-4113-bdf7-9bed9e00dbca", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 8"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8"
2025-11-17T09:43:02.355222170Z I1117 09:43:02.355009       1 request.go:696] Waited for 1.142637949s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX1
2025-11-17T09:43:03.554681784Z I1117 09:43:03.554540       1 request.go:696] Waited for 1.195649923s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX2
2025-11-17T09:43:04.554841099Z I1117 09:43:04.554707       1 request.go:696] Waited for 1.19661393s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa
2025-11-17T09:46:49.948087995Z I1117 09:46:49.947986       1 request.go:696] Waited for 1.091228625s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-XXXXXX2
2025-11-17T09:46:50.948221405Z I1117 09:46:50.948170       1 request.go:696] Waited for 1.195716104s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-11-17T09:46:52.757692981Z I1117 09:46:52.755374       1 request.go:696] Waited for 1.003781106s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX1
2025-11-17T09:46:56.148088706Z I1117 09:46:56.148029       1 request.go:696] Waited for 1.045636093s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX1
2025-11-17T09:46:57.347617993Z I1117 09:46:57.347559       1 request.go:696] Waited for 1.19561379s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-8-XXXXXX2
2025-11-17T09:46:58.348399572Z I1117 09:46:58.348345       1 request.go:696] Waited for 1.196808066s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-XXXXXX0
