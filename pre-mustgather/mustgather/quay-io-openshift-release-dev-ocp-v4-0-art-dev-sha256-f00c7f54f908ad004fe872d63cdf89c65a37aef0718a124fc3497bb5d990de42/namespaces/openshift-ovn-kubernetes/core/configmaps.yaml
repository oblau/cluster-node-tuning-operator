---
apiVersion: v1
items:
- apiVersion: v1
  data:
    topology-version: "5"
  kind: ConfigMap
  metadata:
    creationTimestamp: "2025-11-17T09:19:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          f:topology-version: {}
      manager: ovn-kubernetes
      operation: Apply
      time: "2025-11-17T09:19:17Z"
    name: control-plane-status
    namespace: openshift-ovn-kubernetes
    resourceVersion: "5413"
    uid: f9ee7fcf-5dfb-4c34-b78a-644796a8449e
- apiVersion: v1
  data:
    ca.crt: |
      -----BEGIN CERTIFICATE-----
      MIIDMjCCAhqgAwIBAgIIHHgNzc5//4QwDQYJKoZIhvcNAQELBQAwNzESMBAGA1UE
      CxMJb3BlbnNoaWZ0MSEwHwYDVQQDExhrdWJlLWFwaXNlcnZlci1sYi1zaWduZXIw
      HhcNMjUxMTE3MDgyMzQ0WhcNMzUxMTE1MDgyMzQ0WjA3MRIwEAYDVQQLEwlvcGVu
      c2hpZnQxITAfBgNVBAMTGGt1YmUtYXBpc2VydmVyLWxiLXNpZ25lcjCCASIwDQYJ
      KoZIhvcNAQEBBQADggEPADCCAQoCggEBANwah5f1ohm8Juc7YZqAadlQY1CJYN3F
      j13m9boZMGdqqpZk2rK5UW+RNnU7oLAZ6y7vncfZNPjPa58prLOAb+JPs0Pz2RRQ
      LuJivgwMLLKM0/n6pyXXWfGw35Orv4ZwFhw1R6PxndtKRItLuf6WuI4BZQt55OaR
      frOMCiEyloteSVM1qQdiSloC83kcMr6rmEkG0CGmZynelhq823tNAJiRu9lTy1DP
      e7Xn9DeysUjaQG4n2C+Ftx8rIqacm4ZpSE0/HyiM+pWUZQVHO4mjQCx/XaWC1n7n
      YdLbGzcyjZCVBEJ0uzyx23enGtFrtE7/OzLhx0b8D04fXCrGWk5ObyMCAwEAAaNC
      MEAwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFJUN
      /eaUADzHMB3uTyJGFINOBL0YMA0GCSqGSIb3DQEBCwUAA4IBAQCC/k7GoEY6p795
      uMnIPcx3e83IoNBJR+eBOluNVsaf6mPJ4PD6z8MUfHpEskvG1rFljc7MFwimoNRL
      TmYXMlcwxRKcK/9gevs7GcVZWHlbTQEA3/J2AdoWDBYRSgIs/lmRtH0LChPLE1Df
      OuZa+oiroeZUDa3k3jMTIv7ikD15F/CrjIRH91K1QUwr1f/McITQm85ycxqkezcy
      xZSybdqRAlM28X8MJHO9o+CPHdeey/5wtLzX/F51buLbttsOzkOf5xZDXrUA0004
      BwfgHpMm6n6pJxL4KW7FxtI7d7M3r6baPV78hATIMsI72XAnbPVzsz6axuN+VeH5
      xW5zX5aD
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDQDCCAiigAwIBAgIIBJsSoG5kz70wDQYJKoZIhvcNAQELBQAwPjESMBAGA1UE
      CxMJb3BlbnNoaWZ0MSgwJgYDVQQDEx9rdWJlLWFwaXNlcnZlci1sb2NhbGhvc3Qt
      c2lnbmVyMB4XDTI1MTExNzA4MjM0NFoXDTM1MTExNTA4MjM0NFowPjESMBAGA1UE
      CxMJb3BlbnNoaWZ0MSgwJgYDVQQDEx9rdWJlLWFwaXNlcnZlci1sb2NhbGhvc3Qt
      c2lnbmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwr3ve9Q8XF9Q
      k/Qnv/topXVMpnqk1SvmnRawkDDur76VIsvMQPzeAs0x/LIgNjCsQGZbw1uH4XDc
      1GGBYr33sdksFFEp0medGkdVxAQS/XBSHkIlwyit+HDByEGJTK3RC6U0rd3lU9ke
      ZmdSfSFrFi0njC0wb+OczL3An/+uQ+99H7EreBktjB12n1/joDmRihHIhjPMxYRJ
      p3nteLehHdF1ltXAAcfILiyiptjr4tGjzbWJOvrg48O/raV/ApusiDv6dkWYSOIV
      xKpWvbtSH2t8oGrkfrA0qrkfgHeIbGFFGFFBJRJ2eP8LUjA3igA2f7RgyxuMrIKX
      kg3pnpWk3QIDAQABo0IwQDAOBgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/BAUwAwEB
      /zAdBgNVHQ4EFgQUgrRTQPDqEN/ntTkRotlW0m/c7DIwDQYJKoZIhvcNAQELBQAD
      ggEBAHffGVowOxIIkKFuaab/q8eTWxgGULImUFx8YmdvRenkRRsn1wiLaeZ4C1gz
      hj+vSBU4tAr0gM45krx7Y8Rv63hHFeWf3NuVY3bgMZ2F9HmdOvsFT0zT9B2pRHZZ
      PY9Tyk1dZSwXDstxEcKFeO6THp3mufHiq/Q93jERrUxv3QS+d4YqW2WBxNgWpfA4
      FL2GMOcdFHCFWi6C3wEXfNld7cKeKeVXODiylNYETHmcXxTJKNgr30iAK721U6MR
      KZsBEoW9pVTLzPIatBFet60EHK3koCUSz8bEbIxSg2Z0lWPnBFrLEti/J8tEPE1G
      1pvzzZFMXGIeDRxrZHCQfdFd1is=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDTDCCAjSgAwIBAgIISPV+UhcVQCwwDQYJKoZIhvcNAQELBQAwRDESMBAGA1UE
      CxMJb3BlbnNoaWZ0MS4wLAYDVQQDEyVrdWJlLWFwaXNlcnZlci1zZXJ2aWNlLW5l
      dHdvcmstc2lnbmVyMB4XDTI1MTExNzA4MjM0NFoXDTM1MTExNTA4MjM0NFowRDES
      MBAGA1UECxMJb3BlbnNoaWZ0MS4wLAYDVQQDEyVrdWJlLWFwaXNlcnZlci1zZXJ2
      aWNlLW5ldHdvcmstc2lnbmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC
      AQEAyQYD92TOQPu8PjzdsB0pfu2h1RdjOkFMzDbVcMOXzeJeNNU3QyDLiXbpF4Um
      MtnaAFKI5+Lixu4D7YuRmrh3L8wo+FBOyi1MvQ+t0jivE4xLxszfpX523VyiGPOy
      CVaTF299NTvdBnPZTexQrqEkLDtVM+fUndpEj7XmyUYXjcsNQNPaMcpYsHPjGtwd
      R0cmvXgYvj8Qp0qIbZoaj9iHpHGnSD+kbZAOdcib0cUwJKyVciyyv6YFdda/HmEm
      vMEIfLAOeISRFN7acyvbsOxU0QQUcVmWsM3YbdshCjJJ6J9AgWRN/uXnqzeQWlJy
      qLMRc/PO/s4qhiWkcftTnGyJUwIDAQABo0IwQDAOBgNVHQ8BAf8EBAMCAqQwDwYD
      VR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUcbLSzO3WDmOy7afA4EAxVL5R51EwDQYJ
      KoZIhvcNAQELBQADggEBAJLBwpS3sVr5OOEqkqax4DV99SMTCzRHAOyAzTXf025D
      YlbxTrnvBZp4yUuniPttgtGi0Zyth7MLYHrp0nQcIWBRNKmt2b6+fb2AzEdSwQii
      oSII/5yN8XIHjqsPAPifV2K3f8VDVhOV6FrhiNqVY+KgnIFkKVmPMRjQI2Cktvzs
      kPtQ7buIaZiUXTcKPpfzqAJzT+/diqtQngZMa4uuMEcIVXv+ZE0lz+N3w3F6wpS7
      9/cKYPz81zuVFaYqx7xzzKqfUZ0P8/VVDWLH9uDTz4l7S/RYBwWM2xRi4mS+auVc
      0amS4vxqS+TkEuBD4EuPFIYc2Bz7XfrW3rpXLN1wOsQ=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDlzCCAn+gAwIBAgIIEdYGC8Zueq4wDQYJKoZIhvcNAQELBQAwWTFXMFUGA1UE
      AwxOb3BlbnNoaWZ0LWt1YmUtYXBpc2VydmVyLW9wZXJhdG9yX2xvY2FsaG9zdC1y
      ZWNvdmVyeS1zZXJ2aW5nLXNpZ25lckAxNzYzMzcxMjI5MB4XDTI1MTExNzA5MjAy
      OFoXDTM1MTExNTA5MjAyOVowWTFXMFUGA1UEAwxOb3BlbnNoaWZ0LWt1YmUtYXBp
      c2VydmVyLW9wZXJhdG9yX2xvY2FsaG9zdC1yZWNvdmVyeS1zZXJ2aW5nLXNpZ25l
      ckAxNzYzMzcxMjI5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyEwQ
      DYRhEETK5M4/B+3PJhRS9xIOk7+h5DWa8qUESoDAJdaUrqpt5XOUTc3U8fvaKdrY
      VH+f62m3C2vMv6w3gHPyI7smokXzrN0FPdbFr3bXLPvwddWC+jTiDkc45ewcicud
      GGTHp1gS4XVdmPTD4I4xj+UoHOesB9+gLN2nuPoj0Fndhv7MURyyxOh1jgR/WC1T
      5OLoqqZXgyLb3pJQRITOcM2R/e581LC+y7rLR+C1VIrkArrdf/vpgumOisamuS1V
      /aObQRkhZJiZGWUQwBUgYCyekewLvbCyPwWXtw7xSj/XuKw8SKpAmSn1+31jFuLY
      UgduO9NYzFBlcR8FyQIDAQABo2MwYTAOBgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/
      BAUwAwEB/zAdBgNVHQ4EFgQU46Cyxbpz+FO5LRelJXgkGsHGWGowHwYDVR0jBBgw
      FoAU46Cyxbpz+FO5LRelJXgkGsHGWGowDQYJKoZIhvcNAQELBQADggEBAKRlOnNx
      6c6qARRMEz1NZ6JlsG90kmxilofSMMcrxiNwUqohX6aiNh7O+uYAWZX52eMJKklm
      rRY1D4E0km0vsQXfTcMLbUIwycvqfzTMRflZ3s8l+nDnIlpyP7Oi0V/+AfJZTWUF
      FggDXmNENiJHH4dCMk+IgieKyYEAmtVjdribmxprLU99pe8oq4pTI2/0Z+lukH63
      Cau6TigbdKu9/KU3KzWnpYyQKrllHLVGXNFqbv0mZWmPA1w+d5zFCmeHFDxXCGtn
      +iIIEpvSZNEFco7kqLBYVj1hF+Wy8KU60oBX18IEOvYwwoCH/U6Htcrftisfi1NY
      PlMzSu7rjEW4pgQ=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDgzCCAmugAwIBAgIICv1WhQwJKBEwDQYJKoZIhvcNAQELBQAwJjEkMCIGA1UE
      AwwbaW5ncmVzcy1vcGVyYXRvckAxNzYzMzcxMzA5MB4XDTI1MTExNzA5MjE0OFoX
      DTI3MTExNzA5MjE0OVowMTEvMC0GA1UEAwwmKi5hcHBzLmhseGNsNTEubGFiLmVu
      Zy50bHYyLnJlZGhhdC5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB
      AQC3rIa4+opCUMXgb3mEtgts6XISeJ4F373tMI8egGfdlOniX70/0bqaC09X5KHH
      m1+pMZi1xaG7NqOfE8gcs9GlTZAsusy//IlTtjXtpfTG0e07xWspzzmajp4PsvYX
      CBUmGpyubqIrv1zNugDTepqwAZYNBiZ8PgKpzRf+MttMxeG4bniasocLq1hmq8fZ
      HY8jZ3BI5tiakvB2CziUKm28ViddM+yyAxvIJ4ayXhzCc5wYN/Cdxpa09qAndeAu
      QiDPMnLgtZfx9TcXK1ORQyLA9+lU9z1XlZxXI3fggjv1ABU1nyNCKeU4PvVncIpf
      SrPn3vkiESnvRq87LZRBUim9AgMBAAGjgakwgaYwDgYDVR0PAQH/BAQDAgWgMBMG
      A1UdJQQMMAoGCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwHQYDVR0OBBYEFNTzfwzK
      d6ikdZ7me+UIMXGpIiWKMB8GA1UdIwQYMBaAFN0G4qQTKqRjGHCGhckfrINChtF3
      MDEGA1UdEQQqMCiCJiouYXBwcy5obHhjbDUxLmxhYi5lbmcudGx2Mi5yZWRoYXQu
      Y29tMA0GCSqGSIb3DQEBCwUAA4IBAQAHduCk357TlUutwzNzesExzrnqI3/aftcP
      yBXNe7hqy4xRX5J3iDngNglB43EDMyBZ95r/bF08VCGz8z+O0HLQacJKoGfuhdE/
      PGaiN6mFeM5PdzNNq08DIKiVVwhLklgK6BP4F/AP+ktAFGE8gVQwbZC8hHESgYPN
      /dQhahXYxtQ0VzQO33LXrlZHX3AgR227FGucqLcn8CXS4UekYXz9bCjs+571TjhW
      gaNqkXYTmCeeMdzUrmbUtHnlrhw4YFSKcsxmGfMtwBkovkzKlF+Y/haw1OXYkgBt
      sdy2gPbW6VQWGDSBhO9CBY/51jACuFnUlWTVO0t4qaJi3fjeSTkS
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy
      ZXNzLW9wZXJhdG9yQDE3NjMzNzEzMDkwHhcNMjUxMTE3MDkyMTQ4WhcNMjcxMTE3
      MDkyMTQ5WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NjMzNzEzMDkw
      ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDcBDMP+nEOv38P0CCHXWsx
      MAGI56RxiRgdlyzVIDeIWfxWJcCVaFZ6/UJvv6dF8EapyKI0huy0Q9tAr8N1klWw
      cFiG4XIdR1dYZSHazWyL6zSM9EI1/zCQOZ4VE2RAW5axchBKOwX90GMqpXRZ3nA2
      TJwAwXzgeKHIlSoS5SmN3/EWYuOjoJNx3bLQ/B0O89wxOF14doWz5YUSmDzoL8A3
      A3yX/m2h1FXxRxrcs4ov1Gcty7aeaUcySzp7E+4Wkvhrg4vmuyEMAdPjDVO9bcBa
      MnJ4BcP4gALMP6EJQziC1CRkJqdQ4+KjZwlYIzbGXz3HJuOBT3S5ZLizF2hBgwdx
      AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G
      A1UdDgQWBBTdBuKkEyqkYxhwhoXJH6yDQobRdzANBgkqhkiG9w0BAQsFAAOCAQEA
      N4RmQr545nyCl2NX8THv5Q8TMjtxTi/dUIF07kVpJQN94RFOuL+fZer6FIg160a/
      +8E2gzZwoybRonPN7zMbw4VRg/EiPZPd/wODliCwW1l+C5ivnZTJIHinLf0+U6sk
      RoIVzd2kIhU63ph8/+SJ/RyVDZKfK7n3OTGUg7e1rwt2ZgL7KMSU2SZ00hM/lqlE
      pvv+GzoGMOiKTtN6J197PtSGf9KisSGsCTPGN1a7HEUimUDZGNNWKPRHNLcPXkXh
      ifC1j/koiwqu89nAHOzQb1n5fYVp0DQ8Gey/Nj1ytgqC42J63HFX1FFdVcaEJFdw
      z6TlPivqlTLLMZC8ilU41w==
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      kubernetes.io/description: Contains a CA bundle that can be used to verify the
        kube-apiserver when using internal endpoints such as the internal service
        IP or kubernetes.default.svc. No other usage is guaranteed across distributions
        of Kubernetes clusters.
    creationTimestamp: "2025-11-17T09:18:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:ca.crt: {}
        f:metadata:
          f:annotations:
            .: {}
            f:kubernetes.io/description: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-11-17T09:27:16Z"
    name: kube-root-ca.crt
    namespace: openshift-ovn-kubernetes
    resourceVersion: "14545"
    uid: 7535eb91-1247-4323-bd58-df28f7163e84
- apiVersion: v1
  data:
    service-ca.crt: |
      -----BEGIN CERTIFICATE-----
      MIIDUTCCAjmgAwIBAgIIHKUjOpzJvOowDQYJKoZIhvcNAQELBQAwNjE0MDIGA1UE
      Awwrb3BlbnNoaWZ0LXNlcnZpY2Utc2VydmluZy1zaWduZXJAMTc2MzM3MTIyMDAe
      Fw0yNTExMTcwOTIwMTlaFw0yODAxMTYwOTIwMjBaMDYxNDAyBgNVBAMMK29wZW5z
      aGlmdC1zZXJ2aWNlLXNlcnZpbmctc2lnbmVyQDE3NjMzNzEyMjAwggEiMA0GCSqG
      SIb3DQEBAQUAA4IBDwAwggEKAoIBAQDJZslGqtAyG+AF1bCbVlUs9sQRLkmJ7l6v
      lUzDwf7FOFhJEqdN7VkIx0nX/mxYUYV1jGURo06noKZrWV6z4LXOfcw4X4XIOzkx
      YNm1kyD9MJGyx6Uv9ZxiAsguqqGwBt0Sf8fXbcqSBCcty7Myr0BNsB/K3FJvbE8p
      nTx+1G+PrmBkRES8GJe7Av7S31gnvZl2J6V7EkyIBJtSHt57Zcs9FBmuDQcphPhg
      38jgQhzlLpUbLMkcJLAD9/polbrprn4prszIlJeew5AqkVsBKffJzeikRgtxU97L
      VkmInVytBlB7mp7rjuIl3mB7oimj5UfVwdT5FG+KwDiaoboG41Q9AgMBAAGjYzBh
      MA4GA1UdDwEB/wQEAwICpDAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBQXa9VP
      LalKu89oL8PUBg9J9mMaQTAfBgNVHSMEGDAWgBQXa9VPLalKu89oL8PUBg9J9mMa
      QTANBgkqhkiG9w0BAQsFAAOCAQEAs46eXJLEiWM7WVii0daJwcY6CjQCT9w76vP+
      V2P3oCRXP+UCTZASIqZRpi64T99R7bM7W7Fda16rpyoy/xcMp0sayrfKkr/j4Nzp
      D6rvx03p38uD5da7M/WblMvjEIZ/aA2D3BjhCde3FPE6q5cpF+X9Rhk2B6MX6Sej
      qCyQ02EDqlTURydLVf63yq4C+166MCtHwKqNRcRT58RUE1svCAFfdBWhU4DKv+WK
      GF0JCAztNK9OhBWcw7iAXYfCHuh7sCfseF51Pm7S54/hMmz9DzGIkAF+4Rq9GT2c
      DuGA0xePSTcjY3jrojdZJovgoxXAcjfwCC7pBYuqu1PrTK5GTw==
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      service.beta.openshift.io/inject-cabundle: "true"
    creationTimestamp: "2025-11-17T09:18:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data: {}
        f:metadata:
          f:annotations:
            .: {}
            f:service.beta.openshift.io/inject-cabundle: {}
      manager: kube-controller-manager
      operation: Update
      time: "2025-11-17T09:18:14Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          f:service-ca.crt: {}
      manager: service-ca-operator
      operation: Update
      time: "2025-11-17T09:20:41Z"
    name: openshift-service-ca.crt
    namespace: openshift-ovn-kubernetes
    resourceVersion: "7930"
    uid: 911aa813-591c-41e1-aeef-4af026f7eda3
- apiVersion: v1
  data:
    ca-bundle.crt: |
      -----BEGIN CERTIFICATE-----
      MIIDTzCCAjegAwIBAgIIKoJ2MztsIxQwDQYJKoZIhvcNAQELBQAwNTEzMDEGA1UE
      Awwqb3BlbnNoaWZ0LW92bi1rdWJlcm5ldGVzX292bi1jYUAxNzYzMzcxMTAzMB4X
      DTI1MTExNzA5MTgyMloXDTM1MTExNTA5MTgyM1owNTEzMDEGA1UEAwwqb3BlbnNo
      aWZ0LW92bi1rdWJlcm5ldGVzX292bi1jYUAxNzYzMzcxMTAzMIIBIjANBgkqhkiG
      9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4oyYYO+OUSqUmEGF1lur0ZcNU0JNqe8ih+jo
      oMCeM9rPezIWoQglisQ+jd6oITpzYigGZIijzNVd9VtzrtR6d7lpj5hP0IZwtJNR
      ImJhOOE32DF1Exfh2D9sjcfUZmMnRqyNAsdaQQVvYp+PUXpAyArLueKCFLEhGMDi
      Hl18zyBA5poUbJOkCvpnniReqi/wHq1LROlh/28zHyC2IvJRqK/kywOJQGIlLCPm
      fPWpb/IFywgs0nLcudNC3vQxyRT44j1DE8sZX2BRh6pHEyUDBiI8VXtuVilArFDd
      21xB3HCrB5Cb+nqBiThGITERvb1awC/Mdj+zkCjW0SLhiN0HHQIDAQABo2MwYTAO
      BgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQU67Jvz33Y
      lyrzCqrHh8KMMe3xaF4wHwYDVR0jBBgwFoAU67Jvz33YlyrzCqrHh8KMMe3xaF4w
      DQYJKoZIhvcNAQELBQADggEBAAvZVd1GSA7nL5e/EC84u2sLjFFH666gjrH9+qxN
      +0gz6Zjr4NtFW9ED7FVk72kGBGCItTDsFwdi+SVbzdLgZBl4hOnqxYWOlGtJMPrx
      xMDUi8NQP1/y3zxW4w6BIKDUUREVl4dYzQtXrStMoQ15jMI2XdtPwuE5QH0MWP8D
      vacdksxsNqekONWI1VPPUIrLuSsFvoeGlvrm3lP+ZOluPOepf8kNtElNoVO7pKkS
      R3FY6R0IEKqVZgZ/JOHEnKcQCMZBORil2YTdMGYVsJsWkxZxu9Ri/1Z7gqRLhZB4
      9qOOox2UBBOLhYEGh0VezYp5gCyINHaf4yRQV7NKzJLmY3Y=
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    creationTimestamp: "2025-11-17T09:18:23Z"
    labels:
      auth.openshift.io/managed-certificate-type: ca-bundle
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:ca-bundle.crt: {}
        f:metadata:
          f:labels:
            .: {}
            f:auth.openshift.io/managed-certificate-type: {}
      manager: cluster-network-operator
      operation: Update
      time: "2025-11-17T09:18:23Z"
    name: ovn-ca
    namespace: openshift-ovn-kubernetes
    resourceVersion: "4879"
    uid: eaa564dd-d942-4a8c-9f5e-e4c0319741a7
- apiVersion: v1
  data:
    ovnkube.conf: |-
      [default]
      mtu="1400"
      cluster-subnets="10.128.0.0/14/23,fd02::/48/64"
      encap-port="6081"
      enable-lflow-cache=true
      lflow-cache-limit-kb=1048576
      enable-udp-aggregation=true

      [kubernetes]
      service-cidrs="172.30.0.0/16,fd03::/112"
      ovn-config-namespace="openshift-ovn-kubernetes"
      apiserver="https://api-int.hlxcl51.XXXXXXXXXXXXXXXXXXXXXXX:6443"
      host-network-namespace="openshift-host-network"
      platform-type="BareMetal"
      healthz-bind-address="0.0.0.0:10256"
      dns-service-namespace="openshift-dns"
      dns-service-name="dns-default"

      [ovnkubernetesfeature]
      enable-egress-ip=true
      enable-egress-firewall=true
      enable-egress-qos=true
      enable-egress-service=true
      egressip-node-healthcheck-port=9107
      enable-multi-network=true

      [gateway]
      mode=shared
      nodeport=true

      [clustermanager]

      [logging]
      libovsdblogfile=/var/log/ovnkube/libovsdb.log
      logfile-maxsize=100
      logfile-maxbackups=5
      logfile-maxage=0
  kind: ConfigMap
  metadata:
    creationTimestamp: "2025-11-17T09:18:21Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          f:ovnkube.conf: {}
        f:metadata:
          f:ownerReferences:
            k:{"uid":"de0dec5f-c369-4191-91db-e20c2450d94e"}: {}
      manager: cluster-network-operator/operconfig
      operation: Apply
      time: "2025-11-17T09:18:21Z"
    name: ovnkube-config
    namespace: openshift-ovn-kubernetes
    ownerReferences:
    - apiVersion: operator.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Network
      name: cluster
      uid: de0dec5f-c369-4191-91db-e20c2450d94e
    resourceVersion: "4859"
    uid: 7a0e4aa8-f0c8-42b6-9f9b-20e6de330a81
- apiVersion: v1
  data:
    ovnkube-lib.sh: |-
      #!/bin/bash
      set -x

      # Add node-specific overrides if the container has mounted any
      K8S_NODE=${K8S_NODE:-}
      if [[ -n "${K8S_NODE}" && -f "/env/${K8S_NODE}" ]]; then
        set -o allexport
        source "/env/${K8S_NODE}"
        set +o allexport
      fi

      northd_pidfile="/var/run/ovn/ovn-northd.pid"
      controller_pidfile="/var/run/ovn/ovn-controller.pid"
      controller_logfile="/var/log/ovn/acl-audit-log.log"
      vswitch_dbsock="/var/run/openvswitch/db.sock"
      nbdb_pidfile="/var/run/ovn/ovnnb_db.pid"
      nbdb_sock="/var/run/ovn/ovnnb_db.sock"
      nbdb_ctl="/var/run/ovn/ovnnb_db.ctl"
      sbdb_pidfile="/var/run/ovn/ovnsb_db.pid"
      sbdb_sock="/var/run/ovn/ovnsb_db.sock"
      sbdb_ctl="/var/run/ovn/ovnsb_db.ctl"

      # start-ovn-controller() starts ovn-controller and does not return until
      # ovn-controller exits
      #
      # Requires the following volume mounts:
      #   /run/openvswitch
      #   /run/ovn/
      #   /etc/openvswitch
      #   /etc/ovn/
      #   /var/lib/openvswitch
      #   /var/log/ovn/
      #   /dev/log
      start-ovn-controller()
      {
        local log_level=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        echo "$(date -Iseconds) - starting ovn-controller"
        exec ovn-controller \
          unix:${vswitch_dbsock} \
          -vfile:off \
          --no-chdir \
          --pidfile=${controller_pidfile} \
          --syslog-method="null" \
          --log-file=${controller_logfile} \
          -vFACILITY:"local0" \
          -vconsole:"${log_level}" \
          -vconsole:"acl_log:off" \
          -vPATTERN:console:"%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m" \
          -vsyslog:"acl_log:info" \
          -vfile:"acl_log:info"
      }

      # quit-ovn-northd() will cleanly shut down ovn-northd. It is intended
      # to be run from a bash 'trap' like so:
      #
      #    trap quit-ovn-northd TERM INT
      quit-ovn-northd()
      {
        echo "$(date -Iseconds) - stopping ovn-northd"
        OVN_MANAGE_OVSDB=no /usr/share/ovn/scripts/ovn-ctl stop_northd
        echo "$(date -Iseconds) - ovn-northd stopped"
        rm -f ${northd_pidfile}
        exit 0
      }

      # run-ovn-northd() starts ovn-northd and does not return until
      # northd exits.
      #
      # Requires the following volume mounts:
      #   /etc/openvswitch/
      #   /var/lib/openvswitch/
      #   /run/openvswitch/
      #   /run/ovn/
      #   /var/log/ovn/
      start-ovn-northd()
      {
        local log_level=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        echo "$(date -Iseconds) - starting ovn-northd"
        exec ovn-northd \
          --no-chdir \
          -vconsole:"${log_level}" \
          -vfile:off \
          -vPATTERN:console:"%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m" \
          --pidfile ${northd_pidfile} \
          --n-threads=1 &
        wait $!
      }

      # start-audit-log-rotation() continuously watches ovn-controller's audit
      # log directory and deletes old logs to ensure the total size of the logs
      # does not exceed a given threshold. This function does not return.
      #
      # Requires the following volume mounts:
      #   /var/log/ovn/
      #   /run/ovn/
      start-audit-log-rotation()
      {
        # Rotate audit log files when then get to max size (in bytes)
        MAXFILESIZE=$(( "50"*1000000 ))
        MAXLOGFILES="5"
        LOGDIR=$(dirname ${controller_logfile})

        # wait a bit for ovn-controller to start
        local retries=0
        while [[ 30 -gt "${retries}" ]]; do
          (( retries += 1 ))
          CONTROLLERPID=$(cat ${controller_pidfile})
          if [[ -n "${CONTROLLERPID}" ]]; then
            break
          fi
          sleep 2
        done
        if [[ -z "${CONTROLLERPID}" ]]; then
          echo "Timed out waiting for ${controller_pidfile}"
          return 1
        fi

        # Redirect err to null so no messages are shown upon rotation
        tail -F ${controller_logfile} 2> /dev/null &

        while true
        do
          # Make sure ovn-controller's logfile exists, and get current size in bytes
          if [ -f "${controller_logfile}" ]; then
            file_size=`du -b ${controller_logfile} | tr -s '\t' ' ' | cut -d' ' -f1`
          else
            ovs-appctl -t /var/run/ovn/ovn-controller.${CONTROLLERPID}.ctl vlog/reopen
            file_size=`du -b ${controller_logfile} | tr -s '\t' ' ' | cut -d' ' -f1`
          fi

          if [ $file_size -gt $MAXFILESIZE ];then
            echo "Rotating OVN ACL Log File"
            timestamp=`date '+%Y-%m-%dT%H-%M-%S'`
            mv ${controller_logfile} ${LOGDIR}/acl-audit-log.$timestamp.log
            ovs-appctl -t /run/ovn/ovn-controller.${CONTROLLERPID}.ctl vlog/reopen
            CONTROLLERPID=$(cat ${controller_pidfile})
          fi

          # Ensure total number of log files does not exceed the maximum configured from OVNPolicyAuditMaxLogFiles
          num_files=$(ls -1 ${LOGDIR}/acl-audit-log* 2>/dev/null | wc -l)
          if [ "$num_files" -gt "$MAXLOGFILES" ]; then
            num_to_delete=$(( num_files - ${MAXLOGFILES} ))
            ls -1t ${LOGDIR}/acl-audit-log* 2>/dev/null | tail -$num_to_delete | xargs -I {} rm {}
          fi

          # sleep for 30 seconds to avoid wasting CPU
          sleep 30
        done
      }

      wait-for-certs()
      {
        local detail=$1
        local privkey=$2
        local clientcert=$3

        if [[ $# -ne 3 ]]; then
          echo "Expected three arguments but got $#"
          exit 1
        fi

        retries=0
        TS=$(date +%s)
        WARN_TS=$(( ${TS} + $(( 20 * 60)) ))
        HAS_LOGGED_INFO=0
        while [[ ! -f "${privkey}" ||  ! -f "${clientcert}" ]] ; do
          CUR_TS=$(date +%s)
          if [[ "${CUR_TS}" -gt "WARN_TS"  ]]; then
            echo "$(date -Iseconds) WARN: ${detail} certs not mounted after 20 minutes."
          elif [[ "${HAS_LOGGED_INFO}" -eq 0 ]] ; then
            echo "$(date -Iseconds) INFO: ${detail} certs not mounted. Waiting one hour."
            HAS_LOGGED_INFO=1
          fi
          sleep 5
        done
      }

      # start-rbac-proxy() starts the kube-rbac-proxy to expose ovnkube metrics to
      # Prometheus on the given listen_port, proxying from upstream_port. This
      # function does not return.
      #
      # Requires the following volume mounts:
      #   /etc/pki/tls/metrics-cert
      start-rbac-proxy-node()
      {
        local detail=$1
        local listen_port=$2
        local upstream_port=$3
        local privkey=$4
        local clientcert=$5

        if [[ $# -ne 5 ]]; then
          echo "Expected five arguments but got $#"
          exit 1
        fi

        # As the secret mount is optional we must wait for the files to be present.
        # The service is created in monitor.yaml and this is created in sdn.yaml.
        # If it isn't created there is probably an issue so we want to crashloop.
        echo "$(date -Iseconds) INFO: waiting for ${detail} certs to be mounted"
        wait-for-certs "${detail}" "${privkey}" "${clientcert}"

        echo "$(date -Iseconds) INFO: ${detail} certs mounted, starting kube-rbac-proxy"
        exec /usr/bin/kube-rbac-proxy \
          --logtostderr \
          --secure-listen-address=:${listen_port} \
          --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 \
          --upstream=http://127.0.0.1:${upstream_port}/ \
          --tls-private-key-file=${privkey} \
          --tls-cert-file=${clientcert}
      }

      # quit-nbdb() will cleanly shut down the northbound dbserver. It is intended
      # to be run from a bash 'trap' like so:
      #
      #    trap quit-nbdb TERM INT
      quit-nbdb()
      {
        echo "$(date -Iseconds) - stopping nbdb"
        /usr/share/ovn/scripts/ovn-ctl stop_nb_ovsdb
        echo "$(date -Iseconds) - nbdb stopped"
        rm -f ${nbdb_pidfile}
        exit 0
      }

      # start-nbdb() starts the OVN northbound database. This function does not
      # return.
      #
      # Requires the following volume mounts:
      #   /etc/ovn
      #   /var/log/ovn
      #   /run/ovn/
      start-nbdb()
      {
        local log_level=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        exec /usr/share/ovn/scripts/ovn-ctl \
          --no-monitor \
          --db-nb-sock=${nbdb_sock} \
          --ovn-nb-log="-vconsole:${log_level} -vfile:off -vPATTERN:console:%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m" \
          run_nb_ovsdb &
        wait $!
      }

      # retry() an operation a number of times, sleeping 2 seconds between each try
      retry() {
        local tries=${1}
        local desc=${2}
        local cmd=${3}

        local retries=0
        while ! ${cmd}; do
          (( retries += 1 ))
          if [[ "${retries}" -gt ${tries} ]]; then
            echo "$(date -Iseconds) - ERROR - ${desc} - too many failed attempts, giving up"
            return 1
          fi
          echo "$(date -Iseconds) - WARN - ${desc} - failed try ${retries}, retrying..."
          sleep 2
        done
        echo "$(date -Iseconds) - INFO - ${desc} - success"
        return 0
      }

      # nbdb-post-start() tweaks nbdb database server settings and sets a number
      # of options in NB_Globals to configure OVN global settings
      nbdb-post-start()
      {
        local northd_probe_interval=${1:-10000}

        rm -f ${nbdb_pidfile}

        # set inactivity probe
        if ! retry 60 "inactivity-probe" "ovn-nbctl -t 5 --inactivity-probe=60000 set-connection punix:${nbdb_sock}"; then
          exit 1
        fi
        # set trim-on-compaction
        if ! retry 60 "trim-on-compaction" "ovn-appctl -t ${nbdb_ctl} --timeout=5 ovsdb-server/memory-trim-on-compaction on"; then
          exit 1
        fi

        # set IC zone
        echo "Setting the IC zone to ${K8S_NODE}"
        IC_OPTION="name=\"${K8S_NODE}\" options:name=\"${K8S_NODE}\""

        # northd probe interval
        echo "Setting northd probe interval to ${northd_probe_interval} ms"
        NORTHD_PROBE_OPTION="options:northd_probe_interval=${northd_probe_interval}"

        # let northd sleep so it takes less CPU
        NORTHD_SLEEP_OPTION="options:northd-backoff-interval-ms=300"

        local ipsec=false
        local ipsec_encapsulation=false

        IPSEC_OPTION="ipsec=${ipsec} options:ipsec_encapsulation=${ipsec_encapsulation}"

        # set all the NB_GLOBAL options
        if ! retry 20 "nb-global options" "ovn-nbctl -t 5 set nb_global . ${IC_OPTION} ${NORTHD_PROBE_OPTION} ${NORTHD_SLEEP_OPTION} ${IPSEC_OPTION}"; then
          exit 1
        fi
      }

      # ovndb-readiness-probe() checks if the the database is in the active state
      # and if not, exits with an error code.
      ovndb-readiness-probe()
      {
        # dbname should be 'sb' or 'nb'
        local dbname=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        local ctlfile
        if [[ "${dbname}" = "nb" ]]; then
          ctlfile=${nbdb_ctl}
        elif [[ "${dbname}" = "sb" ]]; then
          ctlfile=${sbdb_ctl}
        else
          echo "unknown DB name ${dbname}"
          exit 1
        fi

        status=$(/usr/bin/ovn-appctl -t ${ctlfile} --timeout=3 ovsdb-server/sync-status  2>/dev/null | { grep "state: active" || false; })
        if [[ -z "${status}" ]]; then
          echo "${dbname} DB is not running or active."
          exit 1
        fi
      }

      # quit-sbdb() will cleanly shut down the southbound dbserver. It is intended
      # to be run from a bash 'trap' like so:
      #
      #    trap quit-sbdb TERM INT
      quit-sbdb()
      {
        echo "$(date -Iseconds) - stopping sbdb"
        /usr/share/ovn/scripts/ovn-ctl stop_sb_ovsdb
        echo "$(date -Iseconds) - sbdb stopped"
        rm -f ${sbdb_pidfile}
        exit 0
      }

      # start-sbdb() starts the OVN southbound database. This function does not
      # return.
      #
      # Requires the following volume mounts:
      #   /etc/ovn
      #   /var/log/ovn
      #   /run/ovn/
      start-sbdb()
      {
        local log_level=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        exec /usr/share/ovn/scripts/ovn-ctl \
          --no-monitor \
          --db-sb-sock=${sbdb_sock} \
          --ovn-sb-log="-vconsole:${log_level} -vfile:off -vPATTERN:console:%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m" \
          run_sb_ovsdb &
        wait $!
      }

      # sbdb-post-start() tweaks sbdb database server settings
      sbdb-post-start()
      {
        rm -f ${sbdb_pidfile}

        # set inactivity probe
        if ! retry 60 "inactivity-probe" "ovn-sbctl -t 5 --inactivity-probe=180000 set-connection punix:${sbdb_sock}"; then
          exit 1
        fi
        # set trim-on-compaction
        if ! retry 60 "trim-on-compaction" "ovn-appctl -t ${sbdb_ctl} --timeout=5 ovsdb-server/memory-trim-on-compaction on"; then
          exit 1
        fi
      }

      function log()
      {
          echo "$(date --iso-8601=seconds) [{$1}] ${2}"
      }

      # cni-bin-copy() detects the host OS and copies the correct shim binary to
      # the CNI binary directory.
      #
      # Requires the following volume mounts:
      #   /host
      #   /cni-bin-dir
      cni-bin-copy()
      {
        # collect host os information
        . /host/etc/os-release
        rhelmajor=
        # detect which version we're using in order to copy the proper binaries
        case "${ID}" in
          rhcos|scos)
            RHEL_VERSION=$(echo "${CPE_NAME}" | cut -f 5 -d :)
            rhelmajor=$(echo $RHEL_VERSION | sed -E 's/([0-9]+)\.{1}[0-9]+(\.[0-9]+)?/\1/')
          ;;
          rhel) rhelmajor=$(echo "${VERSION_ID}" | cut -f 1 -d .)
          ;;
          fedora)
            if [ "${VARIANT_ID}" == "coreos" ]; then
              rhelmajor=8
            else
              log "cnibincopy" "FATAL ERROR: Unsupported Fedora variant=${VARIANT_ID}"
              exit 1
            fi
          ;;
          *) log "cnibincopy" "FATAL ERROR: Unsupported OS ID=${ID}"; exit 1
          ;;
        esac

        # Set which directory we'll copy from, detect if it exists
        sourcedir=/usr/libexec/cni/
        case "${rhelmajor}" in
          8)
            sourcedir=/usr/libexec/cni/rhel8
          ;;
          9)
            sourcedir=/usr/libexec/cni/rhel9
          ;;
          *)
            log "cnibincopy" "ERROR: RHEL Major Version Unsupported, rhelmajor=${rhelmajor}"
          ;;
        esac

        cp -f "$sourcedir/ovn-k8s-cni-overlay" /cni-bin-dir/
      }

      # start-ovnkube-node starts the ovnkube-node process. This function does not
      # return.
      start-ovnkube-node()
      {
        local log_level=$1
        local metrics_port=$2
        local ovn_metrics_port=$3

        if [[ $# -ne 3 ]]; then
          echo "Expected three arguments but got $#"
          exit 1
        fi

        # copy the right CNI shim for the host OS
        cni-bin-copy

        echo "I$(date "+%m%d %H:%M:%S.%N") - disable conntrack on geneve port"
        iptables -t raw -A PREROUTING -p udp --dport 6081 -j NOTRACK
        iptables -t raw -A OUTPUT -p udp --dport 6081 -j NOTRACK
        ip6tables -t raw -A PREROUTING -p udp --dport 6081 -j NOTRACK
        ip6tables -t raw -A OUTPUT -p udp --dport 6081 -j NOTRACK

        echo "I$(date "+%m%d %H:%M:%S.%N") - starting ovnkube-node"

        if [ "shared" == "shared" ]; then
          gateway_mode_flags="--gateway-mode shared --gateway-interface br-ex"
        elif [ "shared" == "local" ]; then
          gateway_mode_flags="--gateway-mode local --gateway-interface br-ex"
        else
          echo "Invalid OVN_GATEWAY_MODE: \"shared\". Must be \"local\" or \"shared\"."
          exit 1
        fi

        export_network_flows_flags=
        if [[ -n "${NETFLOW_COLLECTORS}" ]] ; then
          export_network_flows_flags="--netflow-targets ${NETFLOW_COLLECTORS}"
        fi
        if [[ -n "${SFLOW_COLLECTORS}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --sflow-targets ${SFLOW_COLLECTORS}"
        fi
        if [[ -n "${IPFIX_COLLECTORS}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --ipfix-targets ${IPFIX_COLLECTORS}"
        fi
        if [[ -n "${IPFIX_CACHE_MAX_FLOWS}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --ipfix-cache-max-flows ${IPFIX_CACHE_MAX_FLOWS}"
        fi
        if [[ -n "${IPFIX_CACHE_ACTIVE_TIMEOUT}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --ipfix-cache-active-timeout ${IPFIX_CACHE_ACTIVE_TIMEOUT}"
        fi
        if [[ -n "${IPFIX_SAMPLING}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --ipfix-sampling ${IPFIX_SAMPLING}"
        fi
        gw_interface_flag=
        # if br-ex1 is configured on the node, we want to use it for external gateway traffic
        if [ -d /sys/class/net/br-ex1 ]; then
          gw_interface_flag="--exgw-interface=br-ex1"
        fi

        node_mgmt_port_netdev_flags=
        if [[ -n "${OVNKUBE_NODE_MGMT_PORT_NETDEV}" ]] ; then
          node_mgmt_port_netdev_flags="--ovnkube-node-mgmt-port-netdev ${OVNKUBE_NODE_MGMT_PORT_NETDEV}"
        fi
        if [[ -n "${OVNKUBE_NODE_MGMT_PORT_DP_RESOURCE_NAME}" ]] ; then
          node_mgmt_port_netdev_flags="$node_mgmt_port_netdev_flags --ovnkube-node-mgmt-port-dp-resource-name ${OVNKUBE_NODE_MGMT_PORT_DP_RESOURCE_NAME}"
        fi

        multi_network_enabled_flag=
        if [[ "true" == "true" ]]; then
          multi_network_enabled_flag="--enable-multi-network"
        fi

        multi_network_policy_enabled_flag=
        if [[ "false" == "true" ]]; then
          multi_network_policy_enabled_flag="--enable-multi-networkpolicy"
        fi

        admin_network_policy_enabled_flag=
        if [[ "false" == "true" ]]; then
          admin_network_policy_enabled_flag="--enable-admin-network-policy"
        fi

        # If IP Forwarding mode is global set it in the host here.
        ip_forwarding_flag=
        if [ "" == "Global" ]; then
          sysctl -w net.ipv4.ip_forward=1
          sysctl -w net.ipv6.conf.all.forwarding=1
        else
          ip_forwarding_flag="--disable-forwarding"
        fi

        NETWORK_NODE_IDENTITY_ENABLE=
        if [[ "true" == "true" ]]; then
          NETWORK_NODE_IDENTITY_ENABLE="
            --bootstrap-kubeconfig=/var/lib/kubelet/kubeconfig
            --cert-dir=/etc/ovn/ovnkube-node-certs
            --cert-duration=24h
          "
        fi

        ovn_v4_join_subnet_opt=
        if [[ "" != "" ]]; then
          ovn_v4_join_subnet_opt="--gateway-v4-join-subnet "
        fi
        ovn_v6_join_subnet_opt=
        if [[ "" != "" ]]; then
          ovn_v6_join_subnet_opt="--gateway-v6-join-subnet "
        fi

        ovn_v4_transit_switch_subnet_opt=
        if [[ "" != "" ]]; then
          ovn_v4_transit_switch_subnet_opt="--cluster-manager-v4-transit-switch-subnet "
        fi
        ovn_v6_transit_switch_subnet_opt=
        if [[ "" != "" ]]; then
          ovn_v6_transit_switch_subnet_opt="--cluster-manager-v6-transit-switch-subnet "
        fi

        exec /usr/bin/ovnkube \
          --init-ovnkube-controller "${K8S_NODE}" \
          --init-node "${K8S_NODE}" \
          --config-file=/run/ovnkube-config/ovnkube.conf \
          --ovn-empty-lb-events \
          --loglevel "${log_level}" \
          --inactivity-probe="${OVN_CONTROLLER_INACTIVITY_PROBE}" \
          ${gateway_mode_flags} \
          ${node_mgmt_port_netdev_flags} \
          --metrics-bind-address "127.0.0.1:${metrics_port}" \
          --ovn-metrics-bind-address "127.0.0.1:${ovn_metrics_port}" \
          --metrics-enable-pprof \
          --metrics-enable-config-duration \
          --export-ovs-metrics \
          --disable-snat-multiple-gws \
          ${export_network_flows_flags} \
          ${multi_network_enabled_flag} \
          ${multi_network_policy_enabled_flag} \
          ${admin_network_policy_enabled_flag} \
          --enable-multicast \
          --zone ${K8S_NODE} \
          --enable-interconnect \
          --acl-logging-rate-limit "20" \
          ${gw_interface_flag} \
          --enable-multi-external-gateway=true \
          ${ip_forwarding_flag} \
          ${NETWORK_NODE_IDENTITY_ENABLE} \
          ${ovn_v4_join_subnet_opt} \
          ${ovn_v6_join_subnet_opt} \
          ${ovn_v4_transit_switch_subnet_opt} \
          ${ovn_v6_transit_switch_subnet_opt}
      }
  kind: ConfigMap
  metadata:
    annotations:
      kubernetes.io/description: |
        This is a script used by the ovn-kubernetes daemonset
      release.openshift.io/version: 4.14.59
    creationTimestamp: "2025-11-17T09:18:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          f:ovnkube-lib.sh: {}
        f:metadata:
          f:annotations:
            f:kubernetes.io/description: {}
            f:release.openshift.io/version: {}
          f:ownerReferences:
            k:{"uid":"de0dec5f-c369-4191-91db-e20c2450d94e"}: {}
      manager: cluster-network-operator/operconfig
      operation: Apply
      time: "2025-11-17T09:19:18Z"
    name: ovnkube-script-lib
    namespace: openshift-ovn-kubernetes
    ownerReferences:
    - apiVersion: operator.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Network
      name: cluster
      uid: de0dec5f-c369-4191-91db-e20c2450d94e
    resourceVersion: "5418"
    uid: 9fbdd1fc-3cae-4c55-ae59-aee9cadb6d0e
- apiVersion: v1
  data:
    ca-bundle.crt: |
      -----BEGIN CERTIFICATE-----
      MIIDVTCCAj2gAwIBAgIIL4KsOxPfNhEwDQYJKoZIhvcNAQELBQAwODE2MDQGA1UE
      Awwtb3BlbnNoaWZ0LW92bi1rdWJlcm5ldGVzX3NpZ25lci1jYUAxNzYzMzcxMTA0
      MB4XDTI1MTExNzA5MTgyM1oXDTM1MTExNTA5MTgyNFowODE2MDQGA1UEAwwtb3Bl
      bnNoaWZ0LW92bi1rdWJlcm5ldGVzX3NpZ25lci1jYUAxNzYzMzcxMTA0MIIBIjAN
      BgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyaYoNBaqFd9s5Pgi8G5zVMG+/0H3
      bGpxr7PXPip+tFpRtqnIJb4qPnaeee7h1MsF4yxRzRpDcunfV1aE/iIJCWA0FpXx
      js45hSY1/qOGh/bZYlE9KtxUTASNDAZk18ykOTXSHTCwly/d9PMP35iq0ghsEytH
      ka2hqN25ZP+9xfwMzQ4nLPpxfOrnRkkgjhElyypfx5abBL0eRhTJMbOyJYfho79P
      VmibBtqKYmfm+OqWH5WcMa5gjsgkgNpYJiIbm93XP7mMUHxjeb9BT1dVWaWldQQO
      bhB5yvr4deRPsotHouWVzMtqZnJREKJhurUWzfGMDgwpNrZVv3LwMsOtzQIDAQAB
      o2MwYTAOBgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQU
      bMCyhBUYETC+ADWybuqqV0pLGakwHwYDVR0jBBgwFoAUbMCyhBUYETC+ADWybuqq
      V0pLGakwDQYJKoZIhvcNAQELBQADggEBAAI+dCO8Id2zxCazcYeCfvnusuxzsyqv
      WM2LCI6HgZAI9upQ71npZa0KyxX7mUoIJzDy+GZC6lvgAfW2xfgZwezqUMO0Z/Uu
      O25oYOjIPxMl7NXZ0v2Lhbav9X5RRd8HzpGiOzpwCLdqrGTn7HAdxKUBlJH/Mdul
      3I5Md7EfplYR+Nohp9TAWbo9Y1Y8TGMC5sb9MTsh2OoOTdWmt40rA2gNzHdRc5K7
      ROSfUE78MNHwqYKsIsClkpjpAkfSrxYxiXBdPi722HHrAx1UEuPoFpvYLvd2nnmv
      vy7qhR4865HT+tJs0r4D1VyRprJEx4XVqMt9QOmHpDn6HR9X+BoWz9Q=
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    creationTimestamp: "2025-11-17T09:18:24Z"
    labels:
      auth.openshift.io/managed-certificate-type: ca-bundle
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:ca-bundle.crt: {}
        f:metadata:
          f:labels:
            .: {}
            f:auth.openshift.io/managed-certificate-type: {}
      manager: cluster-network-operator
      operation: Update
      time: "2025-11-17T09:18:24Z"
    name: signer-ca
    namespace: openshift-ovn-kubernetes
    resourceVersion: "4894"
    uid: e3ede842-0629-4047-80bf-d8c1b1cc9ddb
kind: ConfigMapList
metadata:
  resourceVersion: "32576"
